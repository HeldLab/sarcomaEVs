{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# To analyze and classify the harmonized dataset\n",
    "* note that some classifiers may require using a computer cluster due to memory requirements\n",
    "* These package versions were used for the manuscript: scikit-learn v1.3.2, catboost v1.2.3, xgboost v2.0.3, lightgbm v4.3.0, joblib v1.3.2, optuna v3.5.0, numpy v1.23.4, pandas v2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import chart_studio.plotly as py\n",
    "#import plotly.express as px\n",
    "#import plotly.graph_objects as go\n",
    "#from matplotlib import colors\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from numpy import ma\n",
    "import seaborn as sns\n",
    "import logging\n",
    "#pd.set_option('display.max_columns',200)\n",
    "#import plotnine as p9\n",
    "#from plotnine import *\n",
    "#from plotnine.data import *\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "#from natsort import natsorted\n",
    "from glob import glob\n",
    "import glob\n",
    "from statistics import mean\n",
    "#import shutil\n",
    "import random\n",
    "import copy\n",
    "#from scipy.stats import shapiro, skew\n",
    "\n",
    "#import lime\n",
    "#import lime.lime_tabular\n",
    "\n",
    "#logger = logging.getLogger(__name__)\n",
    "#logger.setLevel(logging.INFO)\n",
    "#logging.basicConfig()\n",
    "\n",
    "import joblib\n",
    "\n",
    "#import sklearn.datasets\n",
    "\n",
    "#from sklearn.preprocessing import FunctionTransformer\n",
    "#from sklearn.metrics import plot_roc_curve\n",
    "#from sklearn.inspection import plot_partial_dependence, permutation_importance\n",
    "#from sklearn.metrics import RocCurveDisplay, plot_confusion_matrix, plot_precision_recall_curve, roc_curve\n",
    "\n",
    "#import PipelineProfiler\n",
    "from joblib import dump, load\n",
    "import scikitplot as skplt\n",
    "#from sklearn.metrics import classification_report\n",
    "import optuna\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import random\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "#from scipy.stats import shapiro, skew\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "#from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier, StackingClassifier, VotingClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import RFECV, RFE, SelectKBest, chi2\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier, ElasticNetCV, LassoLarsCV, ElasticNet, LogisticRegression\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, f1_score, matthews_corrcoef, mean_squared_error, r2_score, mean_absolute_error, cohen_kappa_score, make_scorer\n",
    "from sklearn.metrics import classification_report, roc_auc_score, auc, RocCurveDisplay, roc_curve\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, QuantileTransformer, FunctionTransformer, MinMaxScaler, PowerTransformer, OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "# Hide warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#plotly_template = 'simple_white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "\n",
    "os.chdir('/Users/jasonheld/Manuscripts/2022_Sarcoma-Exosomes/vanTine_001_FinalDataWithControls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some lists to fill later\n",
    "\n",
    "dfTrainModelEvalAllSeeds = pd.DataFrame()\n",
    "seedListStats = []\n",
    "trainFracListStats = []\n",
    "finalClassifierList = []\n",
    "tunedClassifierListForValidation = []\n",
    "finalImportancesMeanList = []\n",
    "finalFeaturesList = []\n",
    "\n",
    "y_testList = []\n",
    "y_predList = []\n",
    "y_probList = []\n",
    "seedList = []\n",
    "seedListStats = []\n",
    "trainFracListStats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key parameters\n",
    "\n",
    "seed = 7 # use seeds 1-7 for each classification to randomize 7 times\n",
    "np.random.seed(7) # match the seed value\n",
    "target = 'dfHarmonizedCancer'\n",
    "quantType = 'areaHarmonized'\n",
    "trainFrac = .75\n",
    "nJobs = 6\n",
    "nSamplesVal = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a dataframe and a target variable to create an X (predictors) dataframe and a y Series\n",
    "\n",
    "def X_y_split(df, target):\n",
    "    \n",
    "    categorical_features = []\n",
    "    continuous_features = []\n",
    "    binary_features = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            categorical_features.append(col)\n",
    "        \n",
    "        else:\n",
    "            if df[col].nunique() <= 2:\n",
    "                binary_features.append(col)\n",
    "            else:\n",
    "                continuous_features.append(col)\n",
    "                \n",
    "    if categorical_features: # if this list isn't empty\n",
    "        if target in categorical_features:\n",
    "            categorical_features.remove(target)\n",
    "        df.drop(categorical_features, axis = 1, inplace = True)\n",
    "\n",
    "    X, y = df.drop([target], axis = 1), df[target]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function parses columns by type.\n",
    "\n",
    "def columns_catNumOrBin(df):\n",
    "\n",
    "    categorical_features = []\n",
    "    continuous_features = []\n",
    "    binary_features = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            categorical_features.append(col)\n",
    "        else:\n",
    "            if df[col].nunique() <= 2:\n",
    "                binary_features.append(col)\n",
    "            else:\n",
    "                continuous_features.append(col)\n",
    "    \n",
    "    return categorical_features,continuous_features, binary_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute 0s\n",
    "\n",
    "def imputeWideDFMinOr0(df):\n",
    "    \n",
    "    for col in df.columns:\n",
    "\n",
    "        if df[col].dtype == object:\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            if quantType == 'areaHarmonized': # i think min makes sense\n",
    "                df[col].fillna(value = df[col].min(), inplace = True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>dataset</th>\n",
       "      <th>patient</th>\n",
       "      <th>group</th>\n",
       "      <th>sample</th>\n",
       "      <th>timing</th>\n",
       "      <th>dataType</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>dfHarmonizedCancer</th>\n",
       "      <th>...</th>\n",
       "      <th>species</th>\n",
       "      <th>sampleType</th>\n",
       "      <th>category</th>\n",
       "      <th>Reviewed</th>\n",
       "      <th>Protein names</th>\n",
       "      <th>geneNamePrimary</th>\n",
       "      <th>nSamples</th>\n",
       "      <th>areaScaled</th>\n",
       "      <th>areaHarmonized</th>\n",
       "      <th>areaHarmonizedAntilog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S</td>\n",
       "      <td>sarcoma</td>\n",
       "      <td>BJW</td>\n",
       "      <td>S.Pre</td>\n",
       "      <td>BJW.S.Pre</td>\n",
       "      <td>Pre</td>\n",
       "      <td>Intensity_Raw</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>M</td>\n",
       "      <td>c</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>Probable non-functional immunoglobulin kappa v...</td>\n",
       "      <td>IGKV3-7</td>\n",
       "      <td>106</td>\n",
       "      <td>-0.158046</td>\n",
       "      <td>0.999174</td>\n",
       "      <td>1.998855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R</td>\n",
       "      <td>sarcoma</td>\n",
       "      <td>BPA</td>\n",
       "      <td>R.Pre</td>\n",
       "      <td>BPA.R.Pre</td>\n",
       "      <td>Pre</td>\n",
       "      <td>Intensity_Raw</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>M</td>\n",
       "      <td>c</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>Probable non-functional immunoglobulin kappa v...</td>\n",
       "      <td>IGKV3-7</td>\n",
       "      <td>106</td>\n",
       "      <td>-0.824573</td>\n",
       "      <td>0.292549</td>\n",
       "      <td>1.224802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S</td>\n",
       "      <td>sarcoma</td>\n",
       "      <td>BPE</td>\n",
       "      <td>S.Pre</td>\n",
       "      <td>BPE.S.Pre</td>\n",
       "      <td>Pre</td>\n",
       "      <td>Intensity_Raw</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>M</td>\n",
       "      <td>c</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>Probable non-functional immunoglobulin kappa v...</td>\n",
       "      <td>IGKV3-7</td>\n",
       "      <td>106</td>\n",
       "      <td>-0.763513</td>\n",
       "      <td>0.357283</td>\n",
       "      <td>1.281011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S</td>\n",
       "      <td>sarcoma</td>\n",
       "      <td>BPV</td>\n",
       "      <td>S.Pre</td>\n",
       "      <td>BPV.S.Pre</td>\n",
       "      <td>Pre</td>\n",
       "      <td>Intensity_Raw</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>F</td>\n",
       "      <td>c</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>Probable non-functional immunoglobulin kappa v...</td>\n",
       "      <td>IGKV3-7</td>\n",
       "      <td>106</td>\n",
       "      <td>-1.288444</td>\n",
       "      <td>-0.199228</td>\n",
       "      <td>0.871016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S</td>\n",
       "      <td>sarcoma</td>\n",
       "      <td>BTK</td>\n",
       "      <td>S.Pre</td>\n",
       "      <td>BTK.S.Pre</td>\n",
       "      <td>Pre</td>\n",
       "      <td>Intensity_Raw</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>M</td>\n",
       "      <td>c</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>Probable non-functional immunoglobulin kappa v...</td>\n",
       "      <td>IGKV3-7</td>\n",
       "      <td>106</td>\n",
       "      <td>-1.285697</td>\n",
       "      <td>-0.196317</td>\n",
       "      <td>0.872776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  sensitivity  dataset patient  group     sample timing       dataType   race  \\\n",
       "0           S  sarcoma     BJW  S.Pre  BJW.S.Pre    Pre  Intensity_Raw  WHITE   \n",
       "1           R  sarcoma     BPA  R.Pre  BPA.R.Pre    Pre  Intensity_Raw  WHITE   \n",
       "2           S  sarcoma     BPE  S.Pre  BPE.S.Pre    Pre  Intensity_Raw  WHITE   \n",
       "3           S  sarcoma     BPV  S.Pre  BPV.S.Pre    Pre  Intensity_Raw  WHITE   \n",
       "4           S  sarcoma     BTK  S.Pre  BTK.S.Pre    Pre  Intensity_Raw  WHITE   \n",
       "\n",
       "  gender dfHarmonizedCancer  ... species sampleType category  Reviewed  \\\n",
       "0      M                  c  ...     NaN        NaN      NaN  reviewed   \n",
       "1      M                  c  ...     NaN        NaN      NaN  reviewed   \n",
       "2      M                  c  ...     NaN        NaN      NaN  reviewed   \n",
       "3      F                  c  ...     NaN        NaN      NaN  reviewed   \n",
       "4      M                  c  ...     NaN        NaN      NaN  reviewed   \n",
       "\n",
       "                                       Protein names geneNamePrimary nSamples  \\\n",
       "0  Probable non-functional immunoglobulin kappa v...         IGKV3-7      106   \n",
       "1  Probable non-functional immunoglobulin kappa v...         IGKV3-7      106   \n",
       "2  Probable non-functional immunoglobulin kappa v...         IGKV3-7      106   \n",
       "3  Probable non-functional immunoglobulin kappa v...         IGKV3-7      106   \n",
       "4  Probable non-functional immunoglobulin kappa v...         IGKV3-7      106   \n",
       "\n",
       "  areaScaled areaHarmonized areaHarmonizedAntilog  \n",
       "0  -0.158046       0.999174              1.998855  \n",
       "1  -0.824573       0.292549              1.224802  \n",
       "2  -0.763513       0.357283              1.281011  \n",
       "3  -1.288444      -0.199228              0.871016  \n",
       "4  -1.285697      -0.196317              0.872776  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load harmonized output from the Harmonizernotebook\n",
    "\n",
    "dfHarmonized = pd.read_excel('/Users/jasonheld/Manuscripts/2022_Sarcoma-Exosomes/Figure2_Data_CancerVsNormal/dfHarmonized.xlsx', index_col=0)\n",
    "dfHarmonized.rename({'cancer':'dfHarmonizedCancer'}, axis='columns', inplace = True)\n",
    "dfHarmonized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidyDFHarmonized (df): # 'Intensity_Raw', 'NormalizedIntensity_Log2'\n",
    "\n",
    "    global quantType\n",
    "    \n",
    "    df = df.loc[:,~df.columns.str.contains('CON_')] # remove contaminants    \n",
    "    df.drop(columns = ['areaHarmonizedAntilog', 'areaScaled', 'area'], inplace = True) # not using these quant columns\n",
    "\n",
    "    totalCols = df.columns\n",
    "    numCols = df._get_numeric_data().columns\n",
    "    catCols = list(set(totalCols)-set(numCols))\n",
    "    catCols.remove('geneNamePrimary')\n",
    "    catCols.remove('sample')\n",
    "    catCols.remove(target)\n",
    "    \n",
    "    #remove rows without areas & duplicates\n",
    "    df.dropna(subset=[quantType], inplace=True)\n",
    "    df.drop_duplicates(inplace = True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidy the dataframe\n",
    "\n",
    "dfHarmonized = tidyDFHarmonized(dfHarmonized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter dfHarmonized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeCrapome(df):\n",
    "    crapome =['KRT1', 'KRT2', 'KRT3', 'KRT4', 'KRT5', 'KRT6', 'KRT7', 'KRT8', 'KRT9', 'KRT10', 'KRT11', 'KRT12', 'KRT13', 'KRT14', 'KRT15', 'KRT16', 'KRT17', 'KRT18', 'KRT19', 'KRT20', 'KRT21', 'KRT22', 'KRT23', 'KRT24']\n",
    "    df.query('geneNamePrimary not in @crapome', inplace = True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFiltered = removeCrapome(dfHarmonized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count protein IDs\n",
    "\n",
    "def filterNDetectionsPerProtAcc(df, nSamplesVal_):\n",
    "\n",
    "    global nSamplesVal\n",
    "    nSamplesVal = nSamplesVal_\n",
    "    print(nSamplesVal)\n",
    "    \n",
    "    df = df.query('nSamples >= @nSamplesVal') # variable set above\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "dfFiltered = filterNDetectionsPerProtAcc(dfFiltered, nSamplesVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### impute NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputeDfMinOr0 (df, colToImpute):\n",
    "    \n",
    "    df[colToImpute].fillna(value = df[colToImpute].min(), inplace = True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFiltered = imputeDfMinOr0(dfFiltered, 'areaHarmonized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pivot to wide data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>geneNamePrimary</th>\n",
       "      <th>sample</th>\n",
       "      <th>dfHarmonizedCancer</th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A2M</th>\n",
       "      <th>ACTA1</th>\n",
       "      <th>ACTB</th>\n",
       "      <th>ACTBL2</th>\n",
       "      <th>ACTG1</th>\n",
       "      <th>ACTG2</th>\n",
       "      <th>ACTN1</th>\n",
       "      <th>...</th>\n",
       "      <th>VIM</th>\n",
       "      <th>VTN</th>\n",
       "      <th>VWF</th>\n",
       "      <th>WDR1</th>\n",
       "      <th>YWHAB</th>\n",
       "      <th>YWHAE</th>\n",
       "      <th>YWHAG</th>\n",
       "      <th>YWHAH</th>\n",
       "      <th>YWHAQ</th>\n",
       "      <th>YWHAZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BIE.S.Pre</td>\n",
       "      <td>c</td>\n",
       "      <td>0.830252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.529702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.238035</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.605869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.662153</td>\n",
       "      <td>1.651220</td>\n",
       "      <td>1.552332</td>\n",
       "      <td>0.394781</td>\n",
       "      <td>0.568536</td>\n",
       "      <td>0.793171</td>\n",
       "      <td>0.088769</td>\n",
       "      <td>0.297304</td>\n",
       "      <td>0.425608</td>\n",
       "      <td>0.877359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BIH.S.Pre</td>\n",
       "      <td>c</td>\n",
       "      <td>0.332276</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.309366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.478284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.456010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410026</td>\n",
       "      <td>1.576208</td>\n",
       "      <td>1.427785</td>\n",
       "      <td>-0.010363</td>\n",
       "      <td>-0.163921</td>\n",
       "      <td>0.264368</td>\n",
       "      <td>-0.276322</td>\n",
       "      <td>0.307422</td>\n",
       "      <td>0.205803</td>\n",
       "      <td>0.476802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BIP.S.Pre</td>\n",
       "      <td>c</td>\n",
       "      <td>0.357704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.208670</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.486699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.312867</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.271596</td>\n",
       "      <td>1.465018</td>\n",
       "      <td>1.357034</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>-0.153920</td>\n",
       "      <td>0.293756</td>\n",
       "      <td>-0.187815</td>\n",
       "      <td>0.363499</td>\n",
       "      <td>0.159687</td>\n",
       "      <td>0.525286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BJE.R.Pre</td>\n",
       "      <td>c</td>\n",
       "      <td>0.482507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.400820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.432288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.640151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263245</td>\n",
       "      <td>1.466153</td>\n",
       "      <td>1.407550</td>\n",
       "      <td>0.464374</td>\n",
       "      <td>0.455905</td>\n",
       "      <td>0.528796</td>\n",
       "      <td>0.104778</td>\n",
       "      <td>0.395489</td>\n",
       "      <td>0.370452</td>\n",
       "      <td>0.871807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BJK.R.Pre</td>\n",
       "      <td>c</td>\n",
       "      <td>0.652147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.550001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.495373</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.642554</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089003</td>\n",
       "      <td>1.599650</td>\n",
       "      <td>1.635177</td>\n",
       "      <td>0.675694</td>\n",
       "      <td>0.686577</td>\n",
       "      <td>0.867544</td>\n",
       "      <td>-0.069715</td>\n",
       "      <td>0.407808</td>\n",
       "      <td>0.310482</td>\n",
       "      <td>0.880123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>MS194879LUM_goncalo_lyden_137_Plasma_a</td>\n",
       "      <td>c</td>\n",
       "      <td>0.695520</td>\n",
       "      <td>2.162397</td>\n",
       "      <td>0.620386</td>\n",
       "      <td>0.152694</td>\n",
       "      <td>0.029239</td>\n",
       "      <td>0.256713</td>\n",
       "      <td>0.125511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.286680</td>\n",
       "      <td>1.280491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.504451</td>\n",
       "      <td>-0.612937</td>\n",
       "      <td>-0.544749</td>\n",
       "      <td>-0.490847</td>\n",
       "      <td>-0.575711</td>\n",
       "      <td>-0.186766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>MS194879LUM_goncalo_lyden_141_Plasma_a</td>\n",
       "      <td>c</td>\n",
       "      <td>0.561369</td>\n",
       "      <td>2.294855</td>\n",
       "      <td>1.989700</td>\n",
       "      <td>1.506687</td>\n",
       "      <td>1.542420</td>\n",
       "      <td>1.693424</td>\n",
       "      <td>1.588997</td>\n",
       "      <td>0.891286</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.260654</td>\n",
       "      <td>1.245262</td>\n",
       "      <td>0.099659</td>\n",
       "      <td>0.677961</td>\n",
       "      <td>0.876450</td>\n",
       "      <td>0.702891</td>\n",
       "      <td>0.786348</td>\n",
       "      <td>0.816816</td>\n",
       "      <td>1.141525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>MS194879LUM_goncalo_lyden_142_Plasma_a</td>\n",
       "      <td>c</td>\n",
       "      <td>0.592910</td>\n",
       "      <td>2.337891</td>\n",
       "      <td>1.489827</td>\n",
       "      <td>1.014633</td>\n",
       "      <td>0.979343</td>\n",
       "      <td>1.144509</td>\n",
       "      <td>1.081129</td>\n",
       "      <td>-0.105925</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.214992</td>\n",
       "      <td>0.924127</td>\n",
       "      <td>-0.631446</td>\n",
       "      <td>0.277585</td>\n",
       "      <td>0.419303</td>\n",
       "      <td>0.280890</td>\n",
       "      <td>0.324307</td>\n",
       "      <td>0.330923</td>\n",
       "      <td>0.792864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>NS172995QEp_Ayuko_Lyden_CTRL63_plasma</td>\n",
       "      <td>n</td>\n",
       "      <td>0.531825</td>\n",
       "      <td>2.062484</td>\n",
       "      <td>0.391271</td>\n",
       "      <td>-0.456651</td>\n",
       "      <td>-0.667611</td>\n",
       "      <td>-0.169866</td>\n",
       "      <td>-0.128379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.803730</td>\n",
       "      <td>0.231407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>NS172995QEp_Ayuko_Lyden_CTRL64_plasma</td>\n",
       "      <td>n</td>\n",
       "      <td>0.836052</td>\n",
       "      <td>2.133768</td>\n",
       "      <td>0.569608</td>\n",
       "      <td>-0.203207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050999</td>\n",
       "      <td>-0.056756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.086213</td>\n",
       "      <td>0.768596</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 375 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "geneNamePrimary                                  sample dfHarmonizedCancer  \\\n",
       "0                                             BIE.S.Pre                  c   \n",
       "1                                             BIH.S.Pre                  c   \n",
       "2                                             BIP.S.Pre                  c   \n",
       "3                                             BJE.R.Pre                  c   \n",
       "4                                             BJK.R.Pre                  c   \n",
       "..                                                  ...                ...   \n",
       "172              MS194879LUM_goncalo_lyden_137_Plasma_a                  c   \n",
       "173              MS194879LUM_goncalo_lyden_141_Plasma_a                  c   \n",
       "174              MS194879LUM_goncalo_lyden_142_Plasma_a                  c   \n",
       "175               NS172995QEp_Ayuko_Lyden_CTRL63_plasma                  n   \n",
       "176               NS172995QEp_Ayuko_Lyden_CTRL64_plasma                  n   \n",
       "\n",
       "geneNamePrimary      A1BG       A2M     ACTA1      ACTB    ACTBL2     ACTG1  \\\n",
       "0                0.830252       NaN  1.529702       NaN  0.238035       NaN   \n",
       "1                0.332276       NaN  1.309366       NaN  0.478284       NaN   \n",
       "2                0.357704       NaN  1.208670       NaN -0.486699       NaN   \n",
       "3                0.482507       NaN  1.400820       NaN  0.432288       NaN   \n",
       "4                0.652147       NaN  1.550001       NaN  0.495373       NaN   \n",
       "..                    ...       ...       ...       ...       ...       ...   \n",
       "172              0.695520  2.162397  0.620386  0.152694  0.029239  0.256713   \n",
       "173              0.561369  2.294855  1.989700  1.506687  1.542420  1.693424   \n",
       "174              0.592910  2.337891  1.489827  1.014633  0.979343  1.144509   \n",
       "175              0.531825  2.062484  0.391271 -0.456651 -0.667611 -0.169866   \n",
       "176              0.836052  2.133768  0.569608 -0.203207       NaN  0.050999   \n",
       "\n",
       "geneNamePrimary     ACTG2     ACTN1  ...       VIM       VTN       VWF  \\\n",
       "0                     NaN  0.605869  ... -0.662153  1.651220  1.552332   \n",
       "1                     NaN  0.456010  ...  0.410026  1.576208  1.427785   \n",
       "2                     NaN  0.312867  ... -0.271596  1.465018  1.357034   \n",
       "3                     NaN  0.640151  ...  0.263245  1.466153  1.407550   \n",
       "4                     NaN  0.642554  ... -0.089003  1.599650  1.635177   \n",
       "..                    ...       ...  ...       ...       ...       ...   \n",
       "172              0.125511       NaN  ...       NaN  1.286680  1.280491   \n",
       "173              1.588997  0.891286  ...       NaN  1.260654  1.245262   \n",
       "174              1.081129 -0.105925  ...       NaN  1.214992  0.924127   \n",
       "175             -0.128379       NaN  ...       NaN  0.803730  0.231407   \n",
       "176             -0.056756       NaN  ...       NaN  1.086213  0.768596   \n",
       "\n",
       "geneNamePrimary      WDR1     YWHAB     YWHAE     YWHAG     YWHAH     YWHAQ  \\\n",
       "0                0.394781  0.568536  0.793171  0.088769  0.297304  0.425608   \n",
       "1               -0.010363 -0.163921  0.264368 -0.276322  0.307422  0.205803   \n",
       "2                0.002520 -0.153920  0.293756 -0.187815  0.363499  0.159687   \n",
       "3                0.464374  0.455905  0.528796  0.104778  0.395489  0.370452   \n",
       "4                0.675694  0.686577  0.867544 -0.069715  0.407808  0.310482   \n",
       "..                    ...       ...       ...       ...       ...       ...   \n",
       "172                   NaN -0.504451 -0.612937 -0.544749 -0.490847 -0.575711   \n",
       "173              0.099659  0.677961  0.876450  0.702891  0.786348  0.816816   \n",
       "174             -0.631446  0.277585  0.419303  0.280890  0.324307  0.330923   \n",
       "175                   NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "176                   NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "geneNamePrimary     YWHAZ  \n",
       "0                0.877359  \n",
       "1                0.476802  \n",
       "2                0.525286  \n",
       "3                0.871807  \n",
       "4                0.880123  \n",
       "..                    ...  \n",
       "172             -0.186766  \n",
       "173              1.141525  \n",
       "174              0.792864  \n",
       "175                   NaN  \n",
       "176                   NaN  \n",
       "\n",
       "[177 rows x 375 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define columns by feature type\n",
    "categorical_features,continuous_features, binary_features = columns_catNumOrBin(dfFiltered)\n",
    "\n",
    "# pivot df by geneNamePrimary\n",
    "indexListTemp = categorical_features.copy()\n",
    "indexListTemp.remove('geneNamePrimary')\n",
    "indexListTemp.remove('protAcc')\n",
    "indexListTemp.remove('Reviewed')\n",
    "indexListTemp.remove('Protein names')\n",
    "\n",
    "dfML = pd.pivot_table(\n",
    "    dfFiltered,\n",
    "    values = 'areaHarmonized',\n",
    "    columns = 'geneNamePrimary',\n",
    "    index = ['sample', target]\n",
    ").reset_index()\n",
    "\n",
    "dfML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features,continuous_features, binary_features = columns_catNumOrBin(dfML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfRemoveAllCatColsButTarget(df):\n",
    "    \n",
    "    categorical_features = []\n",
    "    continuous_features = []\n",
    "    binary_features = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            categorical_features.append(col)\n",
    "        \n",
    "        else:\n",
    "            if df[col].nunique() <= 2:\n",
    "                binary_features.append(col)\n",
    "            else:\n",
    "                continuous_features.append(col)\n",
    "    \n",
    "    categorical_features.remove(target)\n",
    "        \n",
    "    df.drop(categorical_features, axis = 1, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML = dfRemoveAllCatColsButTarget(dfML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dfML into training, validation, and testing datasets prior to any transformation or feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfML.copy()\n",
    "\n",
    "# random splitting into each set\n",
    "dfTrain = df.groupby(target, group_keys=False).sample(frac=trainFrac, random_state=seed)\n",
    "dfTest = df[~df.index.isin(dfTrain.index)]\n",
    "\n",
    "# validation fraction, samples taken from dfTrain.\n",
    "valFrac = .20\n",
    "\n",
    "dfValidation = dfTrain.groupby(target, group_keys=False).sample(frac=valFrac, random_state=seed)\n",
    "dfTrain= dfTrain[~dfTrain.index.isin(dfValidation.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "df = dfML\n",
    "df = df.reset_index(drop = True)\n",
    "df.to_excel('dfML-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '.xlsx')\n",
    "\n",
    "df = dfTest\n",
    "df = df.reset_index(drop = True)\n",
    "df.to_excel('_dfTest_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.xlsx')\n",
    "\n",
    "df = dfValidation\n",
    "df = df.reset_index(drop = True)\n",
    "df.to_excel('_dfValidation_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.xlsx')\n",
    "\n",
    "df = dfTrain\n",
    "df = df.reset_index(drop = True)\n",
    "df.to_excel('_dfTrain_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "\n",
    "X, y = X_y_split(dfTrain, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>geneNamePrimary</th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A2M</th>\n",
       "      <th>ACTA1</th>\n",
       "      <th>ACTB</th>\n",
       "      <th>ACTBL2</th>\n",
       "      <th>ACTG1</th>\n",
       "      <th>ACTG2</th>\n",
       "      <th>ACTN1</th>\n",
       "      <th>ACTN4</th>\n",
       "      <th>ADAM10</th>\n",
       "      <th>...</th>\n",
       "      <th>VIM</th>\n",
       "      <th>VTN</th>\n",
       "      <th>VWF</th>\n",
       "      <th>WDR1</th>\n",
       "      <th>YWHAB</th>\n",
       "      <th>YWHAE</th>\n",
       "      <th>YWHAG</th>\n",
       "      <th>YWHAH</th>\n",
       "      <th>YWHAQ</th>\n",
       "      <th>YWHAZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.427119</td>\n",
       "      <td>2.065347</td>\n",
       "      <td>0.530682</td>\n",
       "      <td>-0.129343</td>\n",
       "      <td>-0.423379</td>\n",
       "      <td>-0.024570</td>\n",
       "      <td>0.026106</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>1.526734</td>\n",
       "      <td>1.481474</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-0.274566</td>\n",
       "      <td>-0.196867</td>\n",
       "      <td>-0.607559</td>\n",
       "      <td>-0.250478</td>\n",
       "      <td>-0.308128</td>\n",
       "      <td>-0.316296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.376259</td>\n",
       "      <td>2.141387</td>\n",
       "      <td>1.295984</td>\n",
       "      <td>0.638655</td>\n",
       "      <td>0.628846</td>\n",
       "      <td>0.946476</td>\n",
       "      <td>0.874163</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764930</td>\n",
       "      <td>0.673287</td>\n",
       "      <td>1.111831</td>\n",
       "      <td>-0.828170</td>\n",
       "      <td>-0.138130</td>\n",
       "      <td>0.010782</td>\n",
       "      <td>-0.160418</td>\n",
       "      <td>-0.101406</td>\n",
       "      <td>-0.190496</td>\n",
       "      <td>0.486893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.562377</td>\n",
       "      <td>2.207797</td>\n",
       "      <td>-0.140634</td>\n",
       "      <td>-0.330657</td>\n",
       "      <td>-0.571826</td>\n",
       "      <td>-0.477617</td>\n",
       "      <td>-0.717801</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>1.117175</td>\n",
       "      <td>1.010081</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-1.796210</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.604599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.772372</td>\n",
       "      <td>1.888034</td>\n",
       "      <td>-0.285125</td>\n",
       "      <td>-1.099648</td>\n",
       "      <td>-0.873302</td>\n",
       "      <td>-0.910295</td>\n",
       "      <td>-0.877917</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>1.394733</td>\n",
       "      <td>0.326443</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-1.796210</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.604599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>-0.028891</td>\n",
       "      <td>1.900702</td>\n",
       "      <td>-0.124414</td>\n",
       "      <td>-0.925085</td>\n",
       "      <td>-0.530581</td>\n",
       "      <td>-0.944076</td>\n",
       "      <td>-0.699827</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>0.895636</td>\n",
       "      <td>1.436516</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-1.796210</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.604599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.321744</td>\n",
       "      <td>1.953634</td>\n",
       "      <td>-0.390985</td>\n",
       "      <td>-1.026904</td>\n",
       "      <td>-1.338389</td>\n",
       "      <td>-0.779894</td>\n",
       "      <td>-1.079842</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>0.922462</td>\n",
       "      <td>0.045334</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-1.796210</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.604599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.389805</td>\n",
       "      <td>2.111616</td>\n",
       "      <td>-0.070390</td>\n",
       "      <td>-0.718909</td>\n",
       "      <td>-1.749931</td>\n",
       "      <td>-0.526616</td>\n",
       "      <td>-0.639962</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>1.297184</td>\n",
       "      <td>0.053385</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-1.796210</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.604599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.531825</td>\n",
       "      <td>2.062484</td>\n",
       "      <td>0.391271</td>\n",
       "      <td>-0.456651</td>\n",
       "      <td>-0.667611</td>\n",
       "      <td>-0.169866</td>\n",
       "      <td>-0.128379</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>0.803730</td>\n",
       "      <td>0.231407</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-1.796210</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.604599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.334590</td>\n",
       "      <td>2.042939</td>\n",
       "      <td>-0.097486</td>\n",
       "      <td>-0.776217</td>\n",
       "      <td>-0.758497</td>\n",
       "      <td>-0.726377</td>\n",
       "      <td>-0.669988</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>0.853324</td>\n",
       "      <td>-2.207381</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-1.796210</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.604599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.066970</td>\n",
       "      <td>2.261982</td>\n",
       "      <td>0.166255</td>\n",
       "      <td>-0.506707</td>\n",
       "      <td>-0.249093</td>\n",
       "      <td>-0.603873</td>\n",
       "      <td>-0.377728</td>\n",
       "      <td>-1.265311</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282797</td>\n",
       "      <td>1.024570</td>\n",
       "      <td>0.678333</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-1.796210</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.604599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 373 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "geneNamePrimary      A1BG       A2M     ACTA1      ACTB    ACTBL2     ACTG1  \\\n",
       "110              0.427119  2.065347  0.530682 -0.129343 -0.423379 -0.024570   \n",
       "146              0.376259  2.141387  1.295984  0.638655  0.628846  0.946476   \n",
       "126              0.562377  2.207797 -0.140634 -0.330657 -0.571826 -0.477617   \n",
       "121              0.772372  1.888034 -0.285125 -1.099648 -0.873302 -0.910295   \n",
       "152             -0.028891  1.900702 -0.124414 -0.925085 -0.530581 -0.944076   \n",
       "..                    ...       ...       ...       ...       ...       ...   \n",
       "86               0.321744  1.953634 -0.390985 -1.026904 -1.338389 -0.779894   \n",
       "102              0.389805  2.111616 -0.070390 -0.718909 -1.749931 -0.526616   \n",
       "175              0.531825  2.062484  0.391271 -0.456651 -0.667611 -0.169866   \n",
       "96               0.334590  2.042939 -0.097486 -0.776217 -0.758497 -0.726377   \n",
       "156              0.066970  2.261982  0.166255 -0.506707 -0.249093 -0.603873   \n",
       "\n",
       "geneNamePrimary     ACTG2     ACTN1     ACTN4    ADAM10  ...       VIM  \\\n",
       "110              0.026106 -1.836067 -1.606185 -1.860046  ... -1.030240   \n",
       "146              0.874163 -1.836067 -1.606185 -1.860046  ...  0.764930   \n",
       "126             -0.717801 -1.836067 -1.606185 -1.860046  ... -1.030240   \n",
       "121             -0.877917 -1.836067 -1.606185 -1.860046  ... -1.030240   \n",
       "152             -0.699827 -1.836067 -1.606185 -1.860046  ... -1.030240   \n",
       "..                    ...       ...       ...       ...  ...       ...   \n",
       "86              -1.079842 -1.836067 -1.606185 -1.860046  ... -1.030240   \n",
       "102             -0.639962 -1.836067 -1.606185 -1.860046  ... -1.030240   \n",
       "175             -0.128379 -1.836067 -1.606185 -1.860046  ... -1.030240   \n",
       "96              -0.669988 -1.836067 -1.606185 -1.860046  ... -1.030240   \n",
       "156             -0.377728 -1.265311 -1.606185 -1.860046  ...  0.282797   \n",
       "\n",
       "geneNamePrimary       VTN       VWF      WDR1     YWHAB     YWHAE     YWHAG  \\\n",
       "110              1.526734  1.481474 -2.104248 -0.274566 -0.196867 -0.607559   \n",
       "146              0.673287  1.111831 -0.828170 -0.138130  0.010782 -0.160418   \n",
       "126              1.117175  1.010081 -2.104248 -1.796210 -1.862757 -2.926084   \n",
       "121              1.394733  0.326443 -2.104248 -1.796210 -1.862757 -2.926084   \n",
       "152              0.895636  1.436516 -2.104248 -1.796210 -1.862757 -2.926084   \n",
       "..                    ...       ...       ...       ...       ...       ...   \n",
       "86               0.922462  0.045334 -2.104248 -1.796210 -1.862757 -2.926084   \n",
       "102              1.297184  0.053385 -2.104248 -1.796210 -1.862757 -2.926084   \n",
       "175              0.803730  0.231407 -2.104248 -1.796210 -1.862757 -2.926084   \n",
       "96               0.853324 -2.207381 -2.104248 -1.796210 -1.862757 -2.926084   \n",
       "156              1.024570  0.678333 -2.104248 -1.796210 -1.862757 -2.926084   \n",
       "\n",
       "geneNamePrimary     YWHAH     YWHAQ     YWHAZ  \n",
       "110             -0.250478 -0.308128 -0.316296  \n",
       "146             -0.101406 -0.190496  0.486893  \n",
       "126             -2.320411 -2.936509 -1.604599  \n",
       "121             -2.320411 -2.936509 -1.604599  \n",
       "152             -2.320411 -2.936509 -1.604599  \n",
       "..                    ...       ...       ...  \n",
       "86              -2.320411 -2.936509 -1.604599  \n",
       "102             -2.320411 -2.936509 -1.604599  \n",
       "175             -2.320411 -2.936509 -1.604599  \n",
       "96              -2.320411 -2.936509 -1.604599  \n",
       "156             -2.320411 -2.936509 -1.604599  \n",
       "\n",
       "[107 rows x 373 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputeWideDFMinOr0(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and scale the data as in the pipeline below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale each gene separately.\n",
    "\n",
    "def transformX (X, transformer):\n",
    "    \n",
    "    if transformer == RobustScaler:\n",
    "        transformer = RobustScaler(with_centering = True, with_scaling = True, quantile_range=(25, 75), unit_variance = False)\n",
    "\n",
    "    X_transformed = transformer.fit_transform(X)\n",
    "    X_transformed = pd.DataFrame(X_transformed, columns=X.columns) # turn back into df\n",
    "\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform, matching the pipeline\n",
    "\n",
    "X_transformed = transformX(X, RobustScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# scale to same variance, this matches the pipeline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_transformed)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_transformed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection with recursive feature extraction (RFE)\n",
    "* tune minFeaturesToSelect and cv to retain ~60-150 proteins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CV strategy\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state = seed)\n",
    "cv_splits = list(cv.split(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scoring\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    '''\n",
    "    This function returns an evaluation metric\n",
    "    '''\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def customRFECV (X_input):\n",
    "\n",
    "    global selected_features\n",
    "    global rfe\n",
    "    global min_features_to_select\n",
    "    \n",
    "    estimator = RandomForestClassifier(random_state=seed)\n",
    "\n",
    "    # for seed 1 = use minFeaturesToSelect = 75 and cv = 6,\n",
    "    # for seed 2 = use minFeaturesToSelect = 125 and cv = 6,\n",
    "    # for seed 3 = use minFeaturesToSelect = 100 and cv = 5,\n",
    "    # for seed 4 = use minFeaturesToSelect = 100 and cv = 3,\n",
    "    # for seed 5 = use minFeaturesToSelect = 100 and cv = 5,\n",
    "    # for seed 6 = use minFeaturesToSelect = 50 and cv = 5,\n",
    "    # for seed 7 = use minFeaturesToSelect = 100 and cv = 4,\n",
    "    \n",
    "    minFeaturesToSelect = 100\n",
    "    \n",
    "    rfe = RFECV(\n",
    "        estimator=estimator, \n",
    "        \n",
    "        cv=4, \n",
    "        scoring= make_scorer(quadratic_weighted_kappa),\n",
    "        n_jobs = nJobs,\n",
    "        step = 1,\n",
    "    )\n",
    "    \n",
    "    rfe.fit(X_scaled, y)\n",
    "    \n",
    "    selected_features = []\n",
    "    \n",
    "    for i, feature in enumerate(X_input.columns):\n",
    "        if rfe.support_[i]:\n",
    "            selected_features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "customRFECV(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "\n",
    "fileName = '_selectedFeaturesRFECV_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "\n",
    "with open(fileName + '.json', 'w') as f:\n",
    "    json.dump(selected_features, f, indent=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to open\n",
    "\n",
    "fileName = '_selectedFeaturesRFECV_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "\n",
    "with open(fileName + '.json', 'r') as f:\n",
    "    selected_features = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;Transforming Distribution&#x27;,\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                (&#x27;Standard Scaler&#x27;, StandardScaler()), (&#x27;Model&#x27;, None)])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;Transforming Distribution&#x27;,\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                (&#x27;Standard Scaler&#x27;, StandardScaler()), (&#x27;Model&#x27;, None)])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler(quantile_range=(25, 75))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">None</label><div class=\"sk-toggleable__content\"><pre>None</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('Transforming Distribution',\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                ('Standard Scaler', StandardScaler()), ('Model', None)])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make an empty sklearn pipeline without a classifier\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Transforming Distribution',  RobustScaler(with_centering = True, with_scaling = True, quantile_range=(25, 75), unit_variance = False)),\n",
    "    ('Standard Scaler', StandardScaler()),\n",
    "    ('Model', None),\n",
    "])\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with training data\n",
    "\n",
    "df = dfTrain.copy(deep = True)\n",
    "\n",
    "categorical_features,continuous_features, binary_features = columns_catNumOrBin(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "\n",
    "df = dfTrain.copy(deep = True)\n",
    "\n",
    "# filtered proteins based on RFE above\n",
    "fileName = '_selectedFeaturesRFECV_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "\n",
    "with open(fileName + '.json', 'r') as f:\n",
    "    selected_featuresFromDisk = json.load(f)\n",
    "\n",
    "colsToFilter = []\n",
    "colsToFilter = copy.deepcopy(selected_featuresFromDisk)\n",
    "colsToFilter.append(target)\n",
    "df = df[colsToFilter] # selected features from RFE\n",
    "\n",
    "X, y = X_y_split(df, target)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = pd.Series(le.fit_transform(y))\n",
    "\n",
    "categorical_features,continuous_features, binary_features = columns_catNumOrBin(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>geneNamePrimary</th>\n",
       "      <th>ACTBL2</th>\n",
       "      <th>ACTG1</th>\n",
       "      <th>ACTG2</th>\n",
       "      <th>ACTN1</th>\n",
       "      <th>ACTN4</th>\n",
       "      <th>ADAM10</th>\n",
       "      <th>ADAMTS13</th>\n",
       "      <th>ADIPOQ</th>\n",
       "      <th>AFM</th>\n",
       "      <th>AGT</th>\n",
       "      <th>...</th>\n",
       "      <th>SVEP1</th>\n",
       "      <th>TFRC</th>\n",
       "      <th>THBS1</th>\n",
       "      <th>TLN2</th>\n",
       "      <th>TPM3</th>\n",
       "      <th>TUBA8</th>\n",
       "      <th>UGT8</th>\n",
       "      <th>VCL</th>\n",
       "      <th>VIM</th>\n",
       "      <th>VWF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-0.423379</td>\n",
       "      <td>-0.024570</td>\n",
       "      <td>0.026106</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-2.440891</td>\n",
       "      <td>-0.395340</td>\n",
       "      <td>0.244759</td>\n",
       "      <td>0.418586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349093</td>\n",
       "      <td>1.513985</td>\n",
       "      <td>-1.410128</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>1.736182</td>\n",
       "      <td>-1.788465</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>1.481474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.628846</td>\n",
       "      <td>0.946476</td>\n",
       "      <td>0.874163</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-2.440891</td>\n",
       "      <td>-2.138182</td>\n",
       "      <td>0.372256</td>\n",
       "      <td>0.003020</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.110371</td>\n",
       "      <td>0.082788</td>\n",
       "      <td>-1.410128</td>\n",
       "      <td>-1.141835</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>1.688060</td>\n",
       "      <td>-0.647795</td>\n",
       "      <td>0.764930</td>\n",
       "      <td>1.111831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-0.571826</td>\n",
       "      <td>-0.477617</td>\n",
       "      <td>-0.717801</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-1.724302</td>\n",
       "      <td>0.106397</td>\n",
       "      <td>-0.695381</td>\n",
       "      <td>0.194567</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.110371</td>\n",
       "      <td>-0.506163</td>\n",
       "      <td>-0.164129</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-0.193621</td>\n",
       "      <td>1.885809</td>\n",
       "      <td>-1.788465</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>1.010081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>-0.873302</td>\n",
       "      <td>-0.910295</td>\n",
       "      <td>-0.877917</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-2.440891</td>\n",
       "      <td>-2.138182</td>\n",
       "      <td>0.555990</td>\n",
       "      <td>0.729403</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.110371</td>\n",
       "      <td>0.240194</td>\n",
       "      <td>-1.410128</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>1.447036</td>\n",
       "      <td>-1.788465</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>0.326443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>-0.530581</td>\n",
       "      <td>-0.944076</td>\n",
       "      <td>-0.699827</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-1.064995</td>\n",
       "      <td>-1.090575</td>\n",
       "      <td>-0.280086</td>\n",
       "      <td>0.113807</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.110371</td>\n",
       "      <td>-1.407680</td>\n",
       "      <td>0.330266</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>1.471968</td>\n",
       "      <td>-1.788465</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>1.436516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-1.338389</td>\n",
       "      <td>-0.779894</td>\n",
       "      <td>-1.079842</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-2.440891</td>\n",
       "      <td>-0.914108</td>\n",
       "      <td>0.179224</td>\n",
       "      <td>0.156814</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.245765</td>\n",
       "      <td>0.039113</td>\n",
       "      <td>-0.862686</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>1.432024</td>\n",
       "      <td>-1.788465</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>0.045334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-1.749931</td>\n",
       "      <td>-0.526616</td>\n",
       "      <td>-0.639962</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-2.440891</td>\n",
       "      <td>-2.138182</td>\n",
       "      <td>0.164797</td>\n",
       "      <td>0.325150</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.110371</td>\n",
       "      <td>0.134644</td>\n",
       "      <td>0.522581</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>0.030291</td>\n",
       "      <td>-1.001484</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>0.053385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>-0.667611</td>\n",
       "      <td>-0.169866</td>\n",
       "      <td>-0.128379</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-2.440891</td>\n",
       "      <td>-2.138182</td>\n",
       "      <td>0.152263</td>\n",
       "      <td>-1.038392</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569083</td>\n",
       "      <td>0.925464</td>\n",
       "      <td>0.394859</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>0.030291</td>\n",
       "      <td>-1.788465</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>0.231407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.758497</td>\n",
       "      <td>-0.726377</td>\n",
       "      <td>-0.669988</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-2.440891</td>\n",
       "      <td>-2.138182</td>\n",
       "      <td>0.541622</td>\n",
       "      <td>-0.894811</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.110371</td>\n",
       "      <td>-0.971333</td>\n",
       "      <td>-0.903362</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>1.257198</td>\n",
       "      <td>-1.788465</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>-2.207381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-0.249093</td>\n",
       "      <td>-0.603873</td>\n",
       "      <td>-0.377728</td>\n",
       "      <td>-1.265311</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-2.440891</td>\n",
       "      <td>-1.255815</td>\n",
       "      <td>-1.160970</td>\n",
       "      <td>-0.060001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.454097</td>\n",
       "      <td>-0.305702</td>\n",
       "      <td>-1.410128</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>1.619110</td>\n",
       "      <td>-1.788465</td>\n",
       "      <td>0.282797</td>\n",
       "      <td>0.678333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 239 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "geneNamePrimary    ACTBL2     ACTG1     ACTG2     ACTN1     ACTN4    ADAM10  \\\n",
       "110             -0.423379 -0.024570  0.026106 -1.836067 -1.606185 -1.860046   \n",
       "146              0.628846  0.946476  0.874163 -1.836067 -1.606185 -1.860046   \n",
       "126             -0.571826 -0.477617 -0.717801 -1.836067 -1.606185 -1.860046   \n",
       "121             -0.873302 -0.910295 -0.877917 -1.836067 -1.606185 -1.860046   \n",
       "152             -0.530581 -0.944076 -0.699827 -1.836067 -1.606185 -1.860046   \n",
       "..                    ...       ...       ...       ...       ...       ...   \n",
       "86              -1.338389 -0.779894 -1.079842 -1.836067 -1.606185 -1.860046   \n",
       "102             -1.749931 -0.526616 -0.639962 -1.836067 -1.606185 -1.860046   \n",
       "175             -0.667611 -0.169866 -0.128379 -1.836067 -1.606185 -1.860046   \n",
       "96              -0.758497 -0.726377 -0.669988 -1.836067 -1.606185 -1.860046   \n",
       "156             -0.249093 -0.603873 -0.377728 -1.265311 -1.606185 -1.860046   \n",
       "\n",
       "geneNamePrimary  ADAMTS13    ADIPOQ       AFM       AGT  ...     SVEP1  \\\n",
       "110             -2.440891 -0.395340  0.244759  0.418586  ... -0.349093   \n",
       "146             -2.440891 -2.138182  0.372256  0.003020  ... -2.110371   \n",
       "126             -1.724302  0.106397 -0.695381  0.194567  ... -2.110371   \n",
       "121             -2.440891 -2.138182  0.555990  0.729403  ... -2.110371   \n",
       "152             -1.064995 -1.090575 -0.280086  0.113807  ... -2.110371   \n",
       "..                    ...       ...       ...       ...  ...       ...   \n",
       "86              -2.440891 -0.914108  0.179224  0.156814  ... -1.245765   \n",
       "102             -2.440891 -2.138182  0.164797  0.325150  ... -2.110371   \n",
       "175             -2.440891 -2.138182  0.152263 -1.038392  ... -0.569083   \n",
       "96              -2.440891 -2.138182  0.541622 -0.894811  ... -2.110371   \n",
       "156             -2.440891 -1.255815 -1.160970 -0.060001  ... -0.454097   \n",
       "\n",
       "geneNamePrimary      TFRC     THBS1      TLN2      TPM3     TUBA8      UGT8  \\\n",
       "110              1.513985 -1.410128 -1.974323 -1.859943 -1.954009  1.736182   \n",
       "146              0.082788 -1.410128 -1.141835 -1.859943 -1.954009  1.688060   \n",
       "126             -0.506163 -0.164129 -1.974323 -1.859943 -0.193621  1.885809   \n",
       "121              0.240194 -1.410128 -1.974323 -1.859943 -1.954009  1.447036   \n",
       "152             -1.407680  0.330266 -1.974323 -1.859943 -1.954009  1.471968   \n",
       "..                    ...       ...       ...       ...       ...       ...   \n",
       "86               0.039113 -0.862686 -1.974323 -1.859943 -1.954009  1.432024   \n",
       "102              0.134644  0.522581 -1.974323 -1.859943 -1.954009  0.030291   \n",
       "175              0.925464  0.394859 -1.974323 -1.859943 -1.954009  0.030291   \n",
       "96              -0.971333 -0.903362 -1.974323 -1.859943 -1.954009  1.257198   \n",
       "156             -0.305702 -1.410128 -1.974323 -1.859943 -1.954009  1.619110   \n",
       "\n",
       "geneNamePrimary       VCL       VIM       VWF  \n",
       "110             -1.788465 -1.030240  1.481474  \n",
       "146             -0.647795  0.764930  1.111831  \n",
       "126             -1.788465 -1.030240  1.010081  \n",
       "121             -1.788465 -1.030240  0.326443  \n",
       "152             -1.788465 -1.030240  1.436516  \n",
       "..                    ...       ...       ...  \n",
       "86              -1.788465 -1.030240  0.045334  \n",
       "102             -1.001484 -1.030240  0.053385  \n",
       "175             -1.788465 -1.030240  0.231407  \n",
       "96              -1.788465 -1.030240 -2.207381  \n",
       "156             -1.788465  0.282797  0.678333  \n",
       "\n",
       "[107 rows x 239 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# impute 0s, as above\n",
    "\n",
    "imputeWideDFMinOr0(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of classification algorithms\n",
    "\n",
    "classifiers = [\n",
    "    ('HistGradientBoostingClassifier', HistGradientBoostingClassifier(random_state = seed)),\n",
    "    ('CatBoostClassifier', CatBoostClassifier(random_state = seed, verbose = False)),\n",
    "    ('LGBMClassifier', LGBMClassifier(random_state = seed, verbosity = -1)),\n",
    "    ('XGBClassifier', XGBClassifier(random_state = seed)),\n",
    "    ('AdaBoostClassifier', AdaBoostClassifier(random_state = seed)),\n",
    "    ('RandomForestClassifier', RandomForestClassifier(random_state = seed)),\n",
    "    ('BaggingClassifier', BaggingClassifier(random_state = seed)),\n",
    "    ('ExtraTreesClassifier', ExtraTreesClassifier(random_state = seed)),\n",
    "    ('GradientBoostingClassifier', GradientBoostingClassifier(random_state = seed)),\n",
    "    ('SVC', SVC(random_state = seed)),\n",
    "    ('MLPClassifier', MLPClassifier(random_state = seed)),\n",
    "    ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=3)),\n",
    "    ('GaussianProcessClassifier', GaussianProcessClassifier(random_state = seed)),\n",
    "    ('DecisionTreeClassifier', DecisionTreeClassifier(random_state = seed)),\n",
    "    ('GaussianNB', GaussianNB()),\n",
    "    ('QuadraticDiscriminantAnalysis', QuadraticDiscriminantAnalysis()),\n",
    "    ('LinearSVC', LinearSVC(random_state=seed)),\n",
    "    ('RidgeClassifier', RidgeClassifier(random_state = seed)),\n",
    "    ('SGDClassifier', SGDClassifier(random_state = seed)),\n",
    "    ('XGBClassifier', XGBClassifier(random_state = seed)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# perform cross validation on every model in the list. Use of a network computer cluster may be required\n",
    "\n",
    "print('\\nCross-Validation:')\n",
    "\n",
    "for j, (name, clf) in enumerate(classifiers):\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresCrossValScore = []\n",
    "    r2_scores = []\n",
    "    pipeline.set_params(Model = clf)\n",
    "    \n",
    "    print('\\n')\n",
    "    print(f'\\n{name} Classifier:\\n')\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        \n",
    "        #print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa: {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use Optuna to individually tune each model\n",
    "* score based on Matthew Correlation Coefficient (MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optuna logger\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# CatBoostClassifier, from https://catboost.ai/docs/concepts/python-reference_catboostclassifier\n",
    "\n",
    "def objectiveCatBoostClassifier(trial):\n",
    "    \n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "        'iterations': 500,\n",
    "        'depth': trial.suggest_int('depth', 1, 10),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.05, 1.0),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.1, 10),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0.1, 10),\n",
    "        'verbose': False,\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),\n",
    "        'leaf_estimation_method': trial.suggest_categorical('leaf_estimation_method', ['Newton', 'Gradient']),\n",
    "        'eval_metric': 'MCC'\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = CatBoostClassifier(**params, random_state = seed))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyCatBoostClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyCatBoostClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyCatBoostClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyCatBoostClassifier, load_if_exists=True)\n",
    "studyCatBoostClassifier.optimize(objectiveCatBoostClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsCatBoostClassifier = studyCatBoostClassifier.best_params\n",
    "best_rmse_scoreCatBoostClassifier = studyCatBoostClassifier.best_value\n",
    "\n",
    "print(f'\\nCatBoostClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyCatBoostClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreCatBoostClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsCatBoostClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyCatBoostClassifier, 'CatBoostClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyCatBoostClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Histogram-based Gradient Boosting\n",
    "\n",
    "def objectiveHistGradientBoostingClassifier(trial):\n",
    "    params = {\n",
    "        'loss': trial.suggest_categorical('loss', ['log_loss']),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001,0.1),\n",
    "        'max_iter': trial.suggest_categorical('max_iter', [1000]),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10,200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 30),\n",
    "        'max_bins': trial.suggest_int('max_bins', 100, 255),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5,100),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 1e-10,10.0),\n",
    "        'class_weight': trial.suggest_categorical('class_weight', ['balanced', None])\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = HistGradientBoostingClassifier(**params, random_state = seed))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC \n",
    "         \n",
    "studyName = 'studyHistGradientBoostingClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyHistGradientBoostingClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyHistGradientBoostingClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyHistGradientBoostingClassifier, load_if_exists=True)\n",
    "studyHistGradientBoostingClassifier.optimize(objectiveHistGradientBoostingClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsHistGradientBoostingClassifier = studyHistGradientBoostingClassifier.best_params\n",
    "best_rmse_scoreHistGradientBoostingClassifier = studyHistGradientBoostingClassifier.best_value\n",
    "\n",
    "print(f'\\nHistGradientBoostingClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyHistGradientBoostingClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreHistGradientBoostingClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsHistGradientBoostingClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyHistGradientBoostingClassifier, 'HistGradientBoostingClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyHistGradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# AdaBoostClassifier optimization\n",
    "\n",
    "def objectiveAdaBoostClassifier(trial):\n",
    "    \n",
    "    params = { # this is from sklearn\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1, 200),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.0001, 0.5),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = AdaBoostClassifier(**params, random_state = seed))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "\n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC  \n",
    "\n",
    "studyName = 'studyAdaBoostClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyAdaBoostClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyAdaBoostClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyAdaBoostClassifier, load_if_exists=True)\n",
    "studyAdaBoostClassifier.optimize(objectiveAdaBoostClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsAdaBoostClassifier = studyAdaBoostClassifier.best_params\n",
    "best_rmse_scoreAdaBoostClassifier = studyAdaBoostClassifier.best_value\n",
    "\n",
    "print(f'\\nAdaBoostClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyAdaBoostClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreAdaBoostClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsAdaBoostClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyAdaBoostClassifier, 'AdaBoostClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyAdaBoostClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LGBMClassifier\n",
    "\n",
    "def objectiveLGBMClassifier(trial):\n",
    "    \n",
    "    params = {\n",
    "        'metric': 'rmse',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 20000),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt','dart']),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 10),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 1, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.0001, 0.5),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 100),\n",
    "        #\"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0, 15),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n",
    "        'min_data_per_groups' : trial.suggest_int('min_data_per_groups', 1, 100),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = LGBMClassifier(**params, random_state = seed, n_jobs = nJobs, verbosity = -1))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreAccuracy\n",
    "\n",
    "studyName = 'studyLGBMClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyLGBMClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyLGBMClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyLGBMClassifier, load_if_exists=True)\n",
    "studyLGBMClassifier.optimize(objectiveLGBMClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsLGBMClassifier = studyLGBMClassifier.best_params\n",
    "best_rmse_scoreLGBMClassifier = studyLGBMClassifier.best_value\n",
    "\n",
    "print(f'\\nLGBMClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyLGBMClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreLGBMClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsLGBMClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyLGBMClassifier, 'LGBMClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyLGBMClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "\n",
    "def objectiveRandomForestClassifier(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 2, 500),\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 110),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 30, 10000),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = RandomForestClassifier(**params, random_state = seed, n_jobs = nJobs))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "         \n",
    "studyName = 'studyRandomForestClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyRandomForestClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyRandomForestClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyRandomForestClassifier, load_if_exists=True)\n",
    "studyRandomForestClassifier.optimize(objectiveRandomForestClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "# this is optuna\n",
    "\n",
    "best_paramsRandomForestClassifier = studyRandomForestClassifier.best_params\n",
    "best_rmse_scoreRandomForestClassifier = studyRandomForestClassifier.best_value\n",
    "\n",
    "print(f'\\nRandomForestClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyRandomForestClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreRandomForestClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsRandomForestClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyRandomForestClassifier, 'RandomForestClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyRandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# BaggingClassifier\n",
    "\n",
    "def objectiveBaggingClassifier(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 2, 200),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = BaggingClassifier(**params, random_state = seed, n_jobs = nJobs))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyBaggingClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyBaggingClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyBaggingClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyBaggingClassifier, load_if_exists=True)\n",
    "studyBaggingClassifier.optimize(objectiveBaggingClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsBaggingClassifier = studyBaggingClassifier.best_params\n",
    "best_rmse_scoreBaggingClassifier = studyBaggingClassifier.best_value\n",
    "\n",
    "print(f'\\nBaggingClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyBaggingClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreBaggingClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsBaggingClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyBaggingClassifier, 'BaggingClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyBaggingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# ExtraTreesClassifier\n",
    "\n",
    "def objectiveExtraTreesClassifier(trial):\n",
    "    params = { # these come from SKLearn\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 2, 400),\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 9),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10, 10000),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "        'min_impurity_decrease': trial.suggest_float('min_impurity_decrease',0.0, 0.01),\n",
    "        'min_weight_fraction_leaf': trial.suggest_uniform('min_weight_fraction_leaf', 0.00001,0.4),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = ExtraTreesClassifier(**params, random_state = seed, n_jobs = nJobs))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyExtraTreesClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyExtraTreesClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyExtraTreesClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyExtraTreesClassifier, load_if_exists=True)\n",
    "studyExtraTreesClassifier.optimize(objectiveExtraTreesClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsExtraTreesClassifier = studyExtraTreesClassifier.best_params\n",
    "best_rmse_scoreExtraTreesClassifier = studyExtraTreesClassifier.best_value\n",
    "\n",
    "print(f'\\nExtraTreesClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyExtraTreesClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreExtraTreesClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsExtraTreesClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyExtraTreesClassifier, 'ExtraTreesClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyExtraTreesClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier\n",
    "\n",
    "def objectiveGradientBoostingClassifier(trial):\n",
    "    params = {\n",
    "        'loss': trial.suggest_categorical('loss', ['log_loss', 'exponential']),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.0001, 0.5),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 20, 1000),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0), # fussy\n",
    "        'criterion': trial.suggest_categorical('criterion', ['friedman_mse', 'squared_error']),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10, 10000),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = GradientBoostingClassifier(**params, random_state = seed))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyGradientBoostingClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyGradientBoostingClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyGradientBoostingClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyGradientBoostingClassifier, load_if_exists=True)\n",
    "studyGradientBoostingClassifier.optimize(objectiveGradientBoostingClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsGradientBoostingClassifier = studyGradientBoostingClassifier.best_params\n",
    "best_rmse_scoreGradientBoostingClassifier = studyGradientBoostingClassifier.best_value\n",
    "\n",
    "print(f'\\nGradientBoostingClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyGradientBoostingClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreGradientBoostingClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsGradientBoostingClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyGradientBoostingClassifier, 'GradientBoostingClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyGradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# SVC\n",
    "\n",
    "def objectiveSVC(trial):\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 1e-10, 1e10, log=True),\n",
    "        'kernel': trial.suggest_categorical('kernel', ['rbf', 'linear', 'poly', 'sigmoid']),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),\n",
    "        'shrinking': trial.suggest_categorical('shrinking', [True, False]),\n",
    "        'probability': trial.suggest_categorical('probability', [True, False]),\n",
    "        'decision_function_shape': trial.suggest_categorical('decision_function_shape', ['ovo', 'ovr']),\n",
    "        'probability': True\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = SVC(**params, random_state = seed))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = [] # new!\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studySVC_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudySVC = 'sqlite:///' + studyName +'.db'\n",
    "studySVC = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudySVC, load_if_exists=True)\n",
    "studySVC.optimize(objectiveSVC, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# SVC\n",
    "\n",
    "best_paramsSVC = studySVC.best_params\n",
    "best_rmse_scoreSVC = studySVC.best_value\n",
    "\n",
    "print(f'\\nSVC:\\n')\n",
    "print('Number of finished trials:', len(studySVC.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreSVC} \\n')\n",
    "print(f'\\n Best Params = {best_paramsSVC} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studySVC, 'SVCTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studySVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# MLPClassifier\n",
    "\n",
    "def objectiveMLPClassifier(trial):\n",
    "    params = { # these come from SKLearn\n",
    "        'activation': trial.suggest_categorical('activation', ['identity', 'logistic', 'tanh', 'relu']),\n",
    "        'solver': trial.suggest_categorical('solver', ['lbfgs', 'sgd', 'adam']),\n",
    "        'alpha': trial.suggest_float('alpha', 0.00001, 0.01),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', ['constant', 'invscaling', 'adaptive']),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = MLPClassifier(**params, random_state = seed))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyMLPClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyMLPClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyMLPClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyMLPClassifier, load_if_exists=True)\n",
    "studyMLPClassifier.optimize(objectiveMLPClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsMLPClassifier = studyMLPClassifier.best_params\n",
    "best_rmse_scoreMLPClassifier = studyMLPClassifier.best_value\n",
    "\n",
    "print(f'\\nMLPClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyMLPClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreMLPClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsMLPClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyMLPClassifier, 'MLPClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyMLPClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# KNeighborsClassifier\n",
    "\n",
    "def objectiveKNeighborsClassifier(trial):\n",
    "    params = {\n",
    "        'n_neighbors': trial.suggest_int('n_neighbors', 1,30),\n",
    "        'weights': trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        'metric': trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        'algorithm': trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),\n",
    "        'leaf_size': trial.suggest_int('leaf_size', 5,100),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = KNeighborsClassifier(**params, n_jobs = nJobs))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyKNeighborsClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyKNeighborsClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyKNeighborsClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyKNeighborsClassifier, load_if_exists=True)\n",
    "studyKNeighborsClassifier.optimize(objectiveKNeighborsClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsKNeighborsClassifier = studyKNeighborsClassifier.best_params\n",
    "best_rmse_scoreKNeighborsClassifier = studyKNeighborsClassifier.best_value\n",
    "\n",
    "print(f'\\nKNeighborsClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyKNeighborsClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreKNeighborsClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsKNeighborsClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyKNeighborsClassifier, 'KNeighborsClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyKNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gaussianProcessClassifier has no parameters to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier\n",
    "\n",
    "def objectiveDecisionTreeClassifier(trial):\n",
    "    params = {\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "        'splitter': trial.suggest_categorical('splitter', ['best', 'random']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 6),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 1000),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 2, 10000),\n",
    "        'min_weight_fraction_leaf': trial.suggest_uniform('min_weight_fraction_leaf', 0.0, 0.5), \n",
    "        'ccp_alpha': trial.suggest_float('ccp_alpha', 0.00000001, 1.0, log=True),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = DecisionTreeClassifier(**params, random_state = seed))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = [] # new!\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyDecisionTreeClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyDecisionTreeClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyDecisionTreeClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyDecisionTreeClassifier, load_if_exists=True)\n",
    "studyDecisionTreeClassifier.optimize(objectiveDecisionTreeClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsDecisionTreeClassifier = studyDecisionTreeClassifier.best_params\n",
    "best_rmse_scoreDecisionTreeClassifier = studyDecisionTreeClassifier.best_value\n",
    "\n",
    "print(f'\\nDecisionTreeClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyDecisionTreeClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreDecisionTreeClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsDecisionTreeClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyDecisionTreeClassifier, 'DecisionTreeClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyDecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# GaussianNB\n",
    "\n",
    "def objectiveGaussianNB(trial):\n",
    "    params = {\n",
    "        'var_smoothing': trial.suggest_float('var_smoothing', 1e-13, 1, log = True),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = GaussianNB(**params))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyGaussianNB_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyGaussianNB = 'sqlite:///' + studyName +'.db'\n",
    "studyGaussianNB = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyGaussianNB, load_if_exists=True)\n",
    "studyGaussianNB.optimize(objectiveGaussianNB, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsGaussianNB = studyGaussianNB.best_params\n",
    "best_rmse_scoreGaussianNB = studyGaussianNB.best_value\n",
    "\n",
    "print(f'\\nGaussianNB:\\n')\n",
    "print('Number of finished trials:', len(studyGaussianNB.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreGaussianNB} \\n')\n",
    "print(f'\\n Best Params = {best_paramsGaussianNB} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyGaussianNB, 'GaussianNBTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyGaussianNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# QuadraticDiscriminantAnalysis\n",
    "\n",
    "def objectiveQuadraticDiscriminantAnalysis(trial):\n",
    "    params = {\n",
    "        'reg_param': trial.suggest_float('reg_param', 0, 1),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = QuadraticDiscriminantAnalysis(**params))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = [] # new!\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyQuadraticDiscriminantAnalysis_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyQuadraticDiscriminantAnalysis = 'sqlite:///' + studyName +'.db'\n",
    "studyQuadraticDiscriminantAnalysis = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyQuadraticDiscriminantAnalysis, load_if_exists=True)\n",
    "studyQuadraticDiscriminantAnalysis.optimize(objectiveQuadraticDiscriminantAnalysis, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# QuadraticDiscriminantAnalysis\n",
    "\n",
    "best_paramsQuadraticDiscriminantAnalysis = studyQuadraticDiscriminantAnalysis.best_params\n",
    "best_rmse_scoreQuadraticDiscriminantAnalysis = studyQuadraticDiscriminantAnalysis.best_value\n",
    "\n",
    "print(f'\\nQuadraticDiscriminantAnalysis:\\n')\n",
    "print('Number of finished trials:', len(studyQuadraticDiscriminantAnalysis.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreQuadraticDiscriminantAnalysis} \\n')\n",
    "print(f'\\n Best Params = {best_paramsQuadraticDiscriminantAnalysis} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyQuadraticDiscriminantAnalysis, 'QuadraticDiscriminantAnalysisTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyQuadraticDiscriminantAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# LinearSVC\n",
    "\n",
    "def objectiveLinearSVC(trial):\n",
    "    params = {\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
    "        'loss': trial.suggest_categorical('loss', ['hinge', 'squared_hinge']),\n",
    "        'dual': trial.suggest_categorical('dual', ['auto', True]),\n",
    "        'C': trial.suggest_float('C', 1e-10, 1e10, log=True),\n",
    "        'intercept_scaling': trial.suggest_float('intercept_scaling', 0.1, 2),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = CalibratedClassifierCV(LinearSVC(**params, random_state = seed)))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = [] # new!\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyLinearSVC_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyLinearSVC = 'sqlite:///' + studyName +'.db'\n",
    "studyLinearSVC = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyLinearSVC, load_if_exists=True)\n",
    "studyLinearSVC.optimize(objectiveLinearSVC, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning, ValueError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsLinearSVC = studyLinearSVC.best_params\n",
    "best_rmse_scoreLinearSVC = studyLinearSVC.best_value\n",
    "\n",
    "print(f'\\nLinearSVC:\\n')\n",
    "print('Number of finished trials:', len(studyLinearSVC.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreLinearSVC} \\n')\n",
    "print(f'\\n Best Params = {best_paramsLinearSVC} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyLinearSVC, 'LinearSVCTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyLinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# RidgeClassifier\n",
    "\n",
    "def objectiveRidgeClassifier(trial):\n",
    "    params = {\n",
    "        'alpha': trial.suggest_float('alpha', 0, 10),\n",
    "        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "        'solver': trial.suggest_categorical('solver', ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = RidgeClassifier(**params, random_state = seed))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyRidgeClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyRidgeClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyRidgeClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyRidgeClassifier, load_if_exists=True)\n",
    "studyRidgeClassifier.optimize(objectiveRidgeClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# RidgeClassifier\n",
    "\n",
    "best_paramsRidgeClassifier = studyRidgeClassifier.best_params\n",
    "best_rmse_scoreRidgeClassifier = studyRidgeClassifier.best_value\n",
    "\n",
    "print(f'\\nRidgeClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyRidgeClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreRidgeClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsRidgeClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyRidgeClassifier, 'RidgeClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyRidgeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# SGDClassifier\n",
    "\n",
    "def objectiveSGDClassifier(trial):\n",
    "    params = {\n",
    "        'loss': trial.suggest_categorical('loss', ['log_loss', 'modified_huber']),\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l2', 'l1', 'elasticnet']),\n",
    "        'alpha': trial.suggest_float('alpha', 0.00001, 0.001, log=True),\n",
    "        'l1_ratio': trial.suggest_float('l1_ratio', 0.05, 0.95),\n",
    "        'power_t': trial.suggest_float('power_t', -2, 2),\n",
    "        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = SGDClassifier(**params, random_state = seed, n_jobs = nJobs))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = [] # new!\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studySGDClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudySGDClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studySGDClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudySGDClassifier, load_if_exists=True)\n",
    "studySGDClassifier.optimize(objectiveSGDClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsSGDClassifier = studySGDClassifier.best_params\n",
    "best_rmse_scoreSGDClassifier = studySGDClassifier.best_value\n",
    "\n",
    "print(f'\\nSGDClassifier:\\n')\n",
    "print('Number of finished trials:', len(studySGDClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreSGDClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsSGDClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studySGDClassifier, 'SGDClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studySGDClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# XGBClassifier\n",
    "\n",
    "def objectiveXGBClassifier(trial):\n",
    "    params = {\n",
    "        'booster': trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        'n_estimators': trial.suggest_int(\"n_estimators\", 1, 150),\n",
    "        'learning_rate': trial.suggest_uniform('learning_rate', 0.0000001, 1),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.0001, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if params[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        params[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        params[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if params[\"booster\"] == \"dart\":\n",
    "        params[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        params[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        params[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        params[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "        \n",
    "    pipeline.set_params(Model = XGBClassifier(**params, random_state = seed, nthread = nJobs))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = [] # new!\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC \n",
    "\n",
    "studyName = 'studyXGBClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyXGBClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyXGBClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyXGBClassifier, load_if_exists=True)\n",
    "studyXGBClassifier.optimize(objectiveXGBClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsXGBClassifier = studyXGBClassifier.best_params\n",
    "best_rmse_scoreXGBClassifier = studyXGBClassifier.best_value\n",
    "\n",
    "print(f'\\nXGBClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyXGBClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreXGBClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsXGBClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyXGBClassifier, 'XGBClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyXGBClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate each tuned classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optuna tuned classifiers AS PIPELINES\n",
    "tunedClassifierList = []\n",
    "\n",
    "CatBoostClassifierTuned = joblib.load('CatBoostClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "HistGradientBoostingClassifierTuned = joblib.load('HistGradientBoostingClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "LGBMClassifierTuned = joblib.load('LGBMClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "AdaBoostClassifierTuned = joblib.load('AdaBoostClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "RandomForestClassifierTuned = joblib.load('RandomForestClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "BaggingClassifierTuned = joblib.load('BaggingClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')#ExtraTreesClassifierTuned = joblib.load('ExtraTreesClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "ExtraTreesClassifierTuned = joblib.load('ExtraTreesClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')#ExtraTreesClassifierTuned = joblib.load('ExtraTreesClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "GradientBoostingClassifierTuned = joblib.load('GradientBoostingClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "SVCTuned = joblib.load('SVCTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "MLPClassifierTuned = joblib.load('MLPClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "KNeighborsClassifierTuned = joblib.load('KNeighborsClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "GaussianProcessClassifierTuned = GaussianProcessClassifier(random_state = seed) #has nothing to tune\n",
    "DecisionTreeClassifierTuned = joblib.load('DecisionTreeClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "GaussianNBTuned = joblib.load('GaussianNBTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "QuadraticDiscriminantAnalysisTuned = joblib.load('QuadraticDiscriminantAnalysisTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "LinearSVCTuned = joblib.load('LinearSVCTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "RidgeClassifierTuned = joblib.load('RidgeClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "SGDClassifierTuned = joblib.load('SGDClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "XGBClassifierTuned = joblib.load('XGBClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists to summarize results\n",
    "\n",
    "tunedClassifierList = []\n",
    "seedList = []\n",
    "trainFracList = []\n",
    "mean_scoreKappaTunedClassifierList = []\n",
    "mean_scoreMCCTunedClassifierList= []\n",
    "mean_scoreF1WeightedTunedClassifierList = []\n",
    "mean_scoreAccuracyTunedClassifierList = []\n",
    "mean_scoreROCTunedClassifierList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# catboost with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned CatBoostClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = CatBoostClassifier(**CatBoostClassifierTuned.best_params,random_state = seed, verbose = False))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "    feature_importance.append(pipeline[-1].feature_importances_)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('CatBoostClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# HistGradientBoostingClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned HistGradientBoostingClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = HistGradientBoostingClassifier(**HistGradientBoostingClassifierTuned.best_params,random_state = seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('HistGradientBoostingClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# LGBMClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned LGBMClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = LGBMClassifier(**LGBMClassifierTuned.best_params,random_state = seed, n_jobs=nJobs, verbosity = -1))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "    feature_importance.append(pipeline[-1].feature_importances_)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('LGBMClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# AdaBoostClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned AdaBoostClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = AdaBoostClassifier(**AdaBoostClassifierTuned.best_params,random_state = seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "    feature_importance.append(pipeline[-1].feature_importances_)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('AdaBoostClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# RandomForestClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned RandomForestClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = RandomForestClassifier(**RandomForestClassifierTuned.best_params,random_state = seed, n_jobs=nJobs))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "    feature_importance.append(pipeline[-1].feature_importances_)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('RandomForestClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# BaggingClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned BaggingClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = BaggingClassifier(**BaggingClassifierTuned.best_params,random_state = seed, n_jobs=nJobs))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('BaggingClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# ExtraTreesClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned ExtraTreesClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = ExtraTreesClassifier(**ExtraTreesClassifierTuned.best_params,random_state = seed, n_jobs=nJobs))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "    feature_importance.append(pipeline[-1].feature_importances_)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('ExtraTreesClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned GradientBoostingClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = GradientBoostingClassifier(**GradientBoostingClassifierTuned.best_params,random_state = seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "    feature_importance.append(pipeline[-1].feature_importances_)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('GradientBoostingClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# SVC with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned SVC Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = SVC(**SVCTuned.best_params,random_state = seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('SVC')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# MLPClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned MLPClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = MLPClassifier(**MLPClassifierTuned.best_params,random_state = seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('MLPClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# KNeighborsClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned KNeighborsClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = KNeighborsClassifier(**KNeighborsClassifierTuned.best_params, n_jobs=nJobs))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('KNeighborsClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# GaussianProcessClassifier, which has no parameters to optimize\n",
    "\n",
    "print('\\nTuned gaussianProcessClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = GaussianProcessClassifier(random_state = seed, n_jobs=nJobs))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('GaussianProcessClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned DecisionTreeClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = DecisionTreeClassifier(**DecisionTreeClassifierTuned.best_params,random_state = seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('DecisionTreeClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# GaussianNB with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned GaussianNB Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = GaussianNB(**GaussianNBTuned.best_params))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('GaussianNB')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# QuadraticDiscriminantAnalysis with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned QuadraticDiscriminantAnalysis Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = QuadraticDiscriminantAnalysis(**QuadraticDiscriminantAnalysisTuned.best_params))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('QuadraticDiscriminantAnalysis')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# LinearSVC with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned LinearSVC Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = LinearSVC(**LinearSVCTuned.best_params,random_state = seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('LinearSVC')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# RidgeClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned RidgeClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = RidgeClassifier(**RidgeClassifierTuned.best_params,random_state = seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('RidgeClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# SGDClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned SGDClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = SGDClassifier(**SGDClassifierTuned.best_params,random_state = seed, n_jobs=nJobs))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('SGDClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# XGBClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned XGBClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = XGBClassifier(**XGBClassifierTuned.best_params,random_state = seed, n_jobs=nJobs))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('XGBClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the individually tuned classifiers to the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# load from saved\n",
    "dfValidationFromDisk = pd.read_excel('_dfValidation_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.xlsx')\n",
    "df = dfValidationFromDisk\n",
    "\n",
    "y_validationList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only delected features\n",
    "\n",
    "fileName = '_selectedFeaturesRFECV_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "\n",
    "with open(fileName + '.json', 'r') as f:\n",
    "    selected_featuresFromDisk = json.load(f)\n",
    "\n",
    "colsToKeep = []\n",
    "colsToKeep = copy.deepcopy(selected_featuresFromDisk)\n",
    "colsToKeep.append(target)\n",
    "\n",
    "df = df[colsToKeep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(colsToKeep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split into X and Y\n",
    "\n",
    "X_validation, y_validation = X_y_split(df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "11    0\n",
       "12    0\n",
       "13    0\n",
       "14    0\n",
       "15    0\n",
       "16    0\n",
       "17    0\n",
       "18    1\n",
       "19    1\n",
       "20    1\n",
       "21    1\n",
       "22    1\n",
       "23    1\n",
       "24    1\n",
       "25    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_validation = pd.Series(le.fit_transform(y_validation))\n",
    "y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACTBL2</th>\n",
       "      <th>ACTG1</th>\n",
       "      <th>ACTG2</th>\n",
       "      <th>ACTN1</th>\n",
       "      <th>ACTN4</th>\n",
       "      <th>ADAM10</th>\n",
       "      <th>ADAMTS13</th>\n",
       "      <th>ADIPOQ</th>\n",
       "      <th>AFM</th>\n",
       "      <th>AGT</th>\n",
       "      <th>...</th>\n",
       "      <th>SVEP1</th>\n",
       "      <th>TFRC</th>\n",
       "      <th>THBS1</th>\n",
       "      <th>TLN2</th>\n",
       "      <th>TPM3</th>\n",
       "      <th>TUBA8</th>\n",
       "      <th>UGT8</th>\n",
       "      <th>VCL</th>\n",
       "      <th>VIM</th>\n",
       "      <th>VWF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>0.469387</td>\n",
       "      <td>0.266539</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>-1.197036</td>\n",
       "      <td>-0.468998</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.879373</td>\n",
       "      <td>1.106476</td>\n",
       "      <td>-1.722096</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.865699</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.653913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.338129</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-0.484136</td>\n",
       "      <td>-0.574835</td>\n",
       "      <td>-0.747148</td>\n",
       "      <td>-0.894209</td>\n",
       "      <td>0.482403</td>\n",
       "      <td>0.766467</td>\n",
       "      <td>0.521196</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.062470</td>\n",
       "      <td>-1.605370</td>\n",
       "      <td>-0.133370</td>\n",
       "      <td>0.126968</td>\n",
       "      <td>-0.513470</td>\n",
       "      <td>0.113387</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-0.412096</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>0.480118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.190932</td>\n",
       "      <td>-0.665262</td>\n",
       "      <td>-0.489321</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>0.003612</td>\n",
       "      <td>-0.513159</td>\n",
       "      <td>-0.094621</td>\n",
       "      <td>-0.111761</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>0.368662</td>\n",
       "      <td>-0.559181</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.756829</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.757194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-1.046428</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.301385</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>0.202773</td>\n",
       "      <td>0.974240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.989369</td>\n",
       "      <td>0.255144</td>\n",
       "      <td>-0.512598</td>\n",
       "      <td>0.210279</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.319357</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>0.235380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.869840</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-0.161651</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>0.323115</td>\n",
       "      <td>0.388052</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-0.400457</td>\n",
       "      <td>-0.248735</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.778492</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>0.895316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-0.663550</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.373625</td>\n",
       "      <td>-0.442460</td>\n",
       "      <td>-0.502456</td>\n",
       "      <td>0.263057</td>\n",
       "      <td>0.057436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.328864</td>\n",
       "      <td>0.765137</td>\n",
       "      <td>-0.313762</td>\n",
       "      <td>-0.291602</td>\n",
       "      <td>-0.627326</td>\n",
       "      <td>-0.632982</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-0.988735</td>\n",
       "      <td>-0.173581</td>\n",
       "      <td>-0.071144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.495373</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>0.642554</td>\n",
       "      <td>0.355340</td>\n",
       "      <td>-0.482796</td>\n",
       "      <td>-1.838184</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>-0.947442</td>\n",
       "      <td>-0.330764</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-0.647838</td>\n",
       "      <td>0.767015</td>\n",
       "      <td>-0.540684</td>\n",
       "      <td>0.861175</td>\n",
       "      <td>-0.004139</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>1.246629</td>\n",
       "      <td>-0.089003</td>\n",
       "      <td>1.635177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-0.572469</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.092635</td>\n",
       "      <td>-0.300754</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>0.322989</td>\n",
       "      <td>-0.222300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.891231</td>\n",
       "      <td>-0.524184</td>\n",
       "      <td>0.110614</td>\n",
       "      <td>0.074216</td>\n",
       "      <td>-0.122331</td>\n",
       "      <td>0.658714</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-0.592531</td>\n",
       "      <td>-0.510214</td>\n",
       "      <td>0.025982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-0.369941</td>\n",
       "      <td>-0.837724</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-1.014452</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>0.078044</td>\n",
       "      <td>-0.403351</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.341272</td>\n",
       "      <td>0.334208</td>\n",
       "      <td>-0.421917</td>\n",
       "      <td>-1.347796</td>\n",
       "      <td>-0.304606</td>\n",
       "      <td>0.083342</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-0.803889</td>\n",
       "      <td>0.693706</td>\n",
       "      <td>0.200491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.395138</td>\n",
       "      <td>0.466486</td>\n",
       "      <td>0.405952</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-0.697550</td>\n",
       "      <td>0.382139</td>\n",
       "      <td>0.539671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.440379</td>\n",
       "      <td>0.229268</td>\n",
       "      <td>-0.447268</td>\n",
       "      <td>-0.989630</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.524093</td>\n",
       "      <td>-0.728794</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>0.404425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.112768</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-0.088833</td>\n",
       "      <td>-0.350719</td>\n",
       "      <td>-0.331541</td>\n",
       "      <td>-0.341078</td>\n",
       "      <td>0.319447</td>\n",
       "      <td>0.264053</td>\n",
       "      <td>0.197473</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.176952</td>\n",
       "      <td>-0.082222</td>\n",
       "      <td>0.083108</td>\n",
       "      <td>-0.419636</td>\n",
       "      <td>0.138718</td>\n",
       "      <td>-0.049477</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-0.400096</td>\n",
       "      <td>0.952932</td>\n",
       "      <td>1.499098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.680107</td>\n",
       "      <td>1.125549</td>\n",
       "      <td>0.908032</td>\n",
       "      <td>-0.038281</td>\n",
       "      <td>-0.307818</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-0.810231</td>\n",
       "      <td>0.309620</td>\n",
       "      <td>0.569613</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>0.272116</td>\n",
       "      <td>1.247909</td>\n",
       "      <td>-0.072298</td>\n",
       "      <td>0.014510</td>\n",
       "      <td>0.092349</td>\n",
       "      <td>1.896657</td>\n",
       "      <td>0.027683</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>0.984013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.544613</td>\n",
       "      <td>1.790861</td>\n",
       "      <td>1.715717</td>\n",
       "      <td>0.016835</td>\n",
       "      <td>-0.239013</td>\n",
       "      <td>-0.251111</td>\n",
       "      <td>-0.605179</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>-0.107148</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-0.693613</td>\n",
       "      <td>-0.446925</td>\n",
       "      <td>0.624410</td>\n",
       "      <td>0.585962</td>\n",
       "      <td>0.772154</td>\n",
       "      <td>1.920066</td>\n",
       "      <td>0.999640</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.726569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.536970</td>\n",
       "      <td>0.579530</td>\n",
       "      <td>0.112615</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>-1.197036</td>\n",
       "      <td>-0.468998</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>1.114064</td>\n",
       "      <td>-1.722096</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.845991</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.557656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.156911</td>\n",
       "      <td>1.218710</td>\n",
       "      <td>1.249301</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>-0.728102</td>\n",
       "      <td>-0.468998</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-1.605370</td>\n",
       "      <td>0.415271</td>\n",
       "      <td>-0.845556</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>0.090339</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>0.143346</td>\n",
       "      <td>-0.246846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>-0.266565</td>\n",
       "      <td>-0.744086</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-1.908150</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>0.440445</td>\n",
       "      <td>-0.119638</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-1.207800</td>\n",
       "      <td>-1.722096</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>2.034586</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>0.524012</td>\n",
       "      <td>1.200827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.029239</td>\n",
       "      <td>0.256713</td>\n",
       "      <td>0.125511</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>0.600760</td>\n",
       "      <td>0.382400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.608312</td>\n",
       "      <td>0.292881</td>\n",
       "      <td>-1.722096</td>\n",
       "      <td>-1.185725</td>\n",
       "      <td>-1.721966</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.443986</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.280491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.154410</td>\n",
       "      <td>0.187813</td>\n",
       "      <td>0.238051</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-1.233975</td>\n",
       "      <td>-0.613144</td>\n",
       "      <td>-0.136810</td>\n",
       "      <td>-0.132437</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-0.669456</td>\n",
       "      <td>-1.722096</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.873749</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.344994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>-0.169318</td>\n",
       "      <td>-0.468998</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-1.605370</td>\n",
       "      <td>-0.290301</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.271318</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>-0.381213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.207809</td>\n",
       "      <td>0.724345</td>\n",
       "      <td>0.406078</td>\n",
       "      <td>-0.092329</td>\n",
       "      <td>-0.244823</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>-1.197036</td>\n",
       "      <td>0.164921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.858406</td>\n",
       "      <td>0.052865</td>\n",
       "      <td>0.051868</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>0.362752</td>\n",
       "      <td>1.465867</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.125724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.963421</td>\n",
       "      <td>1.290558</td>\n",
       "      <td>1.135439</td>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.354507</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-0.797828</td>\n",
       "      <td>-0.590578</td>\n",
       "      <td>0.225774</td>\n",
       "      <td>0.395730</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>0.235419</td>\n",
       "      <td>0.408503</td>\n",
       "      <td>-0.418911</td>\n",
       "      <td>-0.091034</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.607009</td>\n",
       "      <td>0.291246</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.666004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.550732</td>\n",
       "      <td>-0.093227</td>\n",
       "      <td>-0.317156</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.121344</td>\n",
       "      <td>-0.280758</td>\n",
       "      <td>-0.418702</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-0.137573</td>\n",
       "      <td>0.762255</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.376162</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.184480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.106094</td>\n",
       "      <td>0.167114</td>\n",
       "      <td>0.550721</td>\n",
       "      <td>-0.508997</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>-0.211772</td>\n",
       "      <td>-0.253430</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-0.888657</td>\n",
       "      <td>0.011648</td>\n",
       "      <td>-1.463034</td>\n",
       "      <td>-0.293315</td>\n",
       "      <td>0.454643</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>-0.087427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>-0.223862</td>\n",
       "      <td>0.036894</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.482492</td>\n",
       "      <td>-0.379793</td>\n",
       "      <td>-0.247008</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-0.661333</td>\n",
       "      <td>0.226026</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.609112</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>-1.376535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.747431</td>\n",
       "      <td>1.058981</td>\n",
       "      <td>0.963931</td>\n",
       "      <td>-0.384664</td>\n",
       "      <td>-0.591007</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-0.931825</td>\n",
       "      <td>-0.333360</td>\n",
       "      <td>0.464917</td>\n",
       "      <td>0.446133</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.409640</td>\n",
       "      <td>-0.024556</td>\n",
       "      <td>0.395426</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-0.023869</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.490495</td>\n",
       "      <td>-0.494921</td>\n",
       "      <td>0.102410</td>\n",
       "      <td>1.550462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-0.865050</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>0.074466</td>\n",
       "      <td>0.375350</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-1.605370</td>\n",
       "      <td>-0.290105</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>0.567572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26 rows × 239 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ACTBL2     ACTG1     ACTG2     ACTN1     ACTN4    ADAM10  ADAMTS13  \\\n",
       "0  -1.505517  0.469387  0.266539 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "1  -0.338129 -1.732577 -1.067345 -0.484136 -0.574835 -0.747148 -0.894209   \n",
       "2  -0.190932 -0.665262 -0.489321 -1.178625 -2.099970 -1.471872  0.003612   \n",
       "3  -1.505517 -1.732577 -1.067345 -1.046428 -2.099970 -1.301385 -2.363432   \n",
       "4  -0.869840 -1.732577 -0.161651 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "5  -1.505517 -1.732577 -1.067345 -0.663550 -2.099970 -1.373625 -0.442460   \n",
       "6   0.495373 -1.732577 -1.067345  0.642554  0.355340 -0.482796 -1.838184   \n",
       "7  -1.505517 -1.732577 -1.067345 -0.572469 -2.099970 -1.092635 -0.300754   \n",
       "8  -1.505517 -1.732577 -1.067345 -0.369941 -0.837724 -1.471872 -1.014452   \n",
       "9   0.395138  0.466486  0.405952 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "10 -0.112768 -1.732577 -1.067345 -0.088833 -0.350719 -0.331541 -0.341078   \n",
       "11  0.680107  1.125549  0.908032 -0.038281 -0.307818 -1.471872 -2.363432   \n",
       "12  1.544613  1.790861  1.715717  0.016835 -0.239013 -0.251111 -0.605179   \n",
       "13 -0.536970  0.579530  0.112615 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "14  1.156911  1.218710  1.249301 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "15 -1.505517 -0.266565 -0.744086 -1.178625 -2.099970 -1.471872 -1.908150   \n",
       "16  0.029239  0.256713  0.125511 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "17  0.154410  0.187813  0.238051 -1.178625 -2.099970 -1.471872 -1.233975   \n",
       "18 -1.505517 -1.067345 -1.067345 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "19 -0.207809  0.724345  0.406078 -0.092329 -0.244823 -1.471872 -2.363432   \n",
       "20  0.963421  1.290558  1.135439  0.575851  0.354507 -1.471872 -0.797828   \n",
       "21 -0.550732 -0.093227 -0.317156 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "22  0.106094  0.167114  0.550721 -0.508997 -2.099970 -1.471872 -2.363432   \n",
       "23 -1.505517 -0.223862  0.036894 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "24  0.747431  1.058981  0.963931 -0.384664 -0.591007 -1.471872 -0.931825   \n",
       "25 -1.505517 -1.732577 -1.067345 -1.178625 -2.099970 -1.471872 -0.865050   \n",
       "\n",
       "      ADIPOQ       AFM       AGT  ...     SVEP1      TFRC     THBS1      TLN2  \\\n",
       "0  -1.517300 -1.197036 -0.468998  ... -0.879373  1.106476 -1.722096 -1.992482   \n",
       "1   0.482403  0.766467  0.521196  ... -1.062470 -1.605370 -0.133370  0.126968   \n",
       "2  -0.513159 -0.094621 -0.111761  ... -2.150082  0.368662 -0.559181 -1.992482   \n",
       "3  -1.517300  0.202773  0.974240  ... -0.989369  0.255144 -0.512598  0.210279   \n",
       "4  -1.517300  0.323115  0.388052  ... -2.150082 -0.400457 -0.248735 -1.992482   \n",
       "5  -0.502456  0.263057  0.057436  ... -0.328864  0.765137 -0.313762 -0.291602   \n",
       "6  -1.517300 -0.947442 -0.330764  ... -2.150082 -0.647838  0.767015 -0.540684   \n",
       "7  -1.517300  0.322989 -0.222300  ... -0.891231 -0.524184  0.110614  0.074216   \n",
       "8  -1.517300  0.078044 -0.403351  ... -0.341272  0.334208 -0.421917 -1.347796   \n",
       "9  -0.697550  0.382139  0.539671  ... -0.440379  0.229268 -0.447268 -0.989630   \n",
       "10  0.319447  0.264053  0.197473  ... -0.176952 -0.082222  0.083108 -0.419636   \n",
       "11 -0.810231  0.309620  0.569613  ... -2.150082  0.272116  1.247909 -0.072298   \n",
       "12 -1.517300  0.018933 -0.107148  ... -2.150082 -0.693613 -0.446925  0.624410   \n",
       "13 -1.517300 -1.197036 -0.468998  ... -2.150082  1.114064 -1.722096 -1.992482   \n",
       "14 -1.517300 -0.728102 -0.468998  ... -2.150082 -1.605370  0.415271 -0.845556   \n",
       "15 -1.517300  0.440445 -0.119638  ... -2.150082 -1.207800 -1.722096 -1.992482   \n",
       "16 -1.517300  0.600760  0.382400  ... -0.608312  0.292881 -1.722096 -1.185725   \n",
       "17 -0.613144 -0.136810 -0.132437  ... -2.150082 -0.669456 -1.722096 -1.992482   \n",
       "18 -1.517300 -0.169318 -0.468998  ... -2.150082 -1.605370 -0.290301 -1.992482   \n",
       "19 -1.517300 -1.197036  0.164921  ... -0.858406  0.052865  0.051868 -1.992482   \n",
       "20 -0.590578  0.225774  0.395730  ... -2.150082  0.235419  0.408503 -0.418911   \n",
       "21 -1.121344 -0.280758 -0.418702  ... -2.150082 -0.137573  0.762255 -1.992482   \n",
       "22 -1.517300 -0.211772 -0.253430  ... -2.150082 -0.888657  0.011648 -1.463034   \n",
       "23 -1.482492 -0.379793 -0.247008  ... -2.150082 -0.661333  0.226026 -1.992482   \n",
       "24 -0.333360  0.464917  0.446133  ... -1.409640 -0.024556  0.395426 -1.992482   \n",
       "25 -1.517300  0.074466  0.375350  ... -2.150082 -1.605370 -0.290105 -1.992482   \n",
       "\n",
       "        TPM3     TUBA8      UGT8       VCL       VIM       VWF  \n",
       "0  -2.095127 -1.427330  1.865699 -1.113636 -1.101460  1.653913  \n",
       "1  -0.513470  0.113387 -0.676301 -0.412096 -1.101460  0.480118  \n",
       "2  -2.095127 -1.427330  1.756829 -1.113636 -1.101460  1.757194  \n",
       "3  -2.095127 -1.319357 -0.676301 -1.113636 -1.101460  0.235380  \n",
       "4  -2.095127 -1.427330  1.778492 -1.113636 -1.101460  0.895316  \n",
       "5  -0.627326 -0.632982 -0.676301 -0.988735 -0.173581 -0.071144  \n",
       "6   0.861175 -0.004139 -0.676301  1.246629 -0.089003  1.635177  \n",
       "7  -0.122331  0.658714 -0.676301 -0.592531 -0.510214  0.025982  \n",
       "8  -0.304606  0.083342 -0.676301 -0.803889  0.693706  0.200491  \n",
       "9  -2.095127 -1.427330  1.524093 -0.728794 -1.101460  0.404425  \n",
       "10  0.138718 -0.049477 -0.676301 -0.400096  0.952932  1.499098  \n",
       "11  0.014510  0.092349  1.896657  0.027683 -1.101460  0.984013  \n",
       "12  0.585962  0.772154  1.920066  0.999640 -1.101460  1.726569  \n",
       "13 -2.095127 -1.427330  1.845991 -1.113636 -1.101460  1.557656  \n",
       "14 -2.095127  0.090339 -0.676301 -1.113636  0.143346 -0.246846  \n",
       "15 -2.095127 -1.427330  2.034586 -1.113636  0.524012  1.200827  \n",
       "16 -1.721966 -1.427330  1.443986 -1.113636 -1.101460  1.280491  \n",
       "17 -2.095127 -1.427330  1.873749 -1.113636 -1.101460  1.344994  \n",
       "18 -2.095127 -1.427330  1.271318 -1.113636 -1.101460 -0.381213  \n",
       "19 -2.095127  0.362752  1.465867 -1.113636 -1.101460  1.125724  \n",
       "20 -0.091034 -1.427330  1.607009  0.291246 -1.101460  1.666004  \n",
       "21 -2.095127 -1.427330  1.376162 -1.113636 -1.101460  1.184480  \n",
       "22 -0.293315  0.454643 -0.676301 -1.113636 -1.101460 -0.087427  \n",
       "23 -2.095127 -1.427330  1.609112 -1.113636 -1.101460 -1.376535  \n",
       "24 -0.023869 -1.427330  1.490495 -0.494921  0.102410  1.550462  \n",
       "25 -2.095127 -1.427330 -0.676301 -1.113636 -1.101460  0.567572  \n",
       "\n",
       "[26 rows x 239 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# impute 0s as above\n",
    "\n",
    "imputeWideDFMinOr0(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute any missing values to the min of the dataset\n",
    "\n",
    "genesWithNANs = X_validation.columns[X_validation.isna().any()].tolist()\n",
    "\n",
    "X_validation[genesWithNANs] = min(X_validation.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some lists to track valation performance\n",
    "\n",
    "classifierListVal=[]\n",
    "y_predListVal=[]\n",
    "y_validationListVal=[]\n",
    "y_probListVal=[]\n",
    "seedListStatsVal=[]\n",
    "trainFracListStatsVal=[]\n",
    "balancedAccuracyListVal =[]\n",
    "precisionListVal=[]\n",
    "recallListVal=[]\n",
    "ROCAUCScoreListVal=[]\n",
    "F1ScoreListVal=[]\n",
    "MCCListVal=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned CatBoostClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = CatBoostClassifier(**CatBoostClassifierTuned.best_params,random_state = seed, verbose = False))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('CatBoostClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned HistGradientBoostingClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = HistGradientBoostingClassifier(**HistGradientBoostingClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('HistGradientBoostingClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred # Visualizing predictions\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned LGBMClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = LGBMClassifier(**LGBMClassifierTuned.best_params,random_state = seed, verbosity = -1))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append(LGBMClassifier)\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred # Visualizing predictions\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned AdaBoostClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = AdaBoostClassifier(**AdaBoostClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('AdaBoostClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned RandomForestClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = RandomForestClassifier(**RandomForestClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('RandomForestClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned BaggingClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = BaggingClassifier(**BaggingClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('BaggingClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned ExtraTreesClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = ExtraTreesClassifier(**ExtraTreesClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('ExtraTreesClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned GradientBoostingClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = GradientBoostingClassifier(**GradientBoostingClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('GradientBoostingClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned SVC For Validation\n",
    "\n",
    "pipeline.set_params(Model = SVC(**SVCTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('SVC')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned MLPClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = MLPClassifier(**MLPClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('MLPClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0]\n",
      "Balanced Accuracy 0.9097222222222222\n",
      "Precision 0.875\n",
      "Recall 0.875\n",
      "ROC AUC score 0.9097222222222222\n",
      "F1 score 0.875\n",
      "MCC 0.8194444444444444\n",
      "Validation score 0.9230769230769231\n",
      "Confusion matrix\n",
      " [[17  1]\n",
      " [ 1  7]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tuned KNeighborsClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = KNeighborsClassifier(**KNeighborsClassifierTuned.best_params))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('KNeighborsClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1]\n",
      "Balanced Accuracy 0.9444444444444444\n",
      "Precision 0.8\n",
      "Recall 1.0\n",
      "ROC AUC score 0.9444444444444444\n",
      "F1 score 0.888888888888889\n",
      "MCC 0.8432740427115678\n",
      "Validation score 0.9230769230769231\n",
      "Confusion matrix\n",
      " [[16  2]\n",
      " [ 0  8]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GaussianProcessClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = GaussianProcessClassifier(random_state = seed, n_jobs=nJobs))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('GaussianProcessClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned DecisionTreeClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = DecisionTreeClassifier(**DecisionTreeClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('DecisionTreeClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned GaussianNB For Validation\n",
    "\n",
    "pipeline.set_params(Model = GaussianNB(**GaussianNBTuned.best_params))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('GaussianNB')\n",
    "y_pred = pipeline.predict(X_validation) # .predict(X)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned QuadraticDiscriminantAnalysis For Validation\n",
    "\n",
    "pipeline.set_params(Model = QuadraticDiscriminantAnalysis(**QuadraticDiscriminantAnalysisTuned.best_params))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('QuadraticDiscriminantAnalysis')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned LinearSVC For Validation\n",
    "\n",
    "pipeline.set_params(Model = LinearSVC(**LinearSVCTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('LinearSVC')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned RidgeClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = RidgeClassifier(**RidgeClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('RidgeClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned SGDClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = SGDClassifier(**SGDClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('SGDClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned XGBClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = XGBClassifier(**XGBClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('XGBClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make final pipeline for each seed and 'test' it on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;Transforming Distribution&#x27;,\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                (&#x27;Standard Scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;Model&#x27;,\n",
       "                 KNeighborsClassifier(algorithm=&#x27;ball_tree&#x27;, leaf_size=45,\n",
       "                                      metric=&#x27;euclidean&#x27;, n_jobs=6,\n",
       "                                      n_neighbors=2))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;Transforming Distribution&#x27;,\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                (&#x27;Standard Scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;Model&#x27;,\n",
       "                 KNeighborsClassifier(algorithm=&#x27;ball_tree&#x27;, leaf_size=45,\n",
       "                                      metric=&#x27;euclidean&#x27;, n_jobs=6,\n",
       "                                      n_neighbors=2))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler(quantile_range=(25, 75))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(algorithm=&#x27;ball_tree&#x27;, leaf_size=45, metric=&#x27;euclidean&#x27;,\n",
       "                     n_jobs=6, n_neighbors=2)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('Transforming Distribution',\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                ('Standard Scaler', StandardScaler()),\n",
       "                ('Model',\n",
       "                 KNeighborsClassifier(algorithm='ball_tree', leaf_size=45,\n",
       "                                      metric='euclidean', n_jobs=6,\n",
       "                                      n_neighbors=2))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final pipeline\n",
    "\n",
    "pipeline.set_params(Model = KNeighborsClassifier(**KNeighborsClassifierTuned.best_params, n_jobs=nJobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;Transforming Distribution&#x27;,\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                (&#x27;Standard Scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;Model&#x27;,\n",
       "                 KNeighborsClassifier(algorithm=&#x27;ball_tree&#x27;, leaf_size=45,\n",
       "                                      metric=&#x27;euclidean&#x27;, n_jobs=6,\n",
       "                                      n_neighbors=2))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;Transforming Distribution&#x27;,\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                (&#x27;Standard Scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;Model&#x27;,\n",
       "                 KNeighborsClassifier(algorithm=&#x27;ball_tree&#x27;, leaf_size=45,\n",
       "                                      metric=&#x27;euclidean&#x27;, n_jobs=6,\n",
       "                                      n_neighbors=2))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler(quantile_range=(25, 75))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(algorithm=&#x27;ball_tree&#x27;, leaf_size=45, metric=&#x27;euclidean&#x27;,\n",
       "                     n_jobs=6, n_neighbors=2)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('Transforming Distribution',\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                ('Standard Scaler', StandardScaler()),\n",
       "                ('Model',\n",
       "                 KNeighborsClassifier(algorithm='ball_tree', leaf_size=45,\n",
       "                                      metric='euclidean', n_jobs=6,\n",
       "                                      n_neighbors=2))])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the pipeline\n",
    "\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# load from saved\n",
    "dfTestFromDisk = pd.read_excel('_dfTest_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.xlsx')\n",
    "df = dfTestFromDisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features based on dfTrain's RFE (optional)\n",
    "\n",
    "fileName = '_selectedFeaturesRFECV_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "\n",
    "with open(fileName + '.json', 'r') as f:\n",
    "    selected_featuresFromDisk = json.load(f)\n",
    "\n",
    "colsToKeep = []\n",
    "colsToKeep = copy.deepcopy(selected_featuresFromDisk)\n",
    "colsToKeep.append(target)\n",
    "\n",
    "df = df[colsToKeep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(colsToKeep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X and Y\n",
    "\n",
    "X_test, y_test = X_y_split(df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode y, as above\n",
    "y_test = pd.Series(le.fit_transform(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACTBL2</th>\n",
       "      <th>ACTG1</th>\n",
       "      <th>ACTG2</th>\n",
       "      <th>ACTN1</th>\n",
       "      <th>ACTN4</th>\n",
       "      <th>ADAM10</th>\n",
       "      <th>ADAMTS13</th>\n",
       "      <th>ADIPOQ</th>\n",
       "      <th>AFM</th>\n",
       "      <th>AGT</th>\n",
       "      <th>...</th>\n",
       "      <th>SVEP1</th>\n",
       "      <th>TFRC</th>\n",
       "      <th>THBS1</th>\n",
       "      <th>TLN2</th>\n",
       "      <th>TPM3</th>\n",
       "      <th>TUBA8</th>\n",
       "      <th>UGT8</th>\n",
       "      <th>VCL</th>\n",
       "      <th>VIM</th>\n",
       "      <th>VWF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.238035</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.605869</td>\n",
       "      <td>0.220409</td>\n",
       "      <td>-0.132511</td>\n",
       "      <td>-0.596869</td>\n",
       "      <td>-0.865540</td>\n",
       "      <td>-0.289417</td>\n",
       "      <td>-0.372957</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.948434</td>\n",
       "      <td>1.096785</td>\n",
       "      <td>-0.605986</td>\n",
       "      <td>0.487313</td>\n",
       "      <td>0.093620</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>0.854703</td>\n",
       "      <td>-0.662153</td>\n",
       "      <td>1.552332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.164659</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.146783</td>\n",
       "      <td>-0.171783</td>\n",
       "      <td>-0.298763</td>\n",
       "      <td>-0.241311</td>\n",
       "      <td>-0.445396</td>\n",
       "      <td>0.533932</td>\n",
       "      <td>0.222233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.651067</td>\n",
       "      <td>0.142736</td>\n",
       "      <td>0.337740</td>\n",
       "      <td>0.046313</td>\n",
       "      <td>0.324696</td>\n",
       "      <td>0.066913</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.247514</td>\n",
       "      <td>0.116570</td>\n",
       "      <td>1.714698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.310621</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.622334</td>\n",
       "      <td>0.296659</td>\n",
       "      <td>-0.191976</td>\n",
       "      <td>-1.654262</td>\n",
       "      <td>-0.505006</td>\n",
       "      <td>-0.360024</td>\n",
       "      <td>-0.191644</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.089110</td>\n",
       "      <td>0.121975</td>\n",
       "      <td>0.877530</td>\n",
       "      <td>-0.639579</td>\n",
       "      <td>0.141469</td>\n",
       "      <td>0.150554</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>0.764212</td>\n",
       "      <td>-0.093682</td>\n",
       "      <td>1.187440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.319442</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.337444</td>\n",
       "      <td>0.027481</td>\n",
       "      <td>-0.339895</td>\n",
       "      <td>-0.337704</td>\n",
       "      <td>-0.997932</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>0.389514</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.681654</td>\n",
       "      <td>-0.072275</td>\n",
       "      <td>0.454740</td>\n",
       "      <td>-0.242015</td>\n",
       "      <td>0.063445</td>\n",
       "      <td>-0.009888</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>0.302514</td>\n",
       "      <td>0.394435</td>\n",
       "      <td>1.805422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.105398</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.411348</td>\n",
       "      <td>-0.490759</td>\n",
       "      <td>-0.080501</td>\n",
       "      <td>-0.365737</td>\n",
       "      <td>-0.277148</td>\n",
       "      <td>0.424039</td>\n",
       "      <td>0.145391</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.856880</td>\n",
       "      <td>-1.931650</td>\n",
       "      <td>0.180562</td>\n",
       "      <td>-0.231454</td>\n",
       "      <td>-0.195705</td>\n",
       "      <td>0.588111</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>0.142215</td>\n",
       "      <td>0.146078</td>\n",
       "      <td>0.058512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.542626</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.062442</td>\n",
       "      <td>-0.526324</td>\n",
       "      <td>-0.712063</td>\n",
       "      <td>0.498297</td>\n",
       "      <td>0.504817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.948027</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>-0.487850</td>\n",
       "      <td>0.141260</td>\n",
       "      <td>-0.902553</td>\n",
       "      <td>-0.477969</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.663922</td>\n",
       "      <td>-0.503718</td>\n",
       "      <td>0.410547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.647026</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.309178</td>\n",
       "      <td>-0.054176</td>\n",
       "      <td>-0.470395</td>\n",
       "      <td>-1.672675</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.077890</td>\n",
       "      <td>-0.107259</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.549213</td>\n",
       "      <td>0.105562</td>\n",
       "      <td>0.570888</td>\n",
       "      <td>0.243236</td>\n",
       "      <td>0.612258</td>\n",
       "      <td>0.290803</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.047862</td>\n",
       "      <td>0.533338</td>\n",
       "      <td>1.494334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-0.844119</td>\n",
       "      <td>-0.141399</td>\n",
       "      <td>-0.671580</td>\n",
       "      <td>0.132653</td>\n",
       "      <td>-0.213185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.541840</td>\n",
       "      <td>-0.020314</td>\n",
       "      <td>-0.455560</td>\n",
       "      <td>0.035101</td>\n",
       "      <td>-0.552049</td>\n",
       "      <td>0.029566</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.621279</td>\n",
       "      <td>-0.115550</td>\n",
       "      <td>0.352758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>0.059684</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.302856</td>\n",
       "      <td>0.469412</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-2.212042</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.240386</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.913663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.566865</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-0.727181</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.339604</td>\n",
       "      <td>-0.022949</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-2.212042</td>\n",
       "      <td>-0.266810</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-0.369020</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.352981</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.157736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.111537</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-0.366964</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.262192</td>\n",
       "      <td>-0.573179</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-2.212042</td>\n",
       "      <td>-0.022853</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-0.335951</td>\n",
       "      <td>-0.143303</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.094270</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.015387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.518616</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-0.712679</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-0.943910</td>\n",
       "      <td>-0.265349</td>\n",
       "      <td>-0.443246</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.977919</td>\n",
       "      <td>0.150015</td>\n",
       "      <td>-0.443500</td>\n",
       "      <td>-1.400045</td>\n",
       "      <td>-0.644660</td>\n",
       "      <td>0.119929</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.917223</td>\n",
       "      <td>0.134167</td>\n",
       "      <td>0.714242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.560181</td>\n",
       "      <td>0.544833</td>\n",
       "      <td>0.162462</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.968511</td>\n",
       "      <td>-0.881045</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>1.198314</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.914506</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.685215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.251677</td>\n",
       "      <td>1.358475</td>\n",
       "      <td>1.242558</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.635046</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.888320</td>\n",
       "      <td>-0.412423</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.479385</td>\n",
       "      <td>-0.468624</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>0.420096</td>\n",
       "      <td>0.689763</td>\n",
       "      <td>1.409121</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>0.826844</td>\n",
       "      <td>1.035853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.317586</td>\n",
       "      <td>0.691659</td>\n",
       "      <td>0.566534</td>\n",
       "      <td>-0.527146</td>\n",
       "      <td>-0.779873</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.447781</td>\n",
       "      <td>-0.495967</td>\n",
       "      <td>0.214131</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.591210</td>\n",
       "      <td>0.481686</td>\n",
       "      <td>-0.863229</td>\n",
       "      <td>-0.951502</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.386907</td>\n",
       "      <td>-0.766551</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.466945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.666744</td>\n",
       "      <td>-0.426753</td>\n",
       "      <td>0.361841</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-0.825229</td>\n",
       "      <td>0.053316</td>\n",
       "      <td>-0.881045</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.854914</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.657721</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>-0.841744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.354246</td>\n",
       "      <td>-0.529297</td>\n",
       "      <td>0.156518</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-0.078220</td>\n",
       "      <td>-0.273276</td>\n",
       "      <td>0.234181</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.650896</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.524331</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>0.579147</td>\n",
       "      <td>1.320569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.306275</td>\n",
       "      <td>-0.319189</td>\n",
       "      <td>-0.623059</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.200790</td>\n",
       "      <td>-0.270904</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-1.100746</td>\n",
       "      <td>0.153237</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.092549</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.807970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.078103</td>\n",
       "      <td>0.668771</td>\n",
       "      <td>0.733058</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.293392</td>\n",
       "      <td>-0.424678</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.143846</td>\n",
       "      <td>1.170063</td>\n",
       "      <td>-0.778114</td>\n",
       "      <td>-0.129786</td>\n",
       "      <td>-0.800246</td>\n",
       "      <td>1.514158</td>\n",
       "      <td>-0.382649</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.028943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.124282</td>\n",
       "      <td>0.491639</td>\n",
       "      <td>0.087445</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-0.692685</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.400563</td>\n",
       "      <td>0.062053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.861677</td>\n",
       "      <td>-0.265849</td>\n",
       "      <td>-0.772325</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-0.152089</td>\n",
       "      <td>1.442185</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.233082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.120665</td>\n",
       "      <td>-0.159721</td>\n",
       "      <td>-0.007275</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>0.091164</td>\n",
       "      <td>0.207575</td>\n",
       "      <td>-0.512949</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.022261</td>\n",
       "      <td>-0.524455</td>\n",
       "      <td>0.703959</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.763738</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>-0.607622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.945370</td>\n",
       "      <td>-0.264794</td>\n",
       "      <td>-1.137084</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.495928</td>\n",
       "      <td>0.389485</td>\n",
       "      <td>-0.601540</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.258549</td>\n",
       "      <td>-0.241983</td>\n",
       "      <td>-0.232726</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.823087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.907975</td>\n",
       "      <td>-0.120321</td>\n",
       "      <td>-0.192863</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.288493</td>\n",
       "      <td>-0.105683</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.049206</td>\n",
       "      <td>-0.593395</td>\n",
       "      <td>0.338131</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.225254</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.298017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.574986</td>\n",
       "      <td>-0.780223</td>\n",
       "      <td>-0.650690</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.177773</td>\n",
       "      <td>-0.007961</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.051644</td>\n",
       "      <td>0.310237</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.548240</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>-0.219977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>0.400107</td>\n",
       "      <td>0.355674</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.415286</td>\n",
       "      <td>-0.286618</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.206153</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.525455</td>\n",
       "      <td>-1.133267</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.573840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.306907</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.337481</td>\n",
       "      <td>-0.614056</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-1.410997</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>-0.396214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-0.538612</td>\n",
       "      <td>-0.538612</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-0.266315</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.024745</td>\n",
       "      <td>-0.222664</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.204238</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.685608</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.600477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.232325</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.136901</td>\n",
       "      <td>0.154829</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.072725</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.737113</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.217506</td>\n",
       "      <td>-0.870685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.038460</td>\n",
       "      <td>-0.644721</td>\n",
       "      <td>-0.009443</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-0.113763</td>\n",
       "      <td>-0.376972</td>\n",
       "      <td>-0.356950</td>\n",
       "      <td>0.372239</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.578188</td>\n",
       "      <td>0.137592</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.693413</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.698812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.472820</td>\n",
       "      <td>0.678585</td>\n",
       "      <td>0.651427</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-0.999879</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.968511</td>\n",
       "      <td>-0.473735</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.024787</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>2.047760</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.479332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.715963</td>\n",
       "      <td>-0.862155</td>\n",
       "      <td>-1.031285</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.326948</td>\n",
       "      <td>0.615792</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.303557</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.474389</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>-0.059296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.211535</td>\n",
       "      <td>1.682050</td>\n",
       "      <td>1.487347</td>\n",
       "      <td>0.214984</td>\n",
       "      <td>0.232982</td>\n",
       "      <td>-0.735157</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.325208</td>\n",
       "      <td>0.297780</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.605795</td>\n",
       "      <td>0.735557</td>\n",
       "      <td>0.279451</td>\n",
       "      <td>0.509170</td>\n",
       "      <td>0.026759</td>\n",
       "      <td>1.451548</td>\n",
       "      <td>0.688723</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.253672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.517525</td>\n",
       "      <td>-0.292850</td>\n",
       "      <td>-0.357244</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.511592</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.538804</td>\n",
       "      <td>-0.108588</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-1.190624</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.034546</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.594409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.225752</td>\n",
       "      <td>1.567625</td>\n",
       "      <td>1.418026</td>\n",
       "      <td>0.634106</td>\n",
       "      <td>0.470620</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.282337</td>\n",
       "      <td>0.950599</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.094850</td>\n",
       "      <td>0.470029</td>\n",
       "      <td>0.746587</td>\n",
       "      <td>0.096326</td>\n",
       "      <td>0.578469</td>\n",
       "      <td>0.373212</td>\n",
       "      <td>1.809622</td>\n",
       "      <td>0.490993</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.001913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.351284</td>\n",
       "      <td>0.810913</td>\n",
       "      <td>0.533514</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-0.629982</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.274823</td>\n",
       "      <td>-0.321923</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.265859</td>\n",
       "      <td>-0.298498</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.318380</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.645993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.118371</td>\n",
       "      <td>1.464801</td>\n",
       "      <td>1.296012</td>\n",
       "      <td>0.355235</td>\n",
       "      <td>0.139351</td>\n",
       "      <td>-0.474816</td>\n",
       "      <td>-1.030897</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.235523</td>\n",
       "      <td>0.405769</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.516065</td>\n",
       "      <td>0.819675</td>\n",
       "      <td>-0.054017</td>\n",
       "      <td>0.463809</td>\n",
       "      <td>0.074526</td>\n",
       "      <td>1.570868</td>\n",
       "      <td>0.374176</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.840446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-0.112933</td>\n",
       "      <td>-0.130619</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-0.550134</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.610157</td>\n",
       "      <td>-0.352111</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-2.212042</td>\n",
       "      <td>-0.796169</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.061120</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.542996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.575008</td>\n",
       "      <td>1.773153</td>\n",
       "      <td>1.727010</td>\n",
       "      <td>-0.155473</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>0.405695</td>\n",
       "      <td>-1.165220</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>1.121927</td>\n",
       "      <td>0.153250</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.140190</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>0.685391</td>\n",
       "      <td>0.861907</td>\n",
       "      <td>-0.059660</td>\n",
       "      <td>1.869754</td>\n",
       "      <td>1.035715</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.309079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-0.262634</td>\n",
       "      <td>-0.107894</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>0.583577</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.287568</td>\n",
       "      <td>0.197638</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.491841</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.623980</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>0.012445</td>\n",
       "      <td>1.914975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.040652</td>\n",
       "      <td>0.484594</td>\n",
       "      <td>0.045431</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.150274</td>\n",
       "      <td>0.151766</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.217914</td>\n",
       "      <td>0.275741</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-0.232793</td>\n",
       "      <td>1.512694</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.768839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.007663</td>\n",
       "      <td>0.612934</td>\n",
       "      <td>0.257528</td>\n",
       "      <td>-0.608294</td>\n",
       "      <td>-0.587654</td>\n",
       "      <td>-0.661967</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.232350</td>\n",
       "      <td>-0.881045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.760438</td>\n",
       "      <td>0.342271</td>\n",
       "      <td>0.987446</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-0.083127</td>\n",
       "      <td>1.404424</td>\n",
       "      <td>-1.204875</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.035254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.830038</td>\n",
       "      <td>0.986249</td>\n",
       "      <td>0.949634</td>\n",
       "      <td>-0.792149</td>\n",
       "      <td>-1.104959</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.294871</td>\n",
       "      <td>0.340790</td>\n",
       "      <td>0.722486</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.537973</td>\n",
       "      <td>0.493954</td>\n",
       "      <td>-0.182145</td>\n",
       "      <td>-1.378484</td>\n",
       "      <td>-0.287746</td>\n",
       "      <td>1.619888</td>\n",
       "      <td>-0.297898</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.172519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.121603</td>\n",
       "      <td>0.736612</td>\n",
       "      <td>0.378226</td>\n",
       "      <td>-0.645225</td>\n",
       "      <td>-0.931143</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-0.292686</td>\n",
       "      <td>0.434549</td>\n",
       "      <td>0.543902</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.009679</td>\n",
       "      <td>-0.345377</td>\n",
       "      <td>-0.651122</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-0.382705</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.402562</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.784080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.542420</td>\n",
       "      <td>1.693424</td>\n",
       "      <td>1.588997</td>\n",
       "      <td>0.891286</td>\n",
       "      <td>0.741147</td>\n",
       "      <td>-0.623075</td>\n",
       "      <td>-1.008431</td>\n",
       "      <td>-0.322892</td>\n",
       "      <td>0.052546</td>\n",
       "      <td>0.426901</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.291694</td>\n",
       "      <td>0.936839</td>\n",
       "      <td>0.453057</td>\n",
       "      <td>0.577657</td>\n",
       "      <td>0.949400</td>\n",
       "      <td>1.667692</td>\n",
       "      <td>0.712856</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.245262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44 rows × 239 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ACTBL2     ACTG1     ACTG2     ACTN1     ACTN4    ADAM10  ADAMTS13  \\\n",
       "0   0.238035 -1.546379 -1.517205  0.605869  0.220409 -0.132511 -0.596869   \n",
       "1   0.164659 -1.546379 -1.517205  0.146783 -0.171783 -0.298763 -0.241311   \n",
       "2   0.310621 -1.546379 -1.517205  0.622334  0.296659 -0.191976 -1.654262   \n",
       "3   0.319442 -1.546379 -1.517205  0.337444  0.027481 -0.339895 -0.337704   \n",
       "4  -0.105398 -1.546379 -1.517205 -0.411348 -0.490759 -0.080501 -0.365737   \n",
       "5  -1.435105 -1.546379 -1.517205 -0.542626 -1.483101 -1.062442 -0.526324   \n",
       "6  -0.647026 -1.546379 -1.517205  0.309178 -0.054176 -0.470395 -1.672675   \n",
       "7  -1.435105 -1.546379 -1.517205 -0.817767 -1.483101 -0.844119 -0.141399   \n",
       "8  -1.435105 -1.546379 -1.517205 -0.817767  0.059684 -1.759557 -1.710125   \n",
       "9  -1.435105 -1.546379 -1.517205 -0.566865 -1.483101 -1.759557 -0.727181   \n",
       "10 -1.435105 -1.546379 -1.517205 -0.111537 -1.483101 -1.759557 -0.366964   \n",
       "11 -1.435105 -1.546379 -1.517205 -0.518616 -1.483101 -0.712679 -1.710125   \n",
       "12 -0.560181  0.544833  0.162462 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "13  1.251677  1.358475  1.242558  0.782361  0.635046 -1.759557 -1.710125   \n",
       "14  0.317586  0.691659  0.566534 -0.527146 -0.779873 -1.759557 -1.447781   \n",
       "15  0.666744 -0.426753  0.361841 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "16  0.354246 -0.529297  0.156518 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "17 -1.306275 -0.319189 -0.623059 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "18  0.078103  0.668771  0.733058 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "19 -0.124282  0.491639  0.087445 -0.817767 -1.483101 -1.759557 -0.692685   \n",
       "20 -0.120665 -0.159721 -0.007275 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "21 -0.945370 -0.264794 -1.137084 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "22 -0.907975 -0.120321 -0.192863 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "23 -0.574986 -0.780223 -0.650690 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "24 -1.435105  0.400107  0.355674 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "25 -1.306907 -1.546379 -1.517205 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "26 -1.435105 -0.538612 -0.538612 -0.817767 -1.483101 -1.759557 -0.266315   \n",
       "27 -1.435105 -1.546379 -1.232325 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "28  0.038460 -0.644721 -0.009443 -0.817767 -1.483101 -1.759557 -0.113763   \n",
       "29  0.472820  0.678585  0.651427 -0.817767 -1.483101 -1.759557 -0.999879   \n",
       "30 -0.715963 -0.862155 -1.031285 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "31  1.211535  1.682050  1.487347  0.214984  0.232982 -0.735157 -1.710125   \n",
       "32 -0.517525 -0.292850 -0.357244 -0.817767 -1.483101 -1.759557 -1.511592   \n",
       "33  1.225752  1.567625  1.418026  0.634106  0.470620 -1.759557 -1.710125   \n",
       "34  0.351284  0.810913  0.533514 -0.817767 -1.483101 -1.759557 -0.629982   \n",
       "35  1.118371  1.464801  1.296012  0.355235  0.139351 -0.474816 -1.030897   \n",
       "36 -1.435105 -0.112933 -0.130619 -0.817767 -1.483101 -1.759557 -0.550134   \n",
       "37  1.575008  1.773153  1.727010 -0.155473 -1.483101  0.405695 -1.165220   \n",
       "38 -1.435105 -0.262634 -0.107894 -0.817767 -1.483101 -1.759557  0.583577   \n",
       "39  0.040652  0.484594  0.045431 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "40  0.007663  0.612934  0.257528 -0.608294 -0.587654 -0.661967 -1.710125   \n",
       "41  0.830038  0.986249  0.949634 -0.792149 -1.104959 -1.759557 -1.710125   \n",
       "42  0.121603  0.736612  0.378226 -0.645225 -0.931143 -1.759557 -1.710125   \n",
       "43  1.542420  1.693424  1.588997  0.891286  0.741147 -0.623075 -1.008431   \n",
       "\n",
       "      ADIPOQ       AFM       AGT  ...     SVEP1      TFRC     THBS1      TLN2  \\\n",
       "0  -0.865540 -0.289417 -0.372957  ... -1.475926 -0.948434  1.096785 -0.605986   \n",
       "1  -0.445396  0.533932  0.222233  ... -0.651067  0.142736  0.337740  0.046313   \n",
       "2  -0.505006 -0.360024 -0.191644  ... -1.089110  0.121975  0.877530 -0.639579   \n",
       "3  -0.997932  0.253700  0.389514  ... -0.681654 -0.072275  0.454740 -0.242015   \n",
       "4  -0.277148  0.424039  0.145391  ... -0.856880 -1.931650  0.180562 -0.231454   \n",
       "5  -0.712063  0.498297  0.504817  ... -0.948027 -0.018118 -0.487850  0.141260   \n",
       "6  -1.637876 -0.077890 -0.107259  ... -0.549213  0.105562  0.570888  0.243236   \n",
       "7  -0.671580  0.132653 -0.213185  ... -0.541840 -0.020314 -0.455560  0.035101   \n",
       "8  -1.637876  0.302856  0.469412  ... -1.475926 -2.212042 -1.259770 -1.624791   \n",
       "9  -1.637876 -0.339604 -0.022949  ... -1.475926 -2.212042 -0.266810 -1.624791   \n",
       "10 -1.637876  0.262192 -0.573179  ... -1.475926 -2.212042 -0.022853 -1.624791   \n",
       "11 -0.943910 -0.265349 -0.443246  ... -0.977919  0.150015 -0.443500 -1.400045   \n",
       "12 -1.637876 -0.968511 -0.881045  ... -1.475926  1.198314 -1.259770 -1.624791   \n",
       "13 -1.637876 -0.888320 -0.412423  ... -1.475926 -0.479385 -0.468624 -1.624791   \n",
       "14 -0.495967  0.214131  0.382353  ... -1.475926 -0.591210  0.481686 -0.863229   \n",
       "15 -0.825229  0.053316 -0.881045  ... -1.475926  0.854914 -1.259770 -1.624791   \n",
       "16 -0.078220 -0.273276  0.234181  ... -1.475926 -0.650896 -1.259770 -1.624791   \n",
       "17 -1.637876 -0.200790 -0.270904  ... -1.475926 -1.100746  0.153237 -1.624791   \n",
       "18 -1.637876 -0.293392 -0.424678  ... -1.475926 -0.143846  1.170063 -0.778114   \n",
       "19 -1.637876  0.400563  0.062053  ... -0.861677 -0.265849 -0.772325 -1.624791   \n",
       "20  0.091164  0.207575 -0.512949  ... -1.022261 -0.524455  0.703959 -1.624791   \n",
       "21 -1.495928  0.389485 -0.601540  ... -1.258549 -0.241983 -0.232726 -1.624791   \n",
       "22 -1.637876  0.288493 -0.105683  ... -1.049206 -0.593395  0.338131 -1.624791   \n",
       "23 -1.637876  0.177773 -0.007961  ... -1.475926 -0.051644  0.310237 -1.624791   \n",
       "24 -1.637876  0.415286 -0.286618  ... -1.475926  0.206153 -1.259770 -1.624791   \n",
       "25 -1.637876  0.337481 -0.614056  ... -1.475926 -1.410997 -1.259770 -1.624791   \n",
       "26 -1.637876  0.024745 -0.222664  ... -1.475926 -0.204238 -1.259770 -1.624791   \n",
       "27 -1.637876  0.136901  0.154829  ... -1.475926 -0.072725 -1.259770 -1.624791   \n",
       "28 -0.376972 -0.356950  0.372239  ... -1.475926  0.578188  0.137592 -1.624791   \n",
       "29 -1.637876 -0.968511 -0.473735  ... -1.475926  0.024787 -1.259770 -1.624791   \n",
       "30 -1.637876  0.326948  0.615792  ... -1.475926  0.303557 -1.259770 -1.624791   \n",
       "31 -1.637876 -0.325208  0.297780  ... -1.475926  0.605795  0.735557  0.279451   \n",
       "32 -1.637876 -0.538804 -0.108588  ... -1.475926 -1.190624 -1.259770 -1.624791   \n",
       "33 -1.637876  0.282337  0.950599  ... -1.094850  0.470029  0.746587  0.096326   \n",
       "34 -1.637876  0.274823 -0.321923  ... -1.475926  0.265859 -0.298498 -1.624791   \n",
       "35 -1.637876  0.235523  0.405769  ... -1.475926  0.516065  0.819675 -0.054017   \n",
       "36 -1.637876 -0.610157 -0.352111  ... -1.475926 -2.212042 -0.796169 -1.624791   \n",
       "37 -1.637876  1.121927  0.153250  ... -1.475926  0.140190 -1.259770  0.685391   \n",
       "38 -1.637876 -0.287568  0.197638  ... -1.475926 -0.491841 -1.259770 -1.624791   \n",
       "39 -1.637876 -0.150274  0.151766  ... -1.475926 -0.217914  0.275741 -1.624791   \n",
       "40 -1.637876 -0.232350 -0.881045  ... -0.760438  0.342271  0.987446 -1.624791   \n",
       "41 -1.294871  0.340790  0.722486  ... -1.475926 -0.537973  0.493954 -0.182145   \n",
       "42 -0.292686  0.434549  0.543902  ... -1.475926 -0.009679 -0.345377 -0.651122   \n",
       "43 -0.322892  0.052546  0.426901  ... -1.475926  0.291694  0.936839  0.453057   \n",
       "\n",
       "        TPM3     TUBA8      UGT8       VCL       VIM       VWF  \n",
       "0   0.487313  0.093620  1.019992  0.854703 -0.662153  1.552332  \n",
       "1   0.324696  0.066913  1.019992 -0.247514  0.116570  1.714698  \n",
       "2   0.141469  0.150554  1.019992  0.764212 -0.093682  1.187440  \n",
       "3   0.063445 -0.009888  1.019992  0.302514  0.394435  1.805422  \n",
       "4  -0.195705  0.588111  1.019992  0.142215  0.146078  0.058512  \n",
       "5  -0.902553 -0.477969  1.019992 -0.663922 -0.503718  0.410547  \n",
       "6   0.612258  0.290803  1.019992 -0.047862  0.533338  1.494334  \n",
       "7  -0.552049  0.029566  1.019992 -0.621279 -0.115550  0.352758  \n",
       "8  -1.575466 -1.117115  1.019992 -0.240386 -0.807743  0.913663  \n",
       "9  -0.369020 -1.117115  1.019992 -0.352981 -0.807743  1.157736  \n",
       "10 -0.335951 -0.143303  1.019992 -0.094270 -0.807743  1.015387  \n",
       "11 -0.644660  0.119929  1.019992 -0.917223  0.134167  0.714242  \n",
       "12 -1.575466 -1.117115  1.914506 -1.287931 -0.807743  1.685215  \n",
       "13  0.420096  0.689763  1.409121  0.177200  0.826844  1.035853  \n",
       "14 -0.951502 -1.117115  1.386907 -0.766551 -0.807743  1.466945  \n",
       "15 -1.575466 -1.117115  1.657721 -1.287931 -0.807743 -0.841744  \n",
       "16 -1.575466 -1.117115  1.524331 -1.287931  0.579147  1.320569  \n",
       "17 -1.575466 -1.117115  1.092549 -1.287931 -0.807743  0.807970  \n",
       "18 -0.129786 -0.800246  1.514158 -0.382649 -0.807743  1.028943  \n",
       "19 -1.575466 -0.152089  1.442185 -1.287931 -0.807743  1.233082  \n",
       "20 -1.575466 -1.117115  1.763738 -1.287931 -0.807743 -0.607622  \n",
       "21 -1.575466 -1.117115  1.019992 -1.287931 -0.807743  0.823087  \n",
       "22 -1.575466 -1.117115  1.225254 -1.287931 -0.807743  0.298017  \n",
       "23 -1.575466 -1.117115  1.548240 -1.287931 -0.807743 -0.219977  \n",
       "24 -1.575466 -1.117115  1.525455 -1.133267 -0.807743  0.573840  \n",
       "25 -1.575466 -1.117115  1.019992 -1.287931 -0.807743 -0.396214  \n",
       "26 -1.575466 -1.117115  1.685608 -1.287931 -0.807743  1.600477  \n",
       "27 -1.575466 -1.117115  1.737113 -1.287931 -0.217506 -0.870685  \n",
       "28 -1.575466 -1.117115  1.693413 -1.287931 -0.807743  1.698812  \n",
       "29 -1.575466 -1.117115  2.047760 -1.287931 -0.807743  1.479332  \n",
       "30 -1.575466 -1.117115  1.474389 -1.287931 -0.807743 -0.059296  \n",
       "31  0.509170  0.026759  1.451548  0.688723 -0.807743  1.253672  \n",
       "32 -1.575466 -1.117115  1.034546 -1.287931 -0.807743  1.594409  \n",
       "33  0.578469  0.373212  1.809622  0.490993 -0.807743  1.001913  \n",
       "34 -1.575466 -1.117115  1.318380 -1.287931 -0.807743  1.645993  \n",
       "35  0.463809  0.074526  1.570868  0.374176 -0.807743  0.840446  \n",
       "36 -1.575466 -1.117115  1.061120 -1.287931 -0.807743  1.542996  \n",
       "37  0.861907 -0.059660  1.869754  1.035715 -0.807743  1.309079  \n",
       "38 -1.575466 -1.117115  1.623980 -1.287931  0.012445  1.914975  \n",
       "39 -1.575466 -0.232793  1.512694 -1.287931 -0.807743  0.768839  \n",
       "40 -1.575466 -0.083127  1.404424 -1.204875 -0.807743  0.035254  \n",
       "41 -1.378484 -0.287746  1.619888 -0.297898 -0.807743  1.172519  \n",
       "42 -1.575466 -0.382705  1.019992 -0.402562 -0.807743  0.784080  \n",
       "43  0.577657  0.949400  1.667692  0.712856 -0.807743  1.245262  \n",
       "\n",
       "[44 rows x 239 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# impute 0s, as above\n",
    "\n",
    "imputeWideDFMinOr0(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute any new proteins in X_test with the min value of the dataset, as above\n",
    "genesWithNANs = X_test.columns[X_test.isna().any()].tolist()\n",
    "\n",
    "X_test[genesWithNANs] = min(X_test.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test) # .predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = pipeline.predict_proba(X_test) # .predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-format this output\n",
    "y_prob = [i[1] for i in y_prob]\n",
    "\n",
    "# append to lists\n",
    "y_predList.append(y_pred)\n",
    "y_testList.append(y_test)\n",
    "y_probList.append(y_prob)\n",
    "seedListStats.append(seed)\n",
    "trainFracListStats.append(trainFrac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy 0.8928571428571428\n",
      "Precision 1.0\n",
      "Recall 0.7857142857142857\n",
      "ROC AUC score 0.8928571428571428\n",
      "F1 score 0.88\n",
      "Test score 0.9318181818181818\n"
     ]
    }
   ],
   "source": [
    "# sklearn scoring: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_test, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_test, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_test, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_test, y_pred))\n",
    "\n",
    "# autoSKLearn scoring\n",
    "#s = pipeline.score(X_train, y_train)\n",
    "#print(f\"Train score {s}\")\n",
    "s = pipeline.score(X_test, y_test)\n",
    "print(f\"Test score {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      " [[30  0]\n",
      " [ 3 11]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 200x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot normalized confusion matrix from the current seed\n",
    "skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True,figsize=(2,2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1377fd300>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from collections.abc import Iterable\n",
    "\n",
    "def flatten(lis):\n",
    "     for item in lis:\n",
    "         if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "             for x in flatten(item):\n",
    "                 yield x\n",
    "         else:        \n",
    "             yield item\n",
    "             \n",
    "cm = confusion_matrix(list(flatten(y_testList)), list(flatten(y_predList))) # for many trials\n",
    "\n",
    "# for all models\n",
    "from collections.abc import Iterable\n",
    "\n",
    "cm = confusion_matrix(list(flatten(y_testList)), list(flatten(y_predList))) # for all models\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 200x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot normalized confusion matrix for all seeds/models in the current loop\n",
    "skplt.metrics.plot_confusion_matrix(list(flatten(y_testList)), list(flatten(y_predList)), normalize=True,figsize=(2,2))\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test stastistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = cm[0][0]\n",
    "FN = cm[1][0] # type II error\n",
    "TP = cm[1][1]\n",
    "FP = cm[0][1] # Type I error\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "precision = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# balanced accurcy\n",
    "bACC = (TPR + TNR) / 2\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# F1 score\n",
    "F1 = (2 * TP) / (2 * TP + FP + FN)\n",
    "# False ommission rate\n",
    "FOR = 1 - NPV\n",
    "# LR+, positive liklihood ratio\n",
    "LRpos = TPR / FPR\n",
    "# LR-, negative liklihood ratio\n",
    "LRneg = FNR / TNR\n",
    "# diagnostic odds ratio\n",
    "oddsRatio = LRpos/LRneg\n",
    "\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sensitivity/TPR is: 0.7857142857142857\n",
      "the specificity/TNR is: 1.0\n",
      "the accuracy is: 0.9318181818181818\n",
      "the balanced accuracy is: 0.8928571428571428\n",
      "the negative predictive value (NPV) is 0.9090909090909091\n",
      "the diagnostic odds ratio is inf\n",
      "The positive LR is inf\n",
      "The negative LR is 0.21428571428571427\n",
      "The F1 score is 0.88\n"
     ]
    }
   ],
   "source": [
    "print(\"the sensitivity/TPR is: \" + str(TPR))\n",
    "print(\"the specificity/TNR is: \" + str(TNR))\n",
    "print(\"the accuracy is: \" + str(ACC))\n",
    "print(\"the balanced accuracy is: \" + str(bACC))\n",
    "print(\"the negative predictive value (NPV) is \" + str(NPV))\n",
    "print(\"the diagnostic odds ratio is \" + str(oddsRatio))\n",
    "print(\"The positive LR is \" + str(LRpos))\n",
    "print(\"The negative LR is \" + str(LRneg))\n",
    "print(\"The F1 score is \" + str(F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_false_positive(threshold_vector, y_test):\n",
    "    true_positive = np.equal(threshold_vector, 1) & np.equal(y_test, 1)\n",
    "    true_negative = np.equal(threshold_vector, 0) & np.equal(y_test, 0)\n",
    "    false_positive = np.equal(threshold_vector, 1) & np.equal(y_test, 0)\n",
    "    false_negative = np.equal(threshold_vector, 0) & np.equal(y_test, 1)\n",
    "\n",
    "    tpr = true_positive.sum() / (true_positive.sum() + false_negative.sum())\n",
    "    fpr = false_positive.sum() / (false_positive.sum() + true_negative.sum())\n",
    "\n",
    "    return tpr, fpr\n",
    "\n",
    "def roc_from_scratch(probabilities, y_test, partitions=100):\n",
    "    roc = np.array([])\n",
    "    for i in range(partitions + 1):\n",
    "        \n",
    "        threshold_vector = np.greater_equal(probabilities, i / partitions).astype(int)\n",
    "        tpr, fpr = true_false_positive(threshold_vector, y_test)\n",
    "        roc = np.append(roc, [fpr, tpr])\n",
    "        \n",
    "    return roc.reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUROC is: 0.9880952380952381\n"
     ]
    }
   ],
   "source": [
    "# calc the AUROC\n",
    "partitions = 100\n",
    "\n",
    "ROC = roc_from_scratch(y_prob, y_test, partitions=partitions) # y_prob[:,1]\n",
    "#ROC = roc_from_scratch(y_prob, y_test, partitions=partitions) # way that usually works\n",
    "fpr, tpr = ROC[:, 0], ROC[:, 1]\n",
    "rectangle_roc = 0\n",
    "for k in range(partitions):\n",
    "        rectangle_roc = rectangle_roc + (fpr[k]- fpr[k + 1]) * tpr[k]\n",
    "print('The AUROC is: ' + str(rectangle_roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC is 0.9880952380952381\n",
      "AUROC is 0.9880952380952381\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# overlaid ROC\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100) # regularly spaced points\n",
    "\n",
    "#set up plotting area\n",
    "plt.figure(figsize=(3, 3))\n",
    "ax = plt.axes() # enables overlay\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'--', color = 'black', lw = 2)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "#plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "fprList = []\n",
    "tprList = []\n",
    "\n",
    "for y_prob, y_test in zip(y_probList,y_testList):\n",
    "    \n",
    "    ROC = roc_from_scratch(np.reshape(y_prob,(-1)),y_test,partitions=100) # y_prob[:,1]\n",
    "    #ROC = roc_from_scratch(y_prob,y_test,partitions=100) # way that usually works\n",
    "\n",
    "    #plt.scatter(ROC[:,0],ROC[:,1],color='black',s=20, alpha = 0.8)\n",
    "    plt.plot(ROC[:,0],ROC[:,1],color='black', alpha = 0.025)\n",
    "    plt.title('ROC Curve',fontsize=12)\n",
    "    plt.xlabel('False Positive Rate',fontsize=12)\n",
    "    plt.ylabel('True Positive Rate',fontsize=12)\n",
    "    #print(ROC[:,1])\n",
    "\n",
    "    sort = np.argsort(ROC[:,0])\n",
    "    interp_tpr = np.interp(mean_fpr, ROC[:,0][sort], ROC[:,1][sort]) # interp needs to be sorted\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    rectangle_roc = 0\n",
    "    for k in range(partitions):\n",
    "        fpr, tpr = ROC[:, 0], ROC[:, 1]\n",
    "        rectangle_roc = rectangle_roc + (fpr[k]- fpr[k + 1]) * tpr[k]\n",
    "    print('AUROC is ' + str(rectangle_roc))    \n",
    "    aucs.append(rectangle_roc)\n",
    "\n",
    "    #if auc < 0.5:\n",
    "    #    probs = [-x for x in probs]\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "#print(mean_tpr)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.3,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"ROC curve\")\n",
    "\n",
    "plt.plot()\n",
    "ax.legend(bbox_to_anchor=(3.3, .5), loc='center right', borderaxespad=0)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.savefig('ROC_overlaid_' + target + '_quantType_' + quantType + '.pdf',format='pdf', dpi=500, bbox_inches='tight')# axis labels\n",
    "plt.savefig('ROC_overlaid_' + target + '_quantType_' + quantType + '.svg',format='svg', dpi=500, bbox_inches='tight')# axis labels. Saves to Fugure2 folder currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "y_testListFlat = list(chain.from_iterable(y_testList)) \n",
    "y_predListFlat = list(chain.from_iterable(y_predList))\n",
    "#y_probListFlat = list(chain.from_iterable(y_probList)) # usually works\n",
    "y_probListFlat = list(chain.from_iterable(y_probList)) # y_probList[:,1]\n",
    "y_1MinusProbListFlat = [1 - x for x in y_probListFlat]\n",
    "y_predProbAFormattedLikeSKLearn = np.stack((y_1MinusProbListFlat, y_probListFlat), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save final lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save y_predList\n",
    "fileName = '_y_predList_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl'\n",
    "\n",
    "with open(fileName, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(y_predList, fp)\n",
    "\n",
    "# reload y_predList\n",
    "with open(fileName, \"rb\") as fp:   # Unpickling\n",
    "    y_predList = pickle.load(fp)\n",
    "\n",
    "\n",
    "\n",
    "# save y_probList\n",
    "fileName = '_y_probList_target-' + target + '_quantType_' + quantType + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl'\n",
    "\n",
    "with open(fileName, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(y_probList, fp)\n",
    "\n",
    "# reload y_probList\n",
    "with open(fileName, \"rb\") as fp:   # Unpickling\n",
    "    y_probList = pickle.load(fp)\n",
    "\n",
    "\n",
    "\n",
    "# save y_testList\n",
    "fileName = '_y_testList_target-' + target + '_quantType_' + quantType + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl'\n",
    "\n",
    "with open(fileName, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(y_testList, fp)\n",
    "\n",
    "# reload y_probList\n",
    "with open(fileName, \"rb\") as fp:   # Unpickling\n",
    "    y_testList = pickle.load(fp)\n",
    "\n",
    "\n",
    "# save finalImportancesMeanList\n",
    "fileName = '_finalImportancesMeanList_target-' + target + '_quantType_' + quantType + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl'\n",
    "\n",
    "with open(fileName, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(finalImportancesMeanList, fp)\n",
    "\n",
    "# reload finalImportancesMeanList\n",
    "with open(fileName, \"rb\") as fp:   # Unpickling\n",
    "    finalImportancesMeanList = pickle.load(fp)\n",
    "    \n",
    "\n",
    "# save finalFeaturesList\n",
    "fileName = '_finalFeaturesList_target-' + target + '_quantType_' + quantType + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl'\n",
    "\n",
    "with open(fileName, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(finalFeaturesList, fp)\n",
    "\n",
    "# reload finalFeaturesList\n",
    "with open(fileName, \"rb\") as fp:   # Unpickling\n",
    "    finalFeaturesList = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a KNN classifier on the whole dataset that can be utilized for analysis of other plasma EV proteomics datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# load from saved\n",
    "dfMLFromDisk = pd.read_excel('_dfML-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "\n",
    "X, y = X_y_split(dfMLFromDisk, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A2M</th>\n",
       "      <th>ACTA1</th>\n",
       "      <th>ACTB</th>\n",
       "      <th>ACTBL2</th>\n",
       "      <th>ACTG1</th>\n",
       "      <th>ACTG2</th>\n",
       "      <th>ACTN1</th>\n",
       "      <th>ACTN4</th>\n",
       "      <th>...</th>\n",
       "      <th>VIM</th>\n",
       "      <th>VTN</th>\n",
       "      <th>VWF</th>\n",
       "      <th>WDR1</th>\n",
       "      <th>YWHAB</th>\n",
       "      <th>YWHAE</th>\n",
       "      <th>YWHAG</th>\n",
       "      <th>YWHAH</th>\n",
       "      <th>YWHAQ</th>\n",
       "      <th>YWHAZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.830252</td>\n",
       "      <td>1.617173</td>\n",
       "      <td>1.529702</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.238035</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.605869</td>\n",
       "      <td>0.220409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.662153</td>\n",
       "      <td>1.651220</td>\n",
       "      <td>1.552332</td>\n",
       "      <td>0.394781</td>\n",
       "      <td>0.568536</td>\n",
       "      <td>0.793171</td>\n",
       "      <td>0.088769</td>\n",
       "      <td>0.297304</td>\n",
       "      <td>0.425608</td>\n",
       "      <td>0.877359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.332276</td>\n",
       "      <td>1.617173</td>\n",
       "      <td>1.309366</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.478284</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.456010</td>\n",
       "      <td>0.041927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410026</td>\n",
       "      <td>1.576208</td>\n",
       "      <td>1.427785</td>\n",
       "      <td>-0.010363</td>\n",
       "      <td>-0.163921</td>\n",
       "      <td>0.264368</td>\n",
       "      <td>-0.276322</td>\n",
       "      <td>0.307422</td>\n",
       "      <td>0.205803</td>\n",
       "      <td>0.476802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.357704</td>\n",
       "      <td>1.617173</td>\n",
       "      <td>1.208670</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.486699</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.312867</td>\n",
       "      <td>-0.110075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.271596</td>\n",
       "      <td>1.465018</td>\n",
       "      <td>1.357034</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>-0.153920</td>\n",
       "      <td>0.293756</td>\n",
       "      <td>-0.187815</td>\n",
       "      <td>0.363499</td>\n",
       "      <td>0.159687</td>\n",
       "      <td>0.525286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.482507</td>\n",
       "      <td>1.617173</td>\n",
       "      <td>1.400820</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.432288</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.640151</td>\n",
       "      <td>0.196756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263245</td>\n",
       "      <td>1.466153</td>\n",
       "      <td>1.407550</td>\n",
       "      <td>0.464374</td>\n",
       "      <td>0.455905</td>\n",
       "      <td>0.528796</td>\n",
       "      <td>0.104778</td>\n",
       "      <td>0.395489</td>\n",
       "      <td>0.370452</td>\n",
       "      <td>0.871807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.652147</td>\n",
       "      <td>1.617173</td>\n",
       "      <td>1.550001</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.495373</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.642554</td>\n",
       "      <td>0.355340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089003</td>\n",
       "      <td>1.599650</td>\n",
       "      <td>1.635177</td>\n",
       "      <td>0.675694</td>\n",
       "      <td>0.686577</td>\n",
       "      <td>0.867544</td>\n",
       "      <td>-0.069715</td>\n",
       "      <td>0.407808</td>\n",
       "      <td>0.310482</td>\n",
       "      <td>0.880123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>172</td>\n",
       "      <td>0.695520</td>\n",
       "      <td>2.162397</td>\n",
       "      <td>0.620386</td>\n",
       "      <td>0.152694</td>\n",
       "      <td>0.029239</td>\n",
       "      <td>0.256713</td>\n",
       "      <td>0.125511</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.286680</td>\n",
       "      <td>1.280491</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-0.504451</td>\n",
       "      <td>-0.612937</td>\n",
       "      <td>-0.544749</td>\n",
       "      <td>-0.490847</td>\n",
       "      <td>-0.575711</td>\n",
       "      <td>-0.186766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>173</td>\n",
       "      <td>0.561369</td>\n",
       "      <td>2.294855</td>\n",
       "      <td>1.989700</td>\n",
       "      <td>1.506687</td>\n",
       "      <td>1.542420</td>\n",
       "      <td>1.693424</td>\n",
       "      <td>1.588997</td>\n",
       "      <td>0.891286</td>\n",
       "      <td>0.741147</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.260654</td>\n",
       "      <td>1.245262</td>\n",
       "      <td>0.099659</td>\n",
       "      <td>0.677961</td>\n",
       "      <td>0.876450</td>\n",
       "      <td>0.702891</td>\n",
       "      <td>0.786348</td>\n",
       "      <td>0.816816</td>\n",
       "      <td>1.141525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>174</td>\n",
       "      <td>0.592910</td>\n",
       "      <td>2.337891</td>\n",
       "      <td>1.489827</td>\n",
       "      <td>1.014633</td>\n",
       "      <td>0.979343</td>\n",
       "      <td>1.144509</td>\n",
       "      <td>1.081129</td>\n",
       "      <td>-0.105925</td>\n",
       "      <td>-0.386698</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.214992</td>\n",
       "      <td>0.924127</td>\n",
       "      <td>-0.631446</td>\n",
       "      <td>0.277585</td>\n",
       "      <td>0.419303</td>\n",
       "      <td>0.280890</td>\n",
       "      <td>0.324307</td>\n",
       "      <td>0.330923</td>\n",
       "      <td>0.792864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>175</td>\n",
       "      <td>0.531825</td>\n",
       "      <td>2.062484</td>\n",
       "      <td>0.391271</td>\n",
       "      <td>-0.456651</td>\n",
       "      <td>-0.667611</td>\n",
       "      <td>-0.169866</td>\n",
       "      <td>-0.128379</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>0.803730</td>\n",
       "      <td>0.231407</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-2.172128</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.723978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>176</td>\n",
       "      <td>0.836052</td>\n",
       "      <td>2.133768</td>\n",
       "      <td>0.569608</td>\n",
       "      <td>-0.203207</td>\n",
       "      <td>-2.183991</td>\n",
       "      <td>0.050999</td>\n",
       "      <td>-0.056756</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.086213</td>\n",
       "      <td>0.768596</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-2.172128</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.723978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 374 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0      A1BG       A2M     ACTA1      ACTB    ACTBL2     ACTG1  \\\n",
       "0             0  0.830252  1.617173  1.529702 -1.517205  0.238035 -1.732577   \n",
       "1             1  0.332276  1.617173  1.309366 -1.517205  0.478284 -1.732577   \n",
       "2             2  0.357704  1.617173  1.208670 -1.517205 -0.486699 -1.732577   \n",
       "3             3  0.482507  1.617173  1.400820 -1.517205  0.432288 -1.732577   \n",
       "4             4  0.652147  1.617173  1.550001 -1.517205  0.495373 -1.732577   \n",
       "..          ...       ...       ...       ...       ...       ...       ...   \n",
       "172         172  0.695520  2.162397  0.620386  0.152694  0.029239  0.256713   \n",
       "173         173  0.561369  2.294855  1.989700  1.506687  1.542420  1.693424   \n",
       "174         174  0.592910  2.337891  1.489827  1.014633  0.979343  1.144509   \n",
       "175         175  0.531825  2.062484  0.391271 -0.456651 -0.667611 -0.169866   \n",
       "176         176  0.836052  2.133768  0.569608 -0.203207 -2.183991  0.050999   \n",
       "\n",
       "        ACTG2     ACTN1     ACTN4  ...       VIM       VTN       VWF  \\\n",
       "0   -1.517205  0.605869  0.220409  ... -0.662153  1.651220  1.552332   \n",
       "1   -1.517205  0.456010  0.041927  ...  0.410026  1.576208  1.427785   \n",
       "2   -1.517205  0.312867 -0.110075  ... -0.271596  1.465018  1.357034   \n",
       "3   -1.517205  0.640151  0.196756  ...  0.263245  1.466153  1.407550   \n",
       "4   -1.517205  0.642554  0.355340  ... -0.089003  1.599650  1.635177   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "172  0.125511 -1.836067 -2.099970  ... -1.101460  1.286680  1.280491   \n",
       "173  1.588997  0.891286  0.741147  ... -1.101460  1.260654  1.245262   \n",
       "174  1.081129 -0.105925 -0.386698  ... -1.101460  1.214992  0.924127   \n",
       "175 -0.128379 -1.836067 -2.099970  ... -1.101460  0.803730  0.231407   \n",
       "176 -0.056756 -1.836067 -2.099970  ... -1.101460  1.086213  0.768596   \n",
       "\n",
       "         WDR1     YWHAB     YWHAE     YWHAG     YWHAH     YWHAQ     YWHAZ  \n",
       "0    0.394781  0.568536  0.793171  0.088769  0.297304  0.425608  0.877359  \n",
       "1   -0.010363 -0.163921  0.264368 -0.276322  0.307422  0.205803  0.476802  \n",
       "2    0.002520 -0.153920  0.293756 -0.187815  0.363499  0.159687  0.525286  \n",
       "3    0.464374  0.455905  0.528796  0.104778  0.395489  0.370452  0.871807  \n",
       "4    0.675694  0.686577  0.867544 -0.069715  0.407808  0.310482  0.880123  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "172 -2.104248 -0.504451 -0.612937 -0.544749 -0.490847 -0.575711 -0.186766  \n",
       "173  0.099659  0.677961  0.876450  0.702891  0.786348  0.816816  1.141525  \n",
       "174 -0.631446  0.277585  0.419303  0.280890  0.324307  0.330923  0.792864  \n",
       "175 -2.104248 -2.172128 -1.862757 -2.926084 -2.320411 -2.936509 -1.723978  \n",
       "176 -2.104248 -2.172128 -1.862757 -2.926084 -2.320411 -2.936509 -1.723978  \n",
       "\n",
       "[177 rows x 374 columns]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputeWideDFMinOr0(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and scale the data as in the pipeline below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform, matching the pipeline\n",
    "\n",
    "X_transformed = transformX(X, RobustScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# scale to same variance, this matches the pipeline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_transformed)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_transformed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection with recursive feature extraction (RFE)\n",
    "* tune minFeaturesToSelect and cv to retain ~60-150 proteins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def customRFECV (X_input):\n",
    "\n",
    "    global selected_features\n",
    "    global rfe\n",
    "    global min_features_to_select\n",
    "    \n",
    "    estimator = RandomForestClassifier(random_state=seed)\n",
    "    \n",
    "    minFeaturesToSelect = 100\n",
    "    \n",
    "    rfe = RFECV(\n",
    "        estimator=estimator, \n",
    "        \n",
    "        cv=3, \n",
    "        scoring= make_scorer(quadratic_weighted_kappa),\n",
    "        n_jobs = nJobs,\n",
    "        step = 1,\n",
    "    )\n",
    "    \n",
    "    rfe.fit(X_scaled, y)\n",
    "    \n",
    "    selected_features = []\n",
    "    \n",
    "    for i, feature in enumerate(X_input.columns):\n",
    "        if rfe.support_[i]:\n",
    "            selected_features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customRFECV(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "\n",
    "fileName = '_selectedFeaturesRFECV_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_wholeDataset'\n",
    "\n",
    "with open(fileName + '.json', 'w') as f:\n",
    "    json.dump(selected_features, f, indent=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to open\n",
    "\n",
    "fileName = '_selectedFeaturesRFECV_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_wholeDataset'\n",
    "\n",
    "with open(fileName + '.json', 'r') as f:\n",
    "    selected_features = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;Transforming Distribution&#x27;,\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                (&#x27;Standard Scaler&#x27;, StandardScaler()), (&#x27;Model&#x27;, None)])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-45\" type=\"checkbox\" ><label for=\"sk-estimator-id-45\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;Transforming Distribution&#x27;,\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                (&#x27;Standard Scaler&#x27;, StandardScaler()), (&#x27;Model&#x27;, None)])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-46\" type=\"checkbox\" ><label for=\"sk-estimator-id-46\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler(quantile_range=(25, 75))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-47\" type=\"checkbox\" ><label for=\"sk-estimator-id-47\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-48\" type=\"checkbox\" ><label for=\"sk-estimator-id-48\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">None</label><div class=\"sk-toggleable__content\"><pre>None</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('Transforming Distribution',\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                ('Standard Scaler', StandardScaler()), ('Model', None)])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make an empty sklearn pipeline without a classifier\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Transforming Distribution',  RobustScaler(with_centering = True, with_scaling = True, quantile_range=(25, 75), unit_variance = False)),\n",
    "    ('Standard Scaler', StandardScaler()),\n",
    "    ('Model', None),\n",
    "])\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = dfMLFromDisk.copy(deep = True)\n",
    "\n",
    "categorical_features,continuous_features, binary_features = columns_catNumOrBin(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "\n",
    "df = dfMLFromDisk.copy(deep = True)\n",
    "\n",
    "# filtered proteins based on RFE above\n",
    "fileName = '_selectedFeaturesRFECV_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_wholeDataset'\n",
    "\n",
    "with open(fileName + '.json', 'r') as f:\n",
    "    selected_featuresFromDisk = json.load(f)\n",
    "\n",
    "colsToFilter = []\n",
    "colsToFilter = copy.deepcopy(selected_featuresFromDisk)\n",
    "colsToFilter.append(target)\n",
    "df = df[colsToFilter] # selected features from RFE\n",
    "\n",
    "X, y = X_y_split(df, target)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = pd.Series(le.fit_transform(y))\n",
    "\n",
    "categorical_features,continuous_features, binary_features = columns_catNumOrBin(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>C1QB</th>\n",
       "      <th>C1R</th>\n",
       "      <th>CD5L</th>\n",
       "      <th>FGA</th>\n",
       "      <th>FGG</th>\n",
       "      <th>P4HB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.498124</td>\n",
       "      <td>-1.252469</td>\n",
       "      <td>1.417838</td>\n",
       "      <td>0.230725</td>\n",
       "      <td>2.364291</td>\n",
       "      <td>-0.362685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.498124</td>\n",
       "      <td>-1.252469</td>\n",
       "      <td>1.077541</td>\n",
       "      <td>0.230725</td>\n",
       "      <td>2.424627</td>\n",
       "      <td>-0.143682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.498124</td>\n",
       "      <td>-1.252469</td>\n",
       "      <td>1.135589</td>\n",
       "      <td>0.230725</td>\n",
       "      <td>2.304597</td>\n",
       "      <td>-0.321209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.498124</td>\n",
       "      <td>-1.252469</td>\n",
       "      <td>1.406347</td>\n",
       "      <td>0.230725</td>\n",
       "      <td>2.353432</td>\n",
       "      <td>-0.145631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.498124</td>\n",
       "      <td>-1.252469</td>\n",
       "      <td>1.093112</td>\n",
       "      <td>0.230725</td>\n",
       "      <td>2.559772</td>\n",
       "      <td>-0.190198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>172</td>\n",
       "      <td>1.076128</td>\n",
       "      <td>0.524960</td>\n",
       "      <td>0.795438</td>\n",
       "      <td>1.829274</td>\n",
       "      <td>1.978604</td>\n",
       "      <td>-2.386640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>173</td>\n",
       "      <td>1.398414</td>\n",
       "      <td>0.760424</td>\n",
       "      <td>1.486885</td>\n",
       "      <td>2.220795</td>\n",
       "      <td>2.376403</td>\n",
       "      <td>-2.151825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>174</td>\n",
       "      <td>1.278032</td>\n",
       "      <td>0.827503</td>\n",
       "      <td>1.638920</td>\n",
       "      <td>1.723370</td>\n",
       "      <td>1.921522</td>\n",
       "      <td>-2.386640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>175</td>\n",
       "      <td>1.202623</td>\n",
       "      <td>0.741566</td>\n",
       "      <td>1.476451</td>\n",
       "      <td>1.676672</td>\n",
       "      <td>1.738884</td>\n",
       "      <td>-2.386640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>176</td>\n",
       "      <td>1.201461</td>\n",
       "      <td>0.752376</td>\n",
       "      <td>1.513334</td>\n",
       "      <td>1.510180</td>\n",
       "      <td>1.541008</td>\n",
       "      <td>-2.386640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0      C1QB       C1R      CD5L       FGA       FGG      P4HB\n",
       "0             0 -0.498124 -1.252469  1.417838  0.230725  2.364291 -0.362685\n",
       "1             1 -0.498124 -1.252469  1.077541  0.230725  2.424627 -0.143682\n",
       "2             2 -0.498124 -1.252469  1.135589  0.230725  2.304597 -0.321209\n",
       "3             3 -0.498124 -1.252469  1.406347  0.230725  2.353432 -0.145631\n",
       "4             4 -0.498124 -1.252469  1.093112  0.230725  2.559772 -0.190198\n",
       "..          ...       ...       ...       ...       ...       ...       ...\n",
       "172         172  1.076128  0.524960  0.795438  1.829274  1.978604 -2.386640\n",
       "173         173  1.398414  0.760424  1.486885  2.220795  2.376403 -2.151825\n",
       "174         174  1.278032  0.827503  1.638920  1.723370  1.921522 -2.386640\n",
       "175         175  1.202623  0.741566  1.476451  1.676672  1.738884 -2.386640\n",
       "176         176  1.201461  0.752376  1.513334  1.510180  1.541008 -2.386640\n",
       "\n",
       "[177 rows x 7 columns]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# impute 0s, as above\n",
    "\n",
    "imputeWideDFMinOr0(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of classification algorithms\n",
    "\n",
    "classifiers = [\n",
    "    ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=3)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation:\n",
      "\n",
      "\n",
      "\n",
      "KNeighborsClassifier Classifier:\n",
      "\n",
      "\n",
      "  Mean Quadratic Weighted Kappa: 0.8755 ± 0.1508\n",
      "\n",
      "  Matthews correlation coefficient: 0.8890 ± 0.1309\n",
      "\n",
      "  F1 weighted score: 0.9434 ± 0.0675\n",
      "\n",
      "  Accuracy (normalized): 0.9504 ± 0.0614\n",
      "\n",
      "  CrossValScore: 0.7089 ± 0.1891\n"
     ]
    }
   ],
   "source": [
    "# perform cross validation on every model in the list. Use of a network computer cluster may be required\n",
    "\n",
    "print('\\nCross-Validation:')\n",
    "\n",
    "for j, (name, clf) in enumerate(classifiers):\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresCrossValScore = []\n",
    "    r2_scores = []\n",
    "    pipeline.set_params(Model = clf)\n",
    "    \n",
    "    print('\\n')\n",
    "    print(f'\\n{name} Classifier:\\n')\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        \n",
    "        #print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa: {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use Optuna to individually tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optuna logger\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# KNeighborsClassifier\n",
    "\n",
    "def objectiveKNeighborsClassifier(trial):\n",
    "    params = {\n",
    "        'n_neighbors': trial.suggest_int('n_neighbors', 1,30),\n",
    "        'weights': trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        'metric': trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        'algorithm': trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),\n",
    "        'leaf_size': trial.suggest_int('leaf_size', 2,200),\n",
    "        'p': trial.suggest_float('p', 0.1,10),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = KNeighborsClassifier(**params, n_jobs = nJobs))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreAccuracy\n",
    "\n",
    "studyName = 'studyKNeighborsClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_wholeDataset'\n",
    "storagestudyKNeighborsClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyKNeighborsClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyKNeighborsClassifier, load_if_exists=True)\n",
    "studyKNeighborsClassifier.optimize(objectiveKNeighborsClassifier, n_trials = 500, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNeighborsClassifier:\n",
      "\n",
      "Number of finished trials: 551\n",
      "\n",
      " Best RMSE score = 0.9718333333333333 \n",
      "\n",
      "\n",
      " Best Params = {'n_neighbors': 1, 'weights': 'distance', 'metric': 'manhattan', 'algorithm': 'auto', 'leaf_size': 169, 'p': 2.976730890736924} \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['KNeighborsClassifierTuned_target-dfHarmonizedCancer_nSampleFilter60_trainFrac-0.75_wholeDataset.pkl']"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_paramsKNeighborsClassifier = studyKNeighborsClassifier.best_params\n",
    "best_rmse_scoreKNeighborsClassifier = studyKNeighborsClassifier.best_value\n",
    "\n",
    "print(f'\\nKNeighborsClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyKNeighborsClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreKNeighborsClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsKNeighborsClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyKNeighborsClassifier, 'KNeighborsClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_wholeDataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plotly.com"
       },
       "data": [
        {
         "mode": "markers",
         "name": "Objective Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550
         ],
         "y": [
          0.9359007936507936,
          0.8224007936507935,
          0.7980515873015874,
          0.938718253968254,
          0.8996626984126983,
          0.9148928571428573,
          0.8291190476190476,
          0.9109246031746032,
          0.912079365079365,
          0.8696230158730159,
          0.9138650793650794,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9127539682539683,
          0.9718333333333333,
          0.8537222222222224,
          0.9171984126984128,
          0.9290238095238095,
          0.8922777777777778,
          0.9664047619047621,
          0.9718333333333333,
          0.9141269841269842,
          0.9469484126984128,
          0.9718333333333333,
          0.9121269841269841,
          0.9647380952380954,
          0.9267460317460318,
          0.9032103174603173,
          0.9469484126984128,
          0.8376746031746031,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.938718253968254,
          0.9503531746031747,
          0.9250793650793651,
          0.906424603174603,
          0.9651309523809525,
          0.924138888888889,
          0.9482698412698415,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9267460317460318,
          0.8824761904761905,
          0.9651309523809525,
          0.9520198412698414,
          0.938718253968254,
          0.805515873015873,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.9093492063492065,
          0.9056746031746034,
          0.938718253968254,
          0.9469484126984128,
          0.8320079365079365,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9664047619047621,
          0.9320674603174604,
          0.9250793650793651,
          0.9718333333333333,
          0.938718253968254,
          0.8922777777777778,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9520198412698414,
          0.9718333333333333,
          0.9320674603174604,
          0.9469484126984128,
          0.9095079365079366,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.879797619047619,
          0.938718253968254,
          0.9718333333333333,
          0.9503531746031747,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9350079365079367,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9171984126984127,
          0.9664047619047621,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9346507936507936,
          0.9718333333333333,
          0.9718333333333333,
          0.9357579365079366,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.9469484126984128,
          0.8318968253968254,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9647380952380954,
          0.9718333333333333,
          0.9469484126984128,
          0.9053888888888888,
          0.8855714285714287,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.8605793650793652,
          0.9718333333333333,
          0.9718333333333333,
          0.9357579365079366,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9458373015873016,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.9538690476190478,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9320674603174604,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9458373015873016,
          0.8951349206349206,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9049126984126984,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9357579365079366,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.9538690476190478,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9141269841269842,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9320674603174604,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9651309523809525,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9538690476190478,
          0.8605793650793652,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9513373015873018,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.8318968253968254,
          0.9718333333333333,
          0.9109246031746032,
          0.9127539682539683,
          0.9718333333333333,
          0.8897380952380953,
          0.9718333333333333,
          0.9718333333333333,
          0.9399246031746032,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9538690476190478,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9320674603174604,
          0.9718333333333333,
          0.8470158730158731,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9170436507936509,
          0.9121269841269841,
          0.9458373015873016,
          0.9718333333333333,
          0.9664047619047621,
          0.9538690476190478,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.8526150793650793,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.8951349206349206,
          0.9718333333333333,
          0.9718333333333333,
          0.9503531746031747,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9320674603174604,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9651309523809525,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9538690476190478,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.8213611111111112,
          0.9171984126984127,
          0.9357579365079366,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9651309523809525,
          0.9267460317460318,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.8824761904761905,
          0.9538690476190478,
          0.9171984126984128,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9320674603174604,
          0.9138650793650794,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9538690476190478,
          0.938718253968254,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9320674603174604,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9141269841269842,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.8989603174603176,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9357579365079366,
          0.8605793650793652,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.849793650793651,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9538690476190478,
          0.8897380952380953,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.8060873015873016,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9538690476190478,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9320674603174604,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9127539682539683,
          0.9458373015873016,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.8656825396825398,
          0.9718333333333333,
          0.9718333333333333,
          0.9095079365079366,
          0.9469484126984128,
          0.9718333333333333,
          0.9320674603174604,
          0.938718253968254,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9651309523809525,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.8922777777777778,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9538690476190478,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9320674603174604,
          0.9718333333333333,
          0.9458373015873016,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9121269841269841,
          0.9469484126984128,
          0.9538690476190478,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9651309523809525,
          0.9320674603174604,
          0.9664047619047621,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9171984126984127,
          0.9718333333333333
         ]
        },
        {
         "mode": "lines",
         "name": "Best Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550
         ],
         "y": [
          0.9359007936507936,
          0.9359007936507936,
          0.9359007936507936,
          0.938718253968254,
          0.938718253968254,
          0.938718253968254,
          0.938718253968254,
          0.938718253968254,
          0.938718253968254,
          0.938718253968254,
          0.938718253968254,
          0.938718253968254,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333
         ]
        },
        {
         "marker": {
          "color": "#cccccc"
         },
         "mode": "markers",
         "name": "Infeasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [],
         "y": []
        }
       ],
       "layout": {
        "autosize": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Optimization History Plot"
        },
        "xaxis": {
         "autorange": true,
         "range": [
          -40.28662420382165,
          590.2866242038217
         ],
         "title": {
          "text": "Trial"
         },
         "type": "linear"
        },
        "yaxis": {
         "autorange": true,
         "range": [
          0.783710375250424,
          0.9861745453844968
         ],
         "title": {
          "text": "Objective Value"
         },
         "type": "linear"
        }
       }
      },
      "image/png": "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",
      "text/html": [
       "<div>                            <div id=\"ec2b51b5-2196-466e-aca0-27a4a92e41a9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ec2b51b5-2196-466e-aca0-27a4a92e41a9\")) {                    Plotly.newPlot(                        \"ec2b51b5-2196-466e-aca0-27a4a92e41a9\",                        [{\"mode\":\"markers\",\"name\":\"Objective Value\",\"x\":[0,1,2,3,4,5,6,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550],\"y\":[0.9359007936507936,0.8224007936507935,0.7980515873015874,0.938718253968254,0.8996626984126983,0.9148928571428573,0.8291190476190476,0.9109246031746032,0.912079365079365,0.8696230158730159,0.9138650793650794,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9127539682539683,0.9718333333333333,0.8537222222222224,0.9171984126984128,0.9290238095238095,0.8922777777777778,0.9664047619047621,0.9718333333333333,0.9141269841269842,0.9469484126984128,0.9718333333333333,0.9121269841269841,0.9647380952380954,0.9267460317460318,0.9032103174603173,0.9469484126984128,0.8376746031746031,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.938718253968254,0.9503531746031747,0.9250793650793651,0.906424603174603,0.9651309523809525,0.924138888888889,0.9482698412698415,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9267460317460318,0.8824761904761905,0.9651309523809525,0.9520198412698414,0.938718253968254,0.805515873015873,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.9093492063492065,0.9056746031746034,0.938718253968254,0.9469484126984128,0.8320079365079365,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9664047619047621,0.9320674603174604,0.9250793650793651,0.9718333333333333,0.938718253968254,0.8922777777777778,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9520198412698414,0.9718333333333333,0.9320674603174604,0.9469484126984128,0.9095079365079366,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.879797619047619,0.938718253968254,0.9718333333333333,0.9503531746031747,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9350079365079367,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9171984126984127,0.9664047619047621,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9346507936507936,0.9718333333333333,0.9718333333333333,0.9357579365079366,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.9469484126984128,0.8318968253968254,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9647380952380954,0.9718333333333333,0.9469484126984128,0.9053888888888888,0.8855714285714287,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.8605793650793652,0.9718333333333333,0.9718333333333333,0.9357579365079366,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9458373015873016,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.9538690476190478,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9320674603174604,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9458373015873016,0.8951349206349206,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9049126984126984,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9357579365079366,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.9538690476190478,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9141269841269842,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9320674603174604,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9651309523809525,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9538690476190478,0.8605793650793652,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9513373015873018,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.8318968253968254,0.9718333333333333,0.9109246031746032,0.9127539682539683,0.9718333333333333,0.8897380952380953,0.9718333333333333,0.9718333333333333,0.9399246031746032,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9538690476190478,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9320674603174604,0.9718333333333333,0.8470158730158731,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9170436507936509,0.9121269841269841,0.9458373015873016,0.9718333333333333,0.9664047619047621,0.9538690476190478,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.8526150793650793,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.8951349206349206,0.9718333333333333,0.9718333333333333,0.9503531746031747,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9320674603174604,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9651309523809525,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9538690476190478,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.8213611111111112,0.9171984126984127,0.9357579365079366,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9651309523809525,0.9267460317460318,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.8824761904761905,0.9538690476190478,0.9171984126984128,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9320674603174604,0.9138650793650794,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9538690476190478,0.938718253968254,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9320674603174604,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9141269841269842,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.8989603174603176,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9357579365079366,0.8605793650793652,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.849793650793651,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9538690476190478,0.8897380952380953,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.8060873015873016,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9538690476190478,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9320674603174604,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9127539682539683,0.9458373015873016,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.8656825396825398,0.9718333333333333,0.9718333333333333,0.9095079365079366,0.9469484126984128,0.9718333333333333,0.9320674603174604,0.938718253968254,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9651309523809525,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.8922777777777778,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9538690476190478,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9320674603174604,0.9718333333333333,0.9458373015873016,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9121269841269841,0.9469484126984128,0.9538690476190478,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9651309523809525,0.9320674603174604,0.9664047619047621,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9171984126984127,0.9718333333333333],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Best Value\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550],\"y\":[0.9359007936507936,0.9359007936507936,0.9359007936507936,0.938718253968254,0.938718253968254,0.938718253968254,0.938718253968254,0.938718253968254,0.938718253968254,0.938718253968254,0.938718253968254,0.938718253968254,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333],\"type\":\"scatter\"},{\"marker\":{\"color\":\"#cccccc\"},\"mode\":\"markers\",\"name\":\"Infeasible Trial\",\"showlegend\":false,\"x\":[],\"y\":[],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Optimization History Plot\"},\"xaxis\":{\"title\":{\"text\":\"Trial\"}},\"yaxis\":{\"title\":{\"text\":\"Objective Value\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ec2b51b5-2196-466e-aca0-27a4a92e41a9');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optuna.visualization.plot_optimization_history(studyKNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
