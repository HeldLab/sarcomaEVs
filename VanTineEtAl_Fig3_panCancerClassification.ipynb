{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# To analyze and classify the harmonized dataset\n",
    "* note that some classifiers may require using a computer cluster due to memory requirements\n",
    "* These package versions were used for the manuscript: scikit-learn v1.3.2, catboost v1.2.3, xgboost v2.0.3, lightgbm v4.3.0, joblib v1.3.2, optuna v3.5.0, numpy v1.23.4, pandas v2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from numpy import ma\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "from glob import glob\n",
    "import glob\n",
    "from statistics import mean\n",
    "import random\n",
    "import copy\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "import scikitplot as skplt\n",
    "import optuna\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import random\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier, StackingClassifier, VotingClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import RFECV, RFE, SelectKBest, chi2\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier, ElasticNetCV, LassoLarsCV, ElasticNet, LogisticRegression\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, f1_score, matthews_corrcoef, mean_squared_error, r2_score, mean_absolute_error, cohen_kappa_score, make_scorer\n",
    "from sklearn.metrics import classification_report, roc_auc_score, auc, RocCurveDisplay, roc_curve\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, QuantileTransformer, FunctionTransformer, MinMaxScaler, PowerTransformer, OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "\n",
    "os.chdir('/Users/jasonheld/Manuscripts/2022_Sarcoma-Exosomes/vanTine_001_FinalDataWithControls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some lists to fill later\n",
    "\n",
    "dfTrainModelEvalAllSeeds = pd.DataFrame()\n",
    "seedListStats = []\n",
    "trainFracListStats = []\n",
    "finalClassifierList = []\n",
    "tunedClassifierListForValidation = []\n",
    "finalImportancesMeanList = []\n",
    "finalFeaturesList = []\n",
    "\n",
    "y_testList = []\n",
    "y_predList = []\n",
    "y_probList = []\n",
    "seedList = []\n",
    "seedListStats = []\n",
    "trainFracListStats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key parameters\n",
    "\n",
    "seed = 7 # use seeds 1-7 for each classification to randomize 7 times\n",
    "np.random.seed(7) # update/match the seed value\n",
    "target = 'dfHarmonizedCancer'\n",
    "quantType = 'areaHarmonized'\n",
    "trainFrac = .75\n",
    "nJobs = 6\n",
    "nSamplesVal = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a dataframe and a target variable to create an X (predictors) dataframe and a y Series\n",
    "\n",
    "def X_y_split(df, target):\n",
    "    \n",
    "    categorical_features = []\n",
    "    continuous_features = []\n",
    "    binary_features = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            categorical_features.append(col)\n",
    "        \n",
    "        else:\n",
    "            if df[col].nunique() <= 2:\n",
    "                binary_features.append(col)\n",
    "            else:\n",
    "                continuous_features.append(col)\n",
    "                \n",
    "    if categorical_features:\n",
    "        if target in categorical_features:\n",
    "            categorical_features.remove(target)\n",
    "        df.drop(categorical_features, axis = 1, inplace = True)\n",
    "\n",
    "    X, y = df.drop([target], axis = 1), df[target]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function parses columns by type.\n",
    "\n",
    "def columns_catNumOrBin(df):\n",
    "\n",
    "    categorical_features = []\n",
    "    continuous_features = []\n",
    "    binary_features = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            categorical_features.append(col)\n",
    "        else:\n",
    "            if df[col].nunique() <= 2:\n",
    "                binary_features.append(col)\n",
    "            else:\n",
    "                continuous_features.append(col)\n",
    "    \n",
    "    return categorical_features,continuous_features, binary_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute 0s with min peptide signal intenisty detected\n",
    "\n",
    "def imputeWideDFMinOr0(df):\n",
    "    \n",
    "    for col in df.columns:\n",
    "\n",
    "        if df[col].dtype == object:\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            if quantType == 'areaHarmonized':\n",
    "                df[col].fillna(value = df[col].min(), inplace = True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load harmonized output from the Harmonizernotebook or the dfHarmonized.xlsx file from the SarcomaEV Github page\n",
    "\n",
    "dfHarmonized = pd.read_excel('/Users/jasonheld/Manuscripts/2022_Sarcoma-Exosomes/Figure2_Data_CancerVsNormal/dfHarmonized.xlsx', index_col=0)\n",
    "dfHarmonized.rename({'cancer':'dfHarmonizedCancer'}, axis='columns', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidyDFHarmonized (df):\n",
    "\n",
    "    global quantType\n",
    "    \n",
    "    df = df.loc[:,~df.columns.str.contains('CON_')] # remove contaminants    \n",
    "    df.drop(columns = ['areaHarmonizedAntilog', 'areaScaled', 'area'], inplace = True)\n",
    "\n",
    "    totalCols = df.columns\n",
    "    numCols = df._get_numeric_data().columns\n",
    "    catCols = list(set(totalCols)-set(numCols))\n",
    "    catCols.remove('geneNamePrimary')\n",
    "    catCols.remove('sample')\n",
    "    catCols.remove(target)\n",
    "    \n",
    "    #remove rows without areas & duplicates\n",
    "    df.dropna(subset=[quantType], inplace=True)\n",
    "    df.drop_duplicates(inplace = True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidy the dataframe\n",
    "\n",
    "dfHarmonized = tidyDFHarmonized(dfHarmonized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter dfHarmonized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeCrapome(df):\n",
    "    crapome =['KRT1', 'KRT2', 'KRT3', 'KRT4', 'KRT5', 'KRT6', 'KRT7', 'KRT8', 'KRT9', 'KRT10', 'KRT11', 'KRT12', 'KRT13', 'KRT14', 'KRT15', 'KRT16', 'KRT17', 'KRT18', 'KRT19', 'KRT20', 'KRT21', 'KRT22', 'KRT23', 'KRT24']\n",
    "    df.query('geneNamePrimary not in @crapome', inplace = True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFiltered = removeCrapome(dfHarmonized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count protein IDs\n",
    "\n",
    "def filterNDetectionsPerProtAcc(df, nSamplesVal_):\n",
    "\n",
    "    global nSamplesVal\n",
    "    nSamplesVal = nSamplesVal_\n",
    "    print(nSamplesVal)\n",
    "    \n",
    "    df = df.query('nSamples >= @nSamplesVal')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "dfFiltered = filterNDetectionsPerProtAcc(dfFiltered, nSamplesVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### impute NANs with minimum intensity in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputeDfMinOr0 (df, colToImpute):\n",
    "    \n",
    "    df[colToImpute].fillna(value = df[colToImpute].min(), inplace = True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFiltered = imputeDfMinOr0(dfFiltered, 'areaHarmonized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pivot to wide data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns by feature type\n",
    "categorical_features,continuous_features, binary_features = columns_catNumOrBin(dfFiltered)\n",
    "\n",
    "# pivot df by geneNamePrimary\n",
    "indexListTemp = categorical_features.copy()\n",
    "indexListTemp.remove('geneNamePrimary')\n",
    "indexListTemp.remove('protAcc')\n",
    "indexListTemp.remove('Reviewed')\n",
    "indexListTemp.remove('Protein names')\n",
    "\n",
    "dfML = pd.pivot_table(\n",
    "    dfFiltered,\n",
    "    values = 'areaHarmonized',\n",
    "    columns = 'geneNamePrimary',\n",
    "    index = ['sample', target]\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove categorical columns\n",
    "\n",
    "categorical_features,continuous_features, binary_features = columns_catNumOrBin(dfML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfRemoveAllCatColsButTarget(df):\n",
    "    \n",
    "    categorical_features = []\n",
    "    continuous_features = []\n",
    "    binary_features = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            categorical_features.append(col)\n",
    "        \n",
    "        else:\n",
    "            if df[col].nunique() <= 2:\n",
    "                binary_features.append(col)\n",
    "            else:\n",
    "                continuous_features.append(col)\n",
    "    \n",
    "    categorical_features.remove(target)\n",
    "        \n",
    "    df.drop(categorical_features, axis = 1, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML = dfRemoveAllCatColsButTarget(dfML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dfML into training, validation, and testing datasets prior to any transformation or feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfML.copy()\n",
    "\n",
    "# random splitting into each set\n",
    "dfTrain = df.groupby(target, group_keys=False).sample(frac=trainFrac, random_state=seed)\n",
    "dfTest = df[~df.index.isin(dfTrain.index)]\n",
    "\n",
    "# validation fraction, samples taken from dfTrain.\n",
    "valFrac = .20\n",
    "\n",
    "dfValidation = dfTrain.groupby(target, group_keys=False).sample(frac=valFrac, random_state=seed)\n",
    "dfTrain= dfTrain[~dfTrain.index.isin(dfValidation.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "df = dfML\n",
    "df = df.reset_index(drop = True)\n",
    "df.to_excel('dfML-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '.xlsx')\n",
    "\n",
    "df = dfTest\n",
    "df = df.reset_index(drop = True)\n",
    "df.to_excel('_dfTest_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.xlsx')\n",
    "\n",
    "df = dfValidation\n",
    "df = df.reset_index(drop = True)\n",
    "df.to_excel('_dfValidation_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.xlsx')\n",
    "\n",
    "df = dfTrain\n",
    "df = df.reset_index(drop = True)\n",
    "df.to_excel('_dfTrain_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "\n",
    "X, y = X_y_split(dfTrain, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>geneNamePrimary</th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A2M</th>\n",
       "      <th>ACTA1</th>\n",
       "      <th>ACTB</th>\n",
       "      <th>ACTBL2</th>\n",
       "      <th>ACTG1</th>\n",
       "      <th>ACTG2</th>\n",
       "      <th>ACTN1</th>\n",
       "      <th>ACTN4</th>\n",
       "      <th>ADAM10</th>\n",
       "      <th>...</th>\n",
       "      <th>VIM</th>\n",
       "      <th>VTN</th>\n",
       "      <th>VWF</th>\n",
       "      <th>WDR1</th>\n",
       "      <th>YWHAB</th>\n",
       "      <th>YWHAE</th>\n",
       "      <th>YWHAG</th>\n",
       "      <th>YWHAH</th>\n",
       "      <th>YWHAQ</th>\n",
       "      <th>YWHAZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.427119</td>\n",
       "      <td>2.065347</td>\n",
       "      <td>0.530682</td>\n",
       "      <td>-0.129343</td>\n",
       "      <td>-0.423379</td>\n",
       "      <td>-0.024570</td>\n",
       "      <td>0.026106</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>1.526734</td>\n",
       "      <td>1.481474</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-0.274566</td>\n",
       "      <td>-0.196867</td>\n",
       "      <td>-0.607559</td>\n",
       "      <td>-0.250478</td>\n",
       "      <td>-0.308128</td>\n",
       "      <td>-0.316296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.376259</td>\n",
       "      <td>2.141387</td>\n",
       "      <td>1.295984</td>\n",
       "      <td>0.638655</td>\n",
       "      <td>0.628846</td>\n",
       "      <td>0.946476</td>\n",
       "      <td>0.874163</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764930</td>\n",
       "      <td>0.673287</td>\n",
       "      <td>1.111831</td>\n",
       "      <td>-0.828170</td>\n",
       "      <td>-0.138130</td>\n",
       "      <td>0.010782</td>\n",
       "      <td>-0.160418</td>\n",
       "      <td>-0.101406</td>\n",
       "      <td>-0.190496</td>\n",
       "      <td>0.486893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.562377</td>\n",
       "      <td>2.207797</td>\n",
       "      <td>-0.140634</td>\n",
       "      <td>-0.330657</td>\n",
       "      <td>-0.571826</td>\n",
       "      <td>-0.477617</td>\n",
       "      <td>-0.717801</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>1.117175</td>\n",
       "      <td>1.010081</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-1.796210</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.604599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.772372</td>\n",
       "      <td>1.888034</td>\n",
       "      <td>-0.285125</td>\n",
       "      <td>-1.099648</td>\n",
       "      <td>-0.873302</td>\n",
       "      <td>-0.910295</td>\n",
       "      <td>-0.877917</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>1.394733</td>\n",
       "      <td>0.326443</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-1.796210</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.604599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>-0.028891</td>\n",
       "      <td>1.900702</td>\n",
       "      <td>-0.124414</td>\n",
       "      <td>-0.925085</td>\n",
       "      <td>-0.530581</td>\n",
       "      <td>-0.944076</td>\n",
       "      <td>-0.699827</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>0.895636</td>\n",
       "      <td>1.436516</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-1.796210</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.604599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.321744</td>\n",
       "      <td>1.953634</td>\n",
       "      <td>-0.390985</td>\n",
       "      <td>-1.026904</td>\n",
       "      <td>-1.338389</td>\n",
       "      <td>-0.779894</td>\n",
       "      <td>-1.079842</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>0.922462</td>\n",
       "      <td>0.045334</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-1.796210</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.604599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.389805</td>\n",
       "      <td>2.111616</td>\n",
       "      <td>-0.070390</td>\n",
       "      <td>-0.718909</td>\n",
       "      <td>-1.749931</td>\n",
       "      <td>-0.526616</td>\n",
       "      <td>-0.639962</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>1.297184</td>\n",
       "      <td>0.053385</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-1.796210</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.604599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.531825</td>\n",
       "      <td>2.062484</td>\n",
       "      <td>0.391271</td>\n",
       "      <td>-0.456651</td>\n",
       "      <td>-0.667611</td>\n",
       "      <td>-0.169866</td>\n",
       "      <td>-0.128379</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>0.803730</td>\n",
       "      <td>0.231407</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-1.796210</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.604599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.334590</td>\n",
       "      <td>2.042939</td>\n",
       "      <td>-0.097486</td>\n",
       "      <td>-0.776217</td>\n",
       "      <td>-0.758497</td>\n",
       "      <td>-0.726377</td>\n",
       "      <td>-0.669988</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>0.853324</td>\n",
       "      <td>-2.207381</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-1.796210</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.604599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.066970</td>\n",
       "      <td>2.261982</td>\n",
       "      <td>0.166255</td>\n",
       "      <td>-0.506707</td>\n",
       "      <td>-0.249093</td>\n",
       "      <td>-0.603873</td>\n",
       "      <td>-0.377728</td>\n",
       "      <td>-1.265311</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282797</td>\n",
       "      <td>1.024570</td>\n",
       "      <td>0.678333</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-1.796210</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.604599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 373 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "geneNamePrimary      A1BG       A2M     ACTA1      ACTB    ACTBL2     ACTG1  \\\n",
       "110              0.427119  2.065347  0.530682 -0.129343 -0.423379 -0.024570   \n",
       "146              0.376259  2.141387  1.295984  0.638655  0.628846  0.946476   \n",
       "126              0.562377  2.207797 -0.140634 -0.330657 -0.571826 -0.477617   \n",
       "121              0.772372  1.888034 -0.285125 -1.099648 -0.873302 -0.910295   \n",
       "152             -0.028891  1.900702 -0.124414 -0.925085 -0.530581 -0.944076   \n",
       "..                    ...       ...       ...       ...       ...       ...   \n",
       "86               0.321744  1.953634 -0.390985 -1.026904 -1.338389 -0.779894   \n",
       "102              0.389805  2.111616 -0.070390 -0.718909 -1.749931 -0.526616   \n",
       "175              0.531825  2.062484  0.391271 -0.456651 -0.667611 -0.169866   \n",
       "96               0.334590  2.042939 -0.097486 -0.776217 -0.758497 -0.726377   \n",
       "156              0.066970  2.261982  0.166255 -0.506707 -0.249093 -0.603873   \n",
       "\n",
       "geneNamePrimary     ACTG2     ACTN1     ACTN4    ADAM10  ...       VIM  \\\n",
       "110              0.026106 -1.836067 -1.606185 -1.860046  ... -1.030240   \n",
       "146              0.874163 -1.836067 -1.606185 -1.860046  ...  0.764930   \n",
       "126             -0.717801 -1.836067 -1.606185 -1.860046  ... -1.030240   \n",
       "121             -0.877917 -1.836067 -1.606185 -1.860046  ... -1.030240   \n",
       "152             -0.699827 -1.836067 -1.606185 -1.860046  ... -1.030240   \n",
       "..                    ...       ...       ...       ...  ...       ...   \n",
       "86              -1.079842 -1.836067 -1.606185 -1.860046  ... -1.030240   \n",
       "102             -0.639962 -1.836067 -1.606185 -1.860046  ... -1.030240   \n",
       "175             -0.128379 -1.836067 -1.606185 -1.860046  ... -1.030240   \n",
       "96              -0.669988 -1.836067 -1.606185 -1.860046  ... -1.030240   \n",
       "156             -0.377728 -1.265311 -1.606185 -1.860046  ...  0.282797   \n",
       "\n",
       "geneNamePrimary       VTN       VWF      WDR1     YWHAB     YWHAE     YWHAG  \\\n",
       "110              1.526734  1.481474 -2.104248 -0.274566 -0.196867 -0.607559   \n",
       "146              0.673287  1.111831 -0.828170 -0.138130  0.010782 -0.160418   \n",
       "126              1.117175  1.010081 -2.104248 -1.796210 -1.862757 -2.926084   \n",
       "121              1.394733  0.326443 -2.104248 -1.796210 -1.862757 -2.926084   \n",
       "152              0.895636  1.436516 -2.104248 -1.796210 -1.862757 -2.926084   \n",
       "..                    ...       ...       ...       ...       ...       ...   \n",
       "86               0.922462  0.045334 -2.104248 -1.796210 -1.862757 -2.926084   \n",
       "102              1.297184  0.053385 -2.104248 -1.796210 -1.862757 -2.926084   \n",
       "175              0.803730  0.231407 -2.104248 -1.796210 -1.862757 -2.926084   \n",
       "96               0.853324 -2.207381 -2.104248 -1.796210 -1.862757 -2.926084   \n",
       "156              1.024570  0.678333 -2.104248 -1.796210 -1.862757 -2.926084   \n",
       "\n",
       "geneNamePrimary     YWHAH     YWHAQ     YWHAZ  \n",
       "110             -0.250478 -0.308128 -0.316296  \n",
       "146             -0.101406 -0.190496  0.486893  \n",
       "126             -2.320411 -2.936509 -1.604599  \n",
       "121             -2.320411 -2.936509 -1.604599  \n",
       "152             -2.320411 -2.936509 -1.604599  \n",
       "..                    ...       ...       ...  \n",
       "86              -2.320411 -2.936509 -1.604599  \n",
       "102             -2.320411 -2.936509 -1.604599  \n",
       "175             -2.320411 -2.936509 -1.604599  \n",
       "96              -2.320411 -2.936509 -1.604599  \n",
       "156             -2.320411 -2.936509 -1.604599  \n",
       "\n",
       "[107 rows x 373 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputeWideDFMinOr0(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and scale the data as in the pipeline below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale each gene separately.\n",
    "\n",
    "def transformX (X, transformer):\n",
    "    \n",
    "    if transformer == RobustScaler:\n",
    "        transformer = RobustScaler(with_centering = True, with_scaling = True, quantile_range=(25, 75), unit_variance = False)\n",
    "\n",
    "    X_transformed = transformer.fit_transform(X)\n",
    "    X_transformed = pd.DataFrame(X_transformed, columns=X.columns) # turn back into df\n",
    "\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform, matching the pipeline\n",
    "\n",
    "X_transformed = transformX(X, RobustScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# scale to same variance, matching the pipeline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_transformed)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_transformed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection with recursive feature extraction (RFE)\n",
    "* tune minFeaturesToSelect and cv to retain ~60-150 proteins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CV strategy\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state = seed)\n",
    "cv_splits = list(cv.split(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scoring\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    '''\n",
    "    This function returns an evaluation metric\n",
    "    '''\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def customRFECV (X_input):\n",
    "\n",
    "    global selected_features\n",
    "    global rfe\n",
    "    global min_features_to_select\n",
    "    \n",
    "    estimator = RandomForestClassifier(random_state=seed)\n",
    "\n",
    "    # for seed 1 = use minFeaturesToSelect = 75 and cv = 6,\n",
    "    # for seed 2 = use minFeaturesToSelect = 125 and cv = 6,\n",
    "    # for seed 3 = use minFeaturesToSelect = 100 and cv = 5,\n",
    "    # for seed 4 = use minFeaturesToSelect = 100 and cv = 3,\n",
    "    # for seed 5 = use minFeaturesToSelect = 100 and cv = 5,\n",
    "    # for seed 6 = use minFeaturesToSelect = 50 and cv = 5,\n",
    "    # for seed 7 = use minFeaturesToSelect = 100 and cv = 4,\n",
    "    \n",
    "    minFeaturesToSelect = 100\n",
    "    \n",
    "    rfe = RFECV(\n",
    "        estimator=estimator, \n",
    "        \n",
    "        cv=4, \n",
    "        scoring= make_scorer(quadratic_weighted_kappa),\n",
    "        n_jobs = nJobs,\n",
    "        step = 1,\n",
    "    )\n",
    "    \n",
    "    rfe.fit(X_scaled, y)\n",
    "    \n",
    "    selected_features = []\n",
    "    \n",
    "    for i, feature in enumerate(X_input.columns):\n",
    "        if rfe.support_[i]:\n",
    "            selected_features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "customRFECV(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "\n",
    "fileName = '_selectedFeaturesRFECV_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "\n",
    "with open(fileName + '.json', 'w') as f:\n",
    "    json.dump(selected_features, f, indent=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to open\n",
    "\n",
    "fileName = '_selectedFeaturesRFECV_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "\n",
    "with open(fileName + '.json', 'r') as f:\n",
    "    selected_features = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;Transforming Distribution&#x27;,\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                (&#x27;Standard Scaler&#x27;, StandardScaler()), (&#x27;Model&#x27;, None)])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;Transforming Distribution&#x27;,\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                (&#x27;Standard Scaler&#x27;, StandardScaler()), (&#x27;Model&#x27;, None)])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler(quantile_range=(25, 75))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">None</label><div class=\"sk-toggleable__content\"><pre>None</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('Transforming Distribution',\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                ('Standard Scaler', StandardScaler()), ('Model', None)])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make an empty sklearn pipeline without a classifier\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Transforming Distribution',  RobustScaler(with_centering = True, with_scaling = True, quantile_range=(25, 75), unit_variance = False)),\n",
    "    ('Standard Scaler', StandardScaler()),\n",
    "    ('Model', None),\n",
    "])\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with training data\n",
    "\n",
    "df = dfTrain.copy(deep = True)\n",
    "\n",
    "categorical_features,continuous_features, binary_features = columns_catNumOrBin(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "\n",
    "df = dfTrain.copy(deep = True)\n",
    "\n",
    "# filtered proteins based on RFE above\n",
    "fileName = '_selectedFeaturesRFECV_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "\n",
    "with open(fileName + '.json', 'r') as f:\n",
    "    selected_featuresFromDisk = json.load(f)\n",
    "\n",
    "colsToFilter = []\n",
    "colsToFilter = copy.deepcopy(selected_featuresFromDisk)\n",
    "colsToFilter.append(target)\n",
    "df = df[colsToFilter] # selected features from RFE\n",
    "\n",
    "X, y = X_y_split(df, target)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = pd.Series(le.fit_transform(y))\n",
    "\n",
    "categorical_features,continuous_features, binary_features = columns_catNumOrBin(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>geneNamePrimary</th>\n",
       "      <th>ACTBL2</th>\n",
       "      <th>ACTG1</th>\n",
       "      <th>ACTG2</th>\n",
       "      <th>ACTN1</th>\n",
       "      <th>ACTN4</th>\n",
       "      <th>ADAM10</th>\n",
       "      <th>ADAMTS13</th>\n",
       "      <th>ADIPOQ</th>\n",
       "      <th>AFM</th>\n",
       "      <th>AGT</th>\n",
       "      <th>...</th>\n",
       "      <th>SVEP1</th>\n",
       "      <th>TFRC</th>\n",
       "      <th>THBS1</th>\n",
       "      <th>TLN2</th>\n",
       "      <th>TPM3</th>\n",
       "      <th>TUBA8</th>\n",
       "      <th>UGT8</th>\n",
       "      <th>VCL</th>\n",
       "      <th>VIM</th>\n",
       "      <th>VWF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-0.423379</td>\n",
       "      <td>-0.024570</td>\n",
       "      <td>0.026106</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-2.440891</td>\n",
       "      <td>-0.395340</td>\n",
       "      <td>0.244759</td>\n",
       "      <td>0.418586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349093</td>\n",
       "      <td>1.513985</td>\n",
       "      <td>-1.410128</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>1.736182</td>\n",
       "      <td>-1.788465</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>1.481474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.628846</td>\n",
       "      <td>0.946476</td>\n",
       "      <td>0.874163</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-2.440891</td>\n",
       "      <td>-2.138182</td>\n",
       "      <td>0.372256</td>\n",
       "      <td>0.003020</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.110371</td>\n",
       "      <td>0.082788</td>\n",
       "      <td>-1.410128</td>\n",
       "      <td>-1.141835</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>1.688060</td>\n",
       "      <td>-0.647795</td>\n",
       "      <td>0.764930</td>\n",
       "      <td>1.111831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-0.571826</td>\n",
       "      <td>-0.477617</td>\n",
       "      <td>-0.717801</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-1.724302</td>\n",
       "      <td>0.106397</td>\n",
       "      <td>-0.695381</td>\n",
       "      <td>0.194567</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.110371</td>\n",
       "      <td>-0.506163</td>\n",
       "      <td>-0.164129</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-0.193621</td>\n",
       "      <td>1.885809</td>\n",
       "      <td>-1.788465</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>1.010081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>-0.873302</td>\n",
       "      <td>-0.910295</td>\n",
       "      <td>-0.877917</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-2.440891</td>\n",
       "      <td>-2.138182</td>\n",
       "      <td>0.555990</td>\n",
       "      <td>0.729403</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.110371</td>\n",
       "      <td>0.240194</td>\n",
       "      <td>-1.410128</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>1.447036</td>\n",
       "      <td>-1.788465</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>0.326443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>-0.530581</td>\n",
       "      <td>-0.944076</td>\n",
       "      <td>-0.699827</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-1.064995</td>\n",
       "      <td>-1.090575</td>\n",
       "      <td>-0.280086</td>\n",
       "      <td>0.113807</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.110371</td>\n",
       "      <td>-1.407680</td>\n",
       "      <td>0.330266</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>1.471968</td>\n",
       "      <td>-1.788465</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>1.436516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-1.338389</td>\n",
       "      <td>-0.779894</td>\n",
       "      <td>-1.079842</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-2.440891</td>\n",
       "      <td>-0.914108</td>\n",
       "      <td>0.179224</td>\n",
       "      <td>0.156814</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.245765</td>\n",
       "      <td>0.039113</td>\n",
       "      <td>-0.862686</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>1.432024</td>\n",
       "      <td>-1.788465</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>0.045334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-1.749931</td>\n",
       "      <td>-0.526616</td>\n",
       "      <td>-0.639962</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-2.440891</td>\n",
       "      <td>-2.138182</td>\n",
       "      <td>0.164797</td>\n",
       "      <td>0.325150</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.110371</td>\n",
       "      <td>0.134644</td>\n",
       "      <td>0.522581</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>0.030291</td>\n",
       "      <td>-1.001484</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>0.053385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>-0.667611</td>\n",
       "      <td>-0.169866</td>\n",
       "      <td>-0.128379</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-2.440891</td>\n",
       "      <td>-2.138182</td>\n",
       "      <td>0.152263</td>\n",
       "      <td>-1.038392</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569083</td>\n",
       "      <td>0.925464</td>\n",
       "      <td>0.394859</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>0.030291</td>\n",
       "      <td>-1.788465</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>0.231407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.758497</td>\n",
       "      <td>-0.726377</td>\n",
       "      <td>-0.669988</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-2.440891</td>\n",
       "      <td>-2.138182</td>\n",
       "      <td>0.541622</td>\n",
       "      <td>-0.894811</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.110371</td>\n",
       "      <td>-0.971333</td>\n",
       "      <td>-0.903362</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>1.257198</td>\n",
       "      <td>-1.788465</td>\n",
       "      <td>-1.030240</td>\n",
       "      <td>-2.207381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-0.249093</td>\n",
       "      <td>-0.603873</td>\n",
       "      <td>-0.377728</td>\n",
       "      <td>-1.265311</td>\n",
       "      <td>-1.606185</td>\n",
       "      <td>-1.860046</td>\n",
       "      <td>-2.440891</td>\n",
       "      <td>-1.255815</td>\n",
       "      <td>-1.160970</td>\n",
       "      <td>-0.060001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.454097</td>\n",
       "      <td>-0.305702</td>\n",
       "      <td>-1.410128</td>\n",
       "      <td>-1.974323</td>\n",
       "      <td>-1.859943</td>\n",
       "      <td>-1.954009</td>\n",
       "      <td>1.619110</td>\n",
       "      <td>-1.788465</td>\n",
       "      <td>0.282797</td>\n",
       "      <td>0.678333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 239 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "geneNamePrimary    ACTBL2     ACTG1     ACTG2     ACTN1     ACTN4    ADAM10  \\\n",
       "110             -0.423379 -0.024570  0.026106 -1.836067 -1.606185 -1.860046   \n",
       "146              0.628846  0.946476  0.874163 -1.836067 -1.606185 -1.860046   \n",
       "126             -0.571826 -0.477617 -0.717801 -1.836067 -1.606185 -1.860046   \n",
       "121             -0.873302 -0.910295 -0.877917 -1.836067 -1.606185 -1.860046   \n",
       "152             -0.530581 -0.944076 -0.699827 -1.836067 -1.606185 -1.860046   \n",
       "..                    ...       ...       ...       ...       ...       ...   \n",
       "86              -1.338389 -0.779894 -1.079842 -1.836067 -1.606185 -1.860046   \n",
       "102             -1.749931 -0.526616 -0.639962 -1.836067 -1.606185 -1.860046   \n",
       "175             -0.667611 -0.169866 -0.128379 -1.836067 -1.606185 -1.860046   \n",
       "96              -0.758497 -0.726377 -0.669988 -1.836067 -1.606185 -1.860046   \n",
       "156             -0.249093 -0.603873 -0.377728 -1.265311 -1.606185 -1.860046   \n",
       "\n",
       "geneNamePrimary  ADAMTS13    ADIPOQ       AFM       AGT  ...     SVEP1  \\\n",
       "110             -2.440891 -0.395340  0.244759  0.418586  ... -0.349093   \n",
       "146             -2.440891 -2.138182  0.372256  0.003020  ... -2.110371   \n",
       "126             -1.724302  0.106397 -0.695381  0.194567  ... -2.110371   \n",
       "121             -2.440891 -2.138182  0.555990  0.729403  ... -2.110371   \n",
       "152             -1.064995 -1.090575 -0.280086  0.113807  ... -2.110371   \n",
       "..                    ...       ...       ...       ...  ...       ...   \n",
       "86              -2.440891 -0.914108  0.179224  0.156814  ... -1.245765   \n",
       "102             -2.440891 -2.138182  0.164797  0.325150  ... -2.110371   \n",
       "175             -2.440891 -2.138182  0.152263 -1.038392  ... -0.569083   \n",
       "96              -2.440891 -2.138182  0.541622 -0.894811  ... -2.110371   \n",
       "156             -2.440891 -1.255815 -1.160970 -0.060001  ... -0.454097   \n",
       "\n",
       "geneNamePrimary      TFRC     THBS1      TLN2      TPM3     TUBA8      UGT8  \\\n",
       "110              1.513985 -1.410128 -1.974323 -1.859943 -1.954009  1.736182   \n",
       "146              0.082788 -1.410128 -1.141835 -1.859943 -1.954009  1.688060   \n",
       "126             -0.506163 -0.164129 -1.974323 -1.859943 -0.193621  1.885809   \n",
       "121              0.240194 -1.410128 -1.974323 -1.859943 -1.954009  1.447036   \n",
       "152             -1.407680  0.330266 -1.974323 -1.859943 -1.954009  1.471968   \n",
       "..                    ...       ...       ...       ...       ...       ...   \n",
       "86               0.039113 -0.862686 -1.974323 -1.859943 -1.954009  1.432024   \n",
       "102              0.134644  0.522581 -1.974323 -1.859943 -1.954009  0.030291   \n",
       "175              0.925464  0.394859 -1.974323 -1.859943 -1.954009  0.030291   \n",
       "96              -0.971333 -0.903362 -1.974323 -1.859943 -1.954009  1.257198   \n",
       "156             -0.305702 -1.410128 -1.974323 -1.859943 -1.954009  1.619110   \n",
       "\n",
       "geneNamePrimary       VCL       VIM       VWF  \n",
       "110             -1.788465 -1.030240  1.481474  \n",
       "146             -0.647795  0.764930  1.111831  \n",
       "126             -1.788465 -1.030240  1.010081  \n",
       "121             -1.788465 -1.030240  0.326443  \n",
       "152             -1.788465 -1.030240  1.436516  \n",
       "..                    ...       ...       ...  \n",
       "86              -1.788465 -1.030240  0.045334  \n",
       "102             -1.001484 -1.030240  0.053385  \n",
       "175             -1.788465 -1.030240  0.231407  \n",
       "96              -1.788465 -1.030240 -2.207381  \n",
       "156             -1.788465  0.282797  0.678333  \n",
       "\n",
       "[107 rows x 239 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# impute 0s, as above\n",
    "\n",
    "imputeWideDFMinOr0(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of classification algorithms\n",
    "\n",
    "classifiers = [\n",
    "    ('HistGradientBoostingClassifier', HistGradientBoostingClassifier(random_state = seed)),\n",
    "    ('CatBoostClassifier', CatBoostClassifier(random_state = seed, verbose = False)),\n",
    "    ('LGBMClassifier', LGBMClassifier(random_state = seed, verbosity = -1)),\n",
    "    ('XGBClassifier', XGBClassifier(random_state = seed)),\n",
    "    ('AdaBoostClassifier', AdaBoostClassifier(random_state = seed)),\n",
    "    ('RandomForestClassifier', RandomForestClassifier(random_state = seed)),\n",
    "    ('BaggingClassifier', BaggingClassifier(random_state = seed)),\n",
    "    ('ExtraTreesClassifier', ExtraTreesClassifier(random_state = seed)),\n",
    "    ('GradientBoostingClassifier', GradientBoostingClassifier(random_state = seed)),\n",
    "    ('SVC', SVC(random_state = seed)),\n",
    "    ('MLPClassifier', MLPClassifier(random_state = seed)),\n",
    "    ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=3)),\n",
    "    ('GaussianProcessClassifier', GaussianProcessClassifier(random_state = seed)),\n",
    "    ('DecisionTreeClassifier', DecisionTreeClassifier(random_state = seed)),\n",
    "    ('GaussianNB', GaussianNB()),\n",
    "    ('QuadraticDiscriminantAnalysis', QuadraticDiscriminantAnalysis()),\n",
    "    ('LinearSVC', LinearSVC(random_state=seed)),\n",
    "    ('RidgeClassifier', RidgeClassifier(random_state = seed)),\n",
    "    ('SGDClassifier', SGDClassifier(random_state = seed)),\n",
    "    ('XGBClassifier', XGBClassifier(random_state = seed)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# perform cross validation on every model in the list. Use of a network computer cluster may be required\n",
    "\n",
    "print('\\nCross-Validation:')\n",
    "\n",
    "for j, (name, clf) in enumerate(classifiers):\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresCrossValScore = []\n",
    "    r2_scores = []\n",
    "    pipeline.set_params(Model = clf)\n",
    "    \n",
    "    print('\\n')\n",
    "    print(f'\\n{name} Classifier:\\n')\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        \n",
    "        #print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa: {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use Optuna to individually tune each model\n",
    "* score based on Matthew Correlation Coefficient (MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optuna logger\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# CatBoostClassifier, from https://catboost.ai/docs/concepts/python-reference_catboostclassifier\n",
    "\n",
    "def objectiveCatBoostClassifier(trial):\n",
    "    \n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "        'iterations': 500,\n",
    "        'depth': trial.suggest_int('depth', 1, 10),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.05, 1.0),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.1, 10),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0.1, 10),\n",
    "        'verbose': False,\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),\n",
    "        'leaf_estimation_method': trial.suggest_categorical('leaf_estimation_method', ['Newton', 'Gradient']),\n",
    "        'eval_metric': 'MCC'\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = CatBoostClassifier(**params, random_state = seed))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyCatBoostClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyCatBoostClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyCatBoostClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyCatBoostClassifier, load_if_exists=True)\n",
    "studyCatBoostClassifier.optimize(objectiveCatBoostClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsCatBoostClassifier = studyCatBoostClassifier.best_params\n",
    "best_rmse_scoreCatBoostClassifier = studyCatBoostClassifier.best_value\n",
    "\n",
    "print(f'\\nCatBoostClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyCatBoostClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreCatBoostClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsCatBoostClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyCatBoostClassifier, 'CatBoostClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyCatBoostClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Histogram-based Gradient Boosting\n",
    "\n",
    "def objectiveHistGradientBoostingClassifier(trial):\n",
    "    params = {\n",
    "        'loss': trial.suggest_categorical('loss', ['log_loss']),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001,0.1),\n",
    "        'max_iter': trial.suggest_categorical('max_iter', [1000]),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10,200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 30),\n",
    "        'max_bins': trial.suggest_int('max_bins', 100, 255),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5,100),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 1e-10,10.0),\n",
    "        'class_weight': trial.suggest_categorical('class_weight', ['balanced', None])\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = HistGradientBoostingClassifier(**params, random_state = seed))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC \n",
    "         \n",
    "studyName = 'studyHistGradientBoostingClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyHistGradientBoostingClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyHistGradientBoostingClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyHistGradientBoostingClassifier, load_if_exists=True)\n",
    "studyHistGradientBoostingClassifier.optimize(objectiveHistGradientBoostingClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsHistGradientBoostingClassifier = studyHistGradientBoostingClassifier.best_params\n",
    "best_rmse_scoreHistGradientBoostingClassifier = studyHistGradientBoostingClassifier.best_value\n",
    "\n",
    "print(f'\\nHistGradientBoostingClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyHistGradientBoostingClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreHistGradientBoostingClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsHistGradientBoostingClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyHistGradientBoostingClassifier, 'HistGradientBoostingClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyHistGradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# AdaBoostClassifier optimization\n",
    "\n",
    "def objectiveAdaBoostClassifier(trial):\n",
    "    \n",
    "    params = { # this is from sklearn\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1, 200),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.0001, 0.5),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = AdaBoostClassifier(**params, random_state = seed))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "\n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC  \n",
    "\n",
    "studyName = 'studyAdaBoostClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyAdaBoostClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyAdaBoostClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyAdaBoostClassifier, load_if_exists=True)\n",
    "studyAdaBoostClassifier.optimize(objectiveAdaBoostClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsAdaBoostClassifier = studyAdaBoostClassifier.best_params\n",
    "best_rmse_scoreAdaBoostClassifier = studyAdaBoostClassifier.best_value\n",
    "\n",
    "print(f'\\nAdaBoostClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyAdaBoostClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreAdaBoostClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsAdaBoostClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyAdaBoostClassifier, 'AdaBoostClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyAdaBoostClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LGBMClassifier\n",
    "\n",
    "def objectiveLGBMClassifier(trial):\n",
    "    \n",
    "    params = {\n",
    "        'metric': 'rmse',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 20000),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt','dart']),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 10),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 1, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.0001, 0.5),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 100),\n",
    "        #\"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0, 15),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n",
    "        'min_data_per_groups' : trial.suggest_int('min_data_per_groups', 1, 100),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = LGBMClassifier(**params, random_state = seed, n_jobs = nJobs, verbosity = -1))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyLGBMClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyLGBMClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyLGBMClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyLGBMClassifier, load_if_exists=True)\n",
    "studyLGBMClassifier.optimize(objectiveLGBMClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsLGBMClassifier = studyLGBMClassifier.best_params\n",
    "best_rmse_scoreLGBMClassifier = studyLGBMClassifier.best_value\n",
    "\n",
    "print(f'\\nLGBMClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyLGBMClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreLGBMClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsLGBMClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyLGBMClassifier, 'LGBMClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyLGBMClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "\n",
    "def objectiveRandomForestClassifier(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 2, 500),\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 110),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 30, 10000),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = RandomForestClassifier(**params, random_state = seed, n_jobs = nJobs))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "         \n",
    "studyName = 'studyRandomForestClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyRandomForestClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyRandomForestClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyRandomForestClassifier, load_if_exists=True)\n",
    "studyRandomForestClassifier.optimize(objectiveRandomForestClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsRandomForestClassifier = studyRandomForestClassifier.best_params\n",
    "best_rmse_scoreRandomForestClassifier = studyRandomForestClassifier.best_value\n",
    "\n",
    "print(f'\\nRandomForestClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyRandomForestClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreRandomForestClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsRandomForestClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyRandomForestClassifier, 'RandomForestClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyRandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# BaggingClassifier\n",
    "\n",
    "def objectiveBaggingClassifier(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 2, 200),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = BaggingClassifier(**params, random_state = seed, n_jobs = nJobs))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyBaggingClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyBaggingClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyBaggingClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyBaggingClassifier, load_if_exists=True)\n",
    "studyBaggingClassifier.optimize(objectiveBaggingClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsBaggingClassifier = studyBaggingClassifier.best_params\n",
    "best_rmse_scoreBaggingClassifier = studyBaggingClassifier.best_value\n",
    "\n",
    "print(f'\\nBaggingClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyBaggingClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreBaggingClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsBaggingClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyBaggingClassifier, 'BaggingClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyBaggingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# ExtraTreesClassifier\n",
    "\n",
    "def objectiveExtraTreesClassifier(trial):\n",
    "    params = { # these come from SKLearn\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 2, 400),\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 9),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10, 10000),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "        'min_impurity_decrease': trial.suggest_float('min_impurity_decrease',0.0, 0.01),\n",
    "        'min_weight_fraction_leaf': trial.suggest_uniform('min_weight_fraction_leaf', 0.00001,0.4),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = ExtraTreesClassifier(**params, random_state = seed, n_jobs = nJobs))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyExtraTreesClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyExtraTreesClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyExtraTreesClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyExtraTreesClassifier, load_if_exists=True)\n",
    "studyExtraTreesClassifier.optimize(objectiveExtraTreesClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsExtraTreesClassifier = studyExtraTreesClassifier.best_params\n",
    "best_rmse_scoreExtraTreesClassifier = studyExtraTreesClassifier.best_value\n",
    "\n",
    "print(f'\\nExtraTreesClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyExtraTreesClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreExtraTreesClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsExtraTreesClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyExtraTreesClassifier, 'ExtraTreesClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyExtraTreesClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier\n",
    "\n",
    "def objectiveGradientBoostingClassifier(trial):\n",
    "    params = {\n",
    "        'loss': trial.suggest_categorical('loss', ['log_loss', 'exponential']),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.0001, 0.5),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 20, 1000),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0), # fussy\n",
    "        'criterion': trial.suggest_categorical('criterion', ['friedman_mse', 'squared_error']),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10, 10000),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = GradientBoostingClassifier(**params, random_state = seed))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyGradientBoostingClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyGradientBoostingClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyGradientBoostingClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyGradientBoostingClassifier, load_if_exists=True)\n",
    "studyGradientBoostingClassifier.optimize(objectiveGradientBoostingClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsGradientBoostingClassifier = studyGradientBoostingClassifier.best_params\n",
    "best_rmse_scoreGradientBoostingClassifier = studyGradientBoostingClassifier.best_value\n",
    "\n",
    "print(f'\\nGradientBoostingClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyGradientBoostingClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreGradientBoostingClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsGradientBoostingClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyGradientBoostingClassifier, 'GradientBoostingClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyGradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# SVC\n",
    "\n",
    "def objectiveSVC(trial):\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 1e-10, 1e10, log=True),\n",
    "        'kernel': trial.suggest_categorical('kernel', ['rbf', 'linear', 'poly', 'sigmoid']),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),\n",
    "        'shrinking': trial.suggest_categorical('shrinking', [True, False]),\n",
    "        'probability': trial.suggest_categorical('probability', [True, False]),\n",
    "        'decision_function_shape': trial.suggest_categorical('decision_function_shape', ['ovo', 'ovr']),\n",
    "        'probability': True\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = SVC(**params, random_state = seed))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = [] # new!\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studySVC_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudySVC = 'sqlite:///' + studyName +'.db'\n",
    "studySVC = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudySVC, load_if_exists=True)\n",
    "studySVC.optimize(objectiveSVC, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsSVC = studySVC.best_params\n",
    "best_rmse_scoreSVC = studySVC.best_value\n",
    "\n",
    "print(f'\\nSVC:\\n')\n",
    "print('Number of finished trials:', len(studySVC.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreSVC} \\n')\n",
    "print(f'\\n Best Params = {best_paramsSVC} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studySVC, 'SVCTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studySVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# MLPClassifier\n",
    "\n",
    "def objectiveMLPClassifier(trial):\n",
    "    params = { # these come from SKLearn\n",
    "        'activation': trial.suggest_categorical('activation', ['identity', 'logistic', 'tanh', 'relu']),\n",
    "        'solver': trial.suggest_categorical('solver', ['lbfgs', 'sgd', 'adam']),\n",
    "        'alpha': trial.suggest_float('alpha', 0.00001, 0.01),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', ['constant', 'invscaling', 'adaptive']),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = MLPClassifier(**params, random_state = seed))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyMLPClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyMLPClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyMLPClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyMLPClassifier, load_if_exists=True)\n",
    "studyMLPClassifier.optimize(objectiveMLPClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsMLPClassifier = studyMLPClassifier.best_params\n",
    "best_rmse_scoreMLPClassifier = studyMLPClassifier.best_value\n",
    "\n",
    "print(f'\\nMLPClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyMLPClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreMLPClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsMLPClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyMLPClassifier, 'MLPClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyMLPClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# KNeighborsClassifier\n",
    "\n",
    "def objectiveKNeighborsClassifier(trial):\n",
    "    params = {\n",
    "        'n_neighbors': trial.suggest_int('n_neighbors', 1,30),\n",
    "        'weights': trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        'metric': trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        'algorithm': trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),\n",
    "        'leaf_size': trial.suggest_int('leaf_size', 5,100),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = KNeighborsClassifier(**params, n_jobs = nJobs))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyKNeighborsClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyKNeighborsClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyKNeighborsClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyKNeighborsClassifier, load_if_exists=True)\n",
    "studyKNeighborsClassifier.optimize(objectiveKNeighborsClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsKNeighborsClassifier = studyKNeighborsClassifier.best_params\n",
    "best_rmse_scoreKNeighborsClassifier = studyKNeighborsClassifier.best_value\n",
    "\n",
    "print(f'\\nKNeighborsClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyKNeighborsClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreKNeighborsClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsKNeighborsClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyKNeighborsClassifier, 'KNeighborsClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyKNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gaussianProcessClassifier has no parameters to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier\n",
    "\n",
    "def objectiveDecisionTreeClassifier(trial):\n",
    "    params = {\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "        'splitter': trial.suggest_categorical('splitter', ['best', 'random']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 6),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 1000),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 2, 10000),\n",
    "        'min_weight_fraction_leaf': trial.suggest_uniform('min_weight_fraction_leaf', 0.0, 0.5), \n",
    "        'ccp_alpha': trial.suggest_float('ccp_alpha', 0.00000001, 1.0, log=True),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = DecisionTreeClassifier(**params, random_state = seed))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = [] # new!\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyDecisionTreeClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyDecisionTreeClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyDecisionTreeClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyDecisionTreeClassifier, load_if_exists=True)\n",
    "studyDecisionTreeClassifier.optimize(objectiveDecisionTreeClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsDecisionTreeClassifier = studyDecisionTreeClassifier.best_params\n",
    "best_rmse_scoreDecisionTreeClassifier = studyDecisionTreeClassifier.best_value\n",
    "\n",
    "print(f'\\nDecisionTreeClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyDecisionTreeClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreDecisionTreeClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsDecisionTreeClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyDecisionTreeClassifier, 'DecisionTreeClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyDecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# GaussianNB\n",
    "\n",
    "def objectiveGaussianNB(trial):\n",
    "    params = {\n",
    "        'var_smoothing': trial.suggest_float('var_smoothing', 1e-13, 1, log = True),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = GaussianNB(**params))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyGaussianNB_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyGaussianNB = 'sqlite:///' + studyName +'.db'\n",
    "studyGaussianNB = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyGaussianNB, load_if_exists=True)\n",
    "studyGaussianNB.optimize(objectiveGaussianNB, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsGaussianNB = studyGaussianNB.best_params\n",
    "best_rmse_scoreGaussianNB = studyGaussianNB.best_value\n",
    "\n",
    "print(f'\\nGaussianNB:\\n')\n",
    "print('Number of finished trials:', len(studyGaussianNB.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreGaussianNB} \\n')\n",
    "print(f'\\n Best Params = {best_paramsGaussianNB} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyGaussianNB, 'GaussianNBTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyGaussianNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# QuadraticDiscriminantAnalysis\n",
    "\n",
    "def objectiveQuadraticDiscriminantAnalysis(trial):\n",
    "    params = {\n",
    "        'reg_param': trial.suggest_float('reg_param', 0, 1),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = QuadraticDiscriminantAnalysis(**params))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = [] # new!\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyQuadraticDiscriminantAnalysis_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyQuadraticDiscriminantAnalysis = 'sqlite:///' + studyName +'.db'\n",
    "studyQuadraticDiscriminantAnalysis = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyQuadraticDiscriminantAnalysis, load_if_exists=True)\n",
    "studyQuadraticDiscriminantAnalysis.optimize(objectiveQuadraticDiscriminantAnalysis, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsQuadraticDiscriminantAnalysis = studyQuadraticDiscriminantAnalysis.best_params\n",
    "best_rmse_scoreQuadraticDiscriminantAnalysis = studyQuadraticDiscriminantAnalysis.best_value\n",
    "\n",
    "print(f'\\nQuadraticDiscriminantAnalysis:\\n')\n",
    "print('Number of finished trials:', len(studyQuadraticDiscriminantAnalysis.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreQuadraticDiscriminantAnalysis} \\n')\n",
    "print(f'\\n Best Params = {best_paramsQuadraticDiscriminantAnalysis} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyQuadraticDiscriminantAnalysis, 'QuadraticDiscriminantAnalysisTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyQuadraticDiscriminantAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# LinearSVC\n",
    "\n",
    "def objectiveLinearSVC(trial):\n",
    "    params = {\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
    "        'loss': trial.suggest_categorical('loss', ['hinge', 'squared_hinge']),\n",
    "        'dual': trial.suggest_categorical('dual', ['auto', True]),\n",
    "        'C': trial.suggest_float('C', 1e-10, 1e10, log=True),\n",
    "        'intercept_scaling': trial.suggest_float('intercept_scaling', 0.1, 2),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = CalibratedClassifierCV(LinearSVC(**params, random_state = seed)))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = [] # new!\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyLinearSVC_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyLinearSVC = 'sqlite:///' + studyName +'.db'\n",
    "studyLinearSVC = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyLinearSVC, load_if_exists=True)\n",
    "studyLinearSVC.optimize(objectiveLinearSVC, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning, ValueError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsLinearSVC = studyLinearSVC.best_params\n",
    "best_rmse_scoreLinearSVC = studyLinearSVC.best_value\n",
    "\n",
    "print(f'\\nLinearSVC:\\n')\n",
    "print('Number of finished trials:', len(studyLinearSVC.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreLinearSVC} \\n')\n",
    "print(f'\\n Best Params = {best_paramsLinearSVC} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyLinearSVC, 'LinearSVCTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyLinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# RidgeClassifier\n",
    "\n",
    "def objectiveRidgeClassifier(trial):\n",
    "    params = {\n",
    "        'alpha': trial.suggest_float('alpha', 0, 10),\n",
    "        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "        'solver': trial.suggest_categorical('solver', ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = RidgeClassifier(**params, random_state = seed))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studyRidgeClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyRidgeClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyRidgeClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyRidgeClassifier, load_if_exists=True)\n",
    "studyRidgeClassifier.optimize(objectiveRidgeClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsRidgeClassifier = studyRidgeClassifier.best_params\n",
    "best_rmse_scoreRidgeClassifier = studyRidgeClassifier.best_value\n",
    "\n",
    "print(f'\\nRidgeClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyRidgeClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreRidgeClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsRidgeClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyRidgeClassifier, 'RidgeClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyRidgeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# SGDClassifier\n",
    "\n",
    "def objectiveSGDClassifier(trial):\n",
    "    params = {\n",
    "        'loss': trial.suggest_categorical('loss', ['log_loss', 'modified_huber']),\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l2', 'l1', 'elasticnet']),\n",
    "        'alpha': trial.suggest_float('alpha', 0.00001, 0.001, log=True),\n",
    "        'l1_ratio': trial.suggest_float('l1_ratio', 0.05, 0.95),\n",
    "        'power_t': trial.suggest_float('power_t', -2, 2),\n",
    "        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = SGDClassifier(**params, random_state = seed, n_jobs = nJobs))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = [] # new!\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC\n",
    "\n",
    "studyName = 'studySGDClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudySGDClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studySGDClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudySGDClassifier, load_if_exists=True)\n",
    "studySGDClassifier.optimize(objectiveSGDClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsSGDClassifier = studySGDClassifier.best_params\n",
    "best_rmse_scoreSGDClassifier = studySGDClassifier.best_value\n",
    "\n",
    "print(f'\\nSGDClassifier:\\n')\n",
    "print('Number of finished trials:', len(studySGDClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreSGDClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsSGDClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studySGDClassifier, 'SGDClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studySGDClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# XGBClassifier\n",
    "\n",
    "def objectiveXGBClassifier(trial):\n",
    "    params = {\n",
    "        'booster': trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        'n_estimators': trial.suggest_int(\"n_estimators\", 1, 150),\n",
    "        'learning_rate': trial.suggest_uniform('learning_rate', 0.0000001, 1),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.0001, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if params[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        params[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        params[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if params[\"booster\"] == \"dart\":\n",
    "        params[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        params[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        params[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        params[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "        \n",
    "    pipeline.set_params(Model = XGBClassifier(**params, random_state = seed, nthread = nJobs))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = [] # new!\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreMCC \n",
    "\n",
    "studyName = 'studyXGBClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "storagestudyXGBClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyXGBClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyXGBClassifier, load_if_exists=True)\n",
    "studyXGBClassifier.optimize(objectiveXGBClassifier, n_trials = 100, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "best_paramsXGBClassifier = studyXGBClassifier.best_params\n",
    "best_rmse_scoreXGBClassifier = studyXGBClassifier.best_value\n",
    "\n",
    "print(f'\\nXGBClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyXGBClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreXGBClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsXGBClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyXGBClassifier, 'XGBClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(studyXGBClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate each tuned classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optuna tuned classifiers AS PIPELINES\n",
    "tunedClassifierList = []\n",
    "\n",
    "CatBoostClassifierTuned = joblib.load('CatBoostClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "HistGradientBoostingClassifierTuned = joblib.load('HistGradientBoostingClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "LGBMClassifierTuned = joblib.load('LGBMClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "AdaBoostClassifierTuned = joblib.load('AdaBoostClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "RandomForestClassifierTuned = joblib.load('RandomForestClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "BaggingClassifierTuned = joblib.load('BaggingClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')#ExtraTreesClassifierTuned = joblib.load('ExtraTreesClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "ExtraTreesClassifierTuned = joblib.load('ExtraTreesClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')#ExtraTreesClassifierTuned = joblib.load('ExtraTreesClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "GradientBoostingClassifierTuned = joblib.load('GradientBoostingClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "SVCTuned = joblib.load('SVCTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "MLPClassifierTuned = joblib.load('MLPClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "KNeighborsClassifierTuned = joblib.load('KNeighborsClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "GaussianProcessClassifierTuned = GaussianProcessClassifier(random_state = seed) #has nothing to tune\n",
    "DecisionTreeClassifierTuned = joblib.load('DecisionTreeClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "GaussianNBTuned = joblib.load('GaussianNBTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "QuadraticDiscriminantAnalysisTuned = joblib.load('QuadraticDiscriminantAnalysisTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "LinearSVCTuned = joblib.load('LinearSVCTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "RidgeClassifierTuned = joblib.load('RidgeClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "SGDClassifierTuned = joblib.load('SGDClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')\n",
    "XGBClassifierTuned = joblib.load('XGBClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists to summarize results\n",
    "\n",
    "tunedClassifierList = []\n",
    "seedList = []\n",
    "trainFracList = []\n",
    "mean_scoreKappaTunedClassifierList = []\n",
    "mean_scoreMCCTunedClassifierList= []\n",
    "mean_scoreF1WeightedTunedClassifierList = []\n",
    "mean_scoreAccuracyTunedClassifierList = []\n",
    "mean_scoreROCTunedClassifierList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# catboost with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned CatBoostClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = CatBoostClassifier(**CatBoostClassifierTuned.best_params,random_state = seed, verbose = False))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "    feature_importance.append(pipeline[-1].feature_importances_)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('CatBoostClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# HistGradientBoostingClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned HistGradientBoostingClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = HistGradientBoostingClassifier(**HistGradientBoostingClassifierTuned.best_params,random_state = seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('HistGradientBoostingClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# LGBMClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned LGBMClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = LGBMClassifier(**LGBMClassifierTuned.best_params,random_state = seed, n_jobs=nJobs, verbosity = -1))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "    feature_importance.append(pipeline[-1].feature_importances_)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('LGBMClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# AdaBoostClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned AdaBoostClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = AdaBoostClassifier(**AdaBoostClassifierTuned.best_params,random_state = seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "    feature_importance.append(pipeline[-1].feature_importances_)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('AdaBoostClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# RandomForestClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned RandomForestClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = RandomForestClassifier(**RandomForestClassifierTuned.best_params,random_state = seed, n_jobs=nJobs))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "    feature_importance.append(pipeline[-1].feature_importances_)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('RandomForestClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# BaggingClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned BaggingClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = BaggingClassifier(**BaggingClassifierTuned.best_params,random_state = seed, n_jobs=nJobs))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('BaggingClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# ExtraTreesClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned ExtraTreesClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = ExtraTreesClassifier(**ExtraTreesClassifierTuned.best_params,random_state = seed, n_jobs=nJobs))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "    feature_importance.append(pipeline[-1].feature_importances_)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('ExtraTreesClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned GradientBoostingClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = GradientBoostingClassifier(**GradientBoostingClassifierTuned.best_params,random_state = seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "    feature_importance.append(pipeline[-1].feature_importances_)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('GradientBoostingClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# SVC with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned SVC Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = SVC(**SVCTuned.best_params,random_state = seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('SVC')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# MLPClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned MLPClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = MLPClassifier(**MLPClassifierTuned.best_params,random_state = seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('MLPClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# KNeighborsClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned KNeighborsClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = KNeighborsClassifier(**KNeighborsClassifierTuned.best_params, n_jobs=nJobs))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('KNeighborsClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# GaussianProcessClassifier, which has no parameters to optimize\n",
    "\n",
    "print('\\nTuned gaussianProcessClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = GaussianProcessClassifier(random_state = seed, n_jobs=nJobs))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('GaussianProcessClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned DecisionTreeClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = DecisionTreeClassifier(**DecisionTreeClassifierTuned.best_params,random_state = seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('DecisionTreeClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# GaussianNB with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned GaussianNB Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = GaussianNB(**GaussianNBTuned.best_params))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('GaussianNB')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# QuadraticDiscriminantAnalysis with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned QuadraticDiscriminantAnalysis Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = QuadraticDiscriminantAnalysis(**QuadraticDiscriminantAnalysisTuned.best_params))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('QuadraticDiscriminantAnalysis')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# LinearSVC with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned LinearSVC Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = LinearSVC(**LinearSVCTuned.best_params,random_state = seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('LinearSVC')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# RidgeClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned RidgeClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = RidgeClassifier(**RidgeClassifierTuned.best_params,random_state = seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('RidgeClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# SGDClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned SGDClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = SGDClassifier(**SGDClassifierTuned.best_params,random_state = seed, n_jobs=nJobs))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('SGDClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# XGBClassifier with optuna-tuned parameters\n",
    "\n",
    "print('\\nTuned XGBClassifier Cross-Validation:')\n",
    "scores = []\n",
    "feature_importance = []\n",
    "print('\\n')\n",
    "scores = []\n",
    "scoresMCC = []\n",
    "scoresF1Weighted = []\n",
    "scoresAccuracy = []\n",
    "scoresROC = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    pipeline.set_params(Model = XGBClassifier(**XGBClassifierTuned.best_params,random_state = seed, n_jobs=nJobs))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "    kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    predictions = pipeline.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "    scores.append(kappa)\n",
    "    scoresMCC.append(mcc)\n",
    "    scoresF1Weighted.append(f1Weighted)\n",
    "    scoresAccuracy.append(accuracy)\n",
    "    scoresROC.append(auc)\n",
    "        \n",
    " #   print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        fold_stdROC = np.std(scoresROC)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa = = {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient = = {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score = = {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized) = = {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  AUROC = = {mean_scoreROC:.4f} \\u00B1 {fold_stdROC:.4f}')\n",
    "\n",
    "        tunedClassifierList.append('XGBClassifier')\n",
    "        seedList.append(seed)\n",
    "        trainFracList.append(trainFrac)\n",
    "        mean_scoreKappaTunedClassifierList.append(mean_score)\n",
    "        mean_scoreMCCTunedClassifierList.append(mean_scoreMCC)\n",
    "        mean_scoreF1WeightedTunedClassifierList.append(mean_scoreF1Weighted)\n",
    "        mean_scoreAccuracyTunedClassifierList.append(mean_scoreAccuracy)\n",
    "        mean_scoreROCTunedClassifierList.append(mean_scoreROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the individually tuned classifiers to the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# load from saved\n",
    "dfValidationFromDisk = pd.read_excel('_dfValidation_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.xlsx')\n",
    "df = dfValidationFromDisk\n",
    "\n",
    "y_validationList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only selected features\n",
    "\n",
    "fileName = '_selectedFeaturesRFECV_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "\n",
    "with open(fileName + '.json', 'r') as f:\n",
    "    selected_featuresFromDisk = json.load(f)\n",
    "\n",
    "colsToKeep = []\n",
    "colsToKeep = copy.deepcopy(selected_featuresFromDisk)\n",
    "colsToKeep.append(target)\n",
    "\n",
    "df = df[colsToKeep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split into X and Y\n",
    "\n",
    "X_validation, y_validation = X_y_split(df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "11    0\n",
       "12    0\n",
       "13    0\n",
       "14    0\n",
       "15    0\n",
       "16    0\n",
       "17    0\n",
       "18    1\n",
       "19    1\n",
       "20    1\n",
       "21    1\n",
       "22    1\n",
       "23    1\n",
       "24    1\n",
       "25    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_validation = pd.Series(le.fit_transform(y_validation))\n",
    "y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACTBL2</th>\n",
       "      <th>ACTG1</th>\n",
       "      <th>ACTG2</th>\n",
       "      <th>ACTN1</th>\n",
       "      <th>ACTN4</th>\n",
       "      <th>ADAM10</th>\n",
       "      <th>ADAMTS13</th>\n",
       "      <th>ADIPOQ</th>\n",
       "      <th>AFM</th>\n",
       "      <th>AGT</th>\n",
       "      <th>...</th>\n",
       "      <th>SVEP1</th>\n",
       "      <th>TFRC</th>\n",
       "      <th>THBS1</th>\n",
       "      <th>TLN2</th>\n",
       "      <th>TPM3</th>\n",
       "      <th>TUBA8</th>\n",
       "      <th>UGT8</th>\n",
       "      <th>VCL</th>\n",
       "      <th>VIM</th>\n",
       "      <th>VWF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>0.469387</td>\n",
       "      <td>0.266539</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>-1.197036</td>\n",
       "      <td>-0.468998</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.879373</td>\n",
       "      <td>1.106476</td>\n",
       "      <td>-1.722096</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.865699</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.653913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.338129</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-0.484136</td>\n",
       "      <td>-0.574835</td>\n",
       "      <td>-0.747148</td>\n",
       "      <td>-0.894209</td>\n",
       "      <td>0.482403</td>\n",
       "      <td>0.766467</td>\n",
       "      <td>0.521196</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.062470</td>\n",
       "      <td>-1.605370</td>\n",
       "      <td>-0.133370</td>\n",
       "      <td>0.126968</td>\n",
       "      <td>-0.513470</td>\n",
       "      <td>0.113387</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-0.412096</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>0.480118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.190932</td>\n",
       "      <td>-0.665262</td>\n",
       "      <td>-0.489321</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>0.003612</td>\n",
       "      <td>-0.513159</td>\n",
       "      <td>-0.094621</td>\n",
       "      <td>-0.111761</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>0.368662</td>\n",
       "      <td>-0.559181</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.756829</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.757194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-1.046428</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.301385</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>0.202773</td>\n",
       "      <td>0.974240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.989369</td>\n",
       "      <td>0.255144</td>\n",
       "      <td>-0.512598</td>\n",
       "      <td>0.210279</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.319357</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>0.235380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.869840</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-0.161651</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>0.323115</td>\n",
       "      <td>0.388052</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-0.400457</td>\n",
       "      <td>-0.248735</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.778492</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>0.895316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-0.663550</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.373625</td>\n",
       "      <td>-0.442460</td>\n",
       "      <td>-0.502456</td>\n",
       "      <td>0.263057</td>\n",
       "      <td>0.057436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.328864</td>\n",
       "      <td>0.765137</td>\n",
       "      <td>-0.313762</td>\n",
       "      <td>-0.291602</td>\n",
       "      <td>-0.627326</td>\n",
       "      <td>-0.632982</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-0.988735</td>\n",
       "      <td>-0.173581</td>\n",
       "      <td>-0.071144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.495373</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>0.642554</td>\n",
       "      <td>0.355340</td>\n",
       "      <td>-0.482796</td>\n",
       "      <td>-1.838184</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>-0.947442</td>\n",
       "      <td>-0.330764</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-0.647838</td>\n",
       "      <td>0.767015</td>\n",
       "      <td>-0.540684</td>\n",
       "      <td>0.861175</td>\n",
       "      <td>-0.004139</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>1.246629</td>\n",
       "      <td>-0.089003</td>\n",
       "      <td>1.635177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-0.572469</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.092635</td>\n",
       "      <td>-0.300754</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>0.322989</td>\n",
       "      <td>-0.222300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.891231</td>\n",
       "      <td>-0.524184</td>\n",
       "      <td>0.110614</td>\n",
       "      <td>0.074216</td>\n",
       "      <td>-0.122331</td>\n",
       "      <td>0.658714</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-0.592531</td>\n",
       "      <td>-0.510214</td>\n",
       "      <td>0.025982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-0.369941</td>\n",
       "      <td>-0.837724</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-1.014452</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>0.078044</td>\n",
       "      <td>-0.403351</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.341272</td>\n",
       "      <td>0.334208</td>\n",
       "      <td>-0.421917</td>\n",
       "      <td>-1.347796</td>\n",
       "      <td>-0.304606</td>\n",
       "      <td>0.083342</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-0.803889</td>\n",
       "      <td>0.693706</td>\n",
       "      <td>0.200491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.395138</td>\n",
       "      <td>0.466486</td>\n",
       "      <td>0.405952</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-0.697550</td>\n",
       "      <td>0.382139</td>\n",
       "      <td>0.539671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.440379</td>\n",
       "      <td>0.229268</td>\n",
       "      <td>-0.447268</td>\n",
       "      <td>-0.989630</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.524093</td>\n",
       "      <td>-0.728794</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>0.404425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.112768</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-0.088833</td>\n",
       "      <td>-0.350719</td>\n",
       "      <td>-0.331541</td>\n",
       "      <td>-0.341078</td>\n",
       "      <td>0.319447</td>\n",
       "      <td>0.264053</td>\n",
       "      <td>0.197473</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.176952</td>\n",
       "      <td>-0.082222</td>\n",
       "      <td>0.083108</td>\n",
       "      <td>-0.419636</td>\n",
       "      <td>0.138718</td>\n",
       "      <td>-0.049477</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-0.400096</td>\n",
       "      <td>0.952932</td>\n",
       "      <td>1.499098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.680107</td>\n",
       "      <td>1.125549</td>\n",
       "      <td>0.908032</td>\n",
       "      <td>-0.038281</td>\n",
       "      <td>-0.307818</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-0.810231</td>\n",
       "      <td>0.309620</td>\n",
       "      <td>0.569613</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>0.272116</td>\n",
       "      <td>1.247909</td>\n",
       "      <td>-0.072298</td>\n",
       "      <td>0.014510</td>\n",
       "      <td>0.092349</td>\n",
       "      <td>1.896657</td>\n",
       "      <td>0.027683</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>0.984013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.544613</td>\n",
       "      <td>1.790861</td>\n",
       "      <td>1.715717</td>\n",
       "      <td>0.016835</td>\n",
       "      <td>-0.239013</td>\n",
       "      <td>-0.251111</td>\n",
       "      <td>-0.605179</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>-0.107148</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-0.693613</td>\n",
       "      <td>-0.446925</td>\n",
       "      <td>0.624410</td>\n",
       "      <td>0.585962</td>\n",
       "      <td>0.772154</td>\n",
       "      <td>1.920066</td>\n",
       "      <td>0.999640</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.726569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.536970</td>\n",
       "      <td>0.579530</td>\n",
       "      <td>0.112615</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>-1.197036</td>\n",
       "      <td>-0.468998</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>1.114064</td>\n",
       "      <td>-1.722096</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.845991</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.557656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.156911</td>\n",
       "      <td>1.218710</td>\n",
       "      <td>1.249301</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>-0.728102</td>\n",
       "      <td>-0.468998</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-1.605370</td>\n",
       "      <td>0.415271</td>\n",
       "      <td>-0.845556</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>0.090339</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>0.143346</td>\n",
       "      <td>-0.246846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>-0.266565</td>\n",
       "      <td>-0.744086</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-1.908150</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>0.440445</td>\n",
       "      <td>-0.119638</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-1.207800</td>\n",
       "      <td>-1.722096</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>2.034586</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>0.524012</td>\n",
       "      <td>1.200827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.029239</td>\n",
       "      <td>0.256713</td>\n",
       "      <td>0.125511</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>0.600760</td>\n",
       "      <td>0.382400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.608312</td>\n",
       "      <td>0.292881</td>\n",
       "      <td>-1.722096</td>\n",
       "      <td>-1.185725</td>\n",
       "      <td>-1.721966</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.443986</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.280491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.154410</td>\n",
       "      <td>0.187813</td>\n",
       "      <td>0.238051</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-1.233975</td>\n",
       "      <td>-0.613144</td>\n",
       "      <td>-0.136810</td>\n",
       "      <td>-0.132437</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-0.669456</td>\n",
       "      <td>-1.722096</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.873749</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.344994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>-0.169318</td>\n",
       "      <td>-0.468998</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-1.605370</td>\n",
       "      <td>-0.290301</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.271318</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>-0.381213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.207809</td>\n",
       "      <td>0.724345</td>\n",
       "      <td>0.406078</td>\n",
       "      <td>-0.092329</td>\n",
       "      <td>-0.244823</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>-1.197036</td>\n",
       "      <td>0.164921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.858406</td>\n",
       "      <td>0.052865</td>\n",
       "      <td>0.051868</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>0.362752</td>\n",
       "      <td>1.465867</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.125724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.963421</td>\n",
       "      <td>1.290558</td>\n",
       "      <td>1.135439</td>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.354507</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-0.797828</td>\n",
       "      <td>-0.590578</td>\n",
       "      <td>0.225774</td>\n",
       "      <td>0.395730</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>0.235419</td>\n",
       "      <td>0.408503</td>\n",
       "      <td>-0.418911</td>\n",
       "      <td>-0.091034</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.607009</td>\n",
       "      <td>0.291246</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.666004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.550732</td>\n",
       "      <td>-0.093227</td>\n",
       "      <td>-0.317156</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.121344</td>\n",
       "      <td>-0.280758</td>\n",
       "      <td>-0.418702</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-0.137573</td>\n",
       "      <td>0.762255</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.376162</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.184480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.106094</td>\n",
       "      <td>0.167114</td>\n",
       "      <td>0.550721</td>\n",
       "      <td>-0.508997</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>-0.211772</td>\n",
       "      <td>-0.253430</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-0.888657</td>\n",
       "      <td>0.011648</td>\n",
       "      <td>-1.463034</td>\n",
       "      <td>-0.293315</td>\n",
       "      <td>0.454643</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>-0.087427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>-0.223862</td>\n",
       "      <td>0.036894</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-2.363432</td>\n",
       "      <td>-1.482492</td>\n",
       "      <td>-0.379793</td>\n",
       "      <td>-0.247008</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-0.661333</td>\n",
       "      <td>0.226026</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.609112</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>-1.376535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.747431</td>\n",
       "      <td>1.058981</td>\n",
       "      <td>0.963931</td>\n",
       "      <td>-0.384664</td>\n",
       "      <td>-0.591007</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-0.931825</td>\n",
       "      <td>-0.333360</td>\n",
       "      <td>0.464917</td>\n",
       "      <td>0.446133</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.409640</td>\n",
       "      <td>-0.024556</td>\n",
       "      <td>0.395426</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-0.023869</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>1.490495</td>\n",
       "      <td>-0.494921</td>\n",
       "      <td>0.102410</td>\n",
       "      <td>1.550462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.505517</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.067345</td>\n",
       "      <td>-1.178625</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>-1.471872</td>\n",
       "      <td>-0.865050</td>\n",
       "      <td>-1.517300</td>\n",
       "      <td>0.074466</td>\n",
       "      <td>0.375350</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150082</td>\n",
       "      <td>-1.605370</td>\n",
       "      <td>-0.290105</td>\n",
       "      <td>-1.992482</td>\n",
       "      <td>-2.095127</td>\n",
       "      <td>-1.427330</td>\n",
       "      <td>-0.676301</td>\n",
       "      <td>-1.113636</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>0.567572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26 rows × 239 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ACTBL2     ACTG1     ACTG2     ACTN1     ACTN4    ADAM10  ADAMTS13  \\\n",
       "0  -1.505517  0.469387  0.266539 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "1  -0.338129 -1.732577 -1.067345 -0.484136 -0.574835 -0.747148 -0.894209   \n",
       "2  -0.190932 -0.665262 -0.489321 -1.178625 -2.099970 -1.471872  0.003612   \n",
       "3  -1.505517 -1.732577 -1.067345 -1.046428 -2.099970 -1.301385 -2.363432   \n",
       "4  -0.869840 -1.732577 -0.161651 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "5  -1.505517 -1.732577 -1.067345 -0.663550 -2.099970 -1.373625 -0.442460   \n",
       "6   0.495373 -1.732577 -1.067345  0.642554  0.355340 -0.482796 -1.838184   \n",
       "7  -1.505517 -1.732577 -1.067345 -0.572469 -2.099970 -1.092635 -0.300754   \n",
       "8  -1.505517 -1.732577 -1.067345 -0.369941 -0.837724 -1.471872 -1.014452   \n",
       "9   0.395138  0.466486  0.405952 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "10 -0.112768 -1.732577 -1.067345 -0.088833 -0.350719 -0.331541 -0.341078   \n",
       "11  0.680107  1.125549  0.908032 -0.038281 -0.307818 -1.471872 -2.363432   \n",
       "12  1.544613  1.790861  1.715717  0.016835 -0.239013 -0.251111 -0.605179   \n",
       "13 -0.536970  0.579530  0.112615 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "14  1.156911  1.218710  1.249301 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "15 -1.505517 -0.266565 -0.744086 -1.178625 -2.099970 -1.471872 -1.908150   \n",
       "16  0.029239  0.256713  0.125511 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "17  0.154410  0.187813  0.238051 -1.178625 -2.099970 -1.471872 -1.233975   \n",
       "18 -1.505517 -1.067345 -1.067345 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "19 -0.207809  0.724345  0.406078 -0.092329 -0.244823 -1.471872 -2.363432   \n",
       "20  0.963421  1.290558  1.135439  0.575851  0.354507 -1.471872 -0.797828   \n",
       "21 -0.550732 -0.093227 -0.317156 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "22  0.106094  0.167114  0.550721 -0.508997 -2.099970 -1.471872 -2.363432   \n",
       "23 -1.505517 -0.223862  0.036894 -1.178625 -2.099970 -1.471872 -2.363432   \n",
       "24  0.747431  1.058981  0.963931 -0.384664 -0.591007 -1.471872 -0.931825   \n",
       "25 -1.505517 -1.732577 -1.067345 -1.178625 -2.099970 -1.471872 -0.865050   \n",
       "\n",
       "      ADIPOQ       AFM       AGT  ...     SVEP1      TFRC     THBS1      TLN2  \\\n",
       "0  -1.517300 -1.197036 -0.468998  ... -0.879373  1.106476 -1.722096 -1.992482   \n",
       "1   0.482403  0.766467  0.521196  ... -1.062470 -1.605370 -0.133370  0.126968   \n",
       "2  -0.513159 -0.094621 -0.111761  ... -2.150082  0.368662 -0.559181 -1.992482   \n",
       "3  -1.517300  0.202773  0.974240  ... -0.989369  0.255144 -0.512598  0.210279   \n",
       "4  -1.517300  0.323115  0.388052  ... -2.150082 -0.400457 -0.248735 -1.992482   \n",
       "5  -0.502456  0.263057  0.057436  ... -0.328864  0.765137 -0.313762 -0.291602   \n",
       "6  -1.517300 -0.947442 -0.330764  ... -2.150082 -0.647838  0.767015 -0.540684   \n",
       "7  -1.517300  0.322989 -0.222300  ... -0.891231 -0.524184  0.110614  0.074216   \n",
       "8  -1.517300  0.078044 -0.403351  ... -0.341272  0.334208 -0.421917 -1.347796   \n",
       "9  -0.697550  0.382139  0.539671  ... -0.440379  0.229268 -0.447268 -0.989630   \n",
       "10  0.319447  0.264053  0.197473  ... -0.176952 -0.082222  0.083108 -0.419636   \n",
       "11 -0.810231  0.309620  0.569613  ... -2.150082  0.272116  1.247909 -0.072298   \n",
       "12 -1.517300  0.018933 -0.107148  ... -2.150082 -0.693613 -0.446925  0.624410   \n",
       "13 -1.517300 -1.197036 -0.468998  ... -2.150082  1.114064 -1.722096 -1.992482   \n",
       "14 -1.517300 -0.728102 -0.468998  ... -2.150082 -1.605370  0.415271 -0.845556   \n",
       "15 -1.517300  0.440445 -0.119638  ... -2.150082 -1.207800 -1.722096 -1.992482   \n",
       "16 -1.517300  0.600760  0.382400  ... -0.608312  0.292881 -1.722096 -1.185725   \n",
       "17 -0.613144 -0.136810 -0.132437  ... -2.150082 -0.669456 -1.722096 -1.992482   \n",
       "18 -1.517300 -0.169318 -0.468998  ... -2.150082 -1.605370 -0.290301 -1.992482   \n",
       "19 -1.517300 -1.197036  0.164921  ... -0.858406  0.052865  0.051868 -1.992482   \n",
       "20 -0.590578  0.225774  0.395730  ... -2.150082  0.235419  0.408503 -0.418911   \n",
       "21 -1.121344 -0.280758 -0.418702  ... -2.150082 -0.137573  0.762255 -1.992482   \n",
       "22 -1.517300 -0.211772 -0.253430  ... -2.150082 -0.888657  0.011648 -1.463034   \n",
       "23 -1.482492 -0.379793 -0.247008  ... -2.150082 -0.661333  0.226026 -1.992482   \n",
       "24 -0.333360  0.464917  0.446133  ... -1.409640 -0.024556  0.395426 -1.992482   \n",
       "25 -1.517300  0.074466  0.375350  ... -2.150082 -1.605370 -0.290105 -1.992482   \n",
       "\n",
       "        TPM3     TUBA8      UGT8       VCL       VIM       VWF  \n",
       "0  -2.095127 -1.427330  1.865699 -1.113636 -1.101460  1.653913  \n",
       "1  -0.513470  0.113387 -0.676301 -0.412096 -1.101460  0.480118  \n",
       "2  -2.095127 -1.427330  1.756829 -1.113636 -1.101460  1.757194  \n",
       "3  -2.095127 -1.319357 -0.676301 -1.113636 -1.101460  0.235380  \n",
       "4  -2.095127 -1.427330  1.778492 -1.113636 -1.101460  0.895316  \n",
       "5  -0.627326 -0.632982 -0.676301 -0.988735 -0.173581 -0.071144  \n",
       "6   0.861175 -0.004139 -0.676301  1.246629 -0.089003  1.635177  \n",
       "7  -0.122331  0.658714 -0.676301 -0.592531 -0.510214  0.025982  \n",
       "8  -0.304606  0.083342 -0.676301 -0.803889  0.693706  0.200491  \n",
       "9  -2.095127 -1.427330  1.524093 -0.728794 -1.101460  0.404425  \n",
       "10  0.138718 -0.049477 -0.676301 -0.400096  0.952932  1.499098  \n",
       "11  0.014510  0.092349  1.896657  0.027683 -1.101460  0.984013  \n",
       "12  0.585962  0.772154  1.920066  0.999640 -1.101460  1.726569  \n",
       "13 -2.095127 -1.427330  1.845991 -1.113636 -1.101460  1.557656  \n",
       "14 -2.095127  0.090339 -0.676301 -1.113636  0.143346 -0.246846  \n",
       "15 -2.095127 -1.427330  2.034586 -1.113636  0.524012  1.200827  \n",
       "16 -1.721966 -1.427330  1.443986 -1.113636 -1.101460  1.280491  \n",
       "17 -2.095127 -1.427330  1.873749 -1.113636 -1.101460  1.344994  \n",
       "18 -2.095127 -1.427330  1.271318 -1.113636 -1.101460 -0.381213  \n",
       "19 -2.095127  0.362752  1.465867 -1.113636 -1.101460  1.125724  \n",
       "20 -0.091034 -1.427330  1.607009  0.291246 -1.101460  1.666004  \n",
       "21 -2.095127 -1.427330  1.376162 -1.113636 -1.101460  1.184480  \n",
       "22 -0.293315  0.454643 -0.676301 -1.113636 -1.101460 -0.087427  \n",
       "23 -2.095127 -1.427330  1.609112 -1.113636 -1.101460 -1.376535  \n",
       "24 -0.023869 -1.427330  1.490495 -0.494921  0.102410  1.550462  \n",
       "25 -2.095127 -1.427330 -0.676301 -1.113636 -1.101460  0.567572  \n",
       "\n",
       "[26 rows x 239 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# impute 0s as above\n",
    "\n",
    "imputeWideDFMinOr0(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute any missing values to the min of the dataset\n",
    "\n",
    "genesWithNANs = X_validation.columns[X_validation.isna().any()].tolist()\n",
    "\n",
    "X_validation[genesWithNANs] = min(X_validation.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some lists to track valation performance\n",
    "\n",
    "classifierListVal=[]\n",
    "y_predListVal=[]\n",
    "y_validationListVal=[]\n",
    "y_probListVal=[]\n",
    "seedListStatsVal=[]\n",
    "trainFracListStatsVal=[]\n",
    "balancedAccuracyListVal =[]\n",
    "precisionListVal=[]\n",
    "recallListVal=[]\n",
    "ROCAUCScoreListVal=[]\n",
    "F1ScoreListVal=[]\n",
    "MCCListVal=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned CatBoostClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = CatBoostClassifier(**CatBoostClassifierTuned.best_params,random_state = seed, verbose = False))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('CatBoostClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned HistGradientBoostingClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = HistGradientBoostingClassifier(**HistGradientBoostingClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('HistGradientBoostingClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred # Visualizing predictions\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned LGBMClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = LGBMClassifier(**LGBMClassifierTuned.best_params,random_state = seed, verbosity = -1))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append(LGBMClassifier)\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred # Visualizing predictions\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned AdaBoostClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = AdaBoostClassifier(**AdaBoostClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('AdaBoostClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned RandomForestClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = RandomForestClassifier(**RandomForestClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('RandomForestClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned BaggingClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = BaggingClassifier(**BaggingClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('BaggingClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned ExtraTreesClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = ExtraTreesClassifier(**ExtraTreesClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('ExtraTreesClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned GradientBoostingClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = GradientBoostingClassifier(**GradientBoostingClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('GradientBoostingClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned SVC For Validation\n",
    "\n",
    "pipeline.set_params(Model = SVC(**SVCTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('SVC')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned MLPClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = MLPClassifier(**MLPClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('MLPClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0]\n",
      "Balanced Accuracy 0.9097222222222222\n",
      "Precision 0.875\n",
      "Recall 0.875\n",
      "ROC AUC score 0.9097222222222222\n",
      "F1 score 0.875\n",
      "MCC 0.8194444444444444\n",
      "Validation score 0.9230769230769231\n",
      "Confusion matrix\n",
      " [[17  1]\n",
      " [ 1  7]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tuned KNeighborsClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = KNeighborsClassifier(**KNeighborsClassifierTuned.best_params))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('KNeighborsClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1]\n",
      "Balanced Accuracy 0.9444444444444444\n",
      "Precision 0.8\n",
      "Recall 1.0\n",
      "ROC AUC score 0.9444444444444444\n",
      "F1 score 0.888888888888889\n",
      "MCC 0.8432740427115678\n",
      "Validation score 0.9230769230769231\n",
      "Confusion matrix\n",
      " [[16  2]\n",
      " [ 0  8]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GaussianProcessClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = GaussianProcessClassifier(random_state = seed, n_jobs=nJobs))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('GaussianProcessClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned DecisionTreeClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = DecisionTreeClassifier(**DecisionTreeClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('DecisionTreeClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned GaussianNB For Validation\n",
    "\n",
    "pipeline.set_params(Model = GaussianNB(**GaussianNBTuned.best_params))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('GaussianNB')\n",
    "y_pred = pipeline.predict(X_validation) # .predict(X)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned QuadraticDiscriminantAnalysis For Validation\n",
    "\n",
    "pipeline.set_params(Model = QuadraticDiscriminantAnalysis(**QuadraticDiscriminantAnalysisTuned.best_params))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('QuadraticDiscriminantAnalysis')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned LinearSVC For Validation\n",
    "\n",
    "pipeline.set_params(Model = LinearSVC(**LinearSVCTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('LinearSVC')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned RidgeClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = RidgeClassifier(**RidgeClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('RidgeClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned SGDClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = SGDClassifier(**SGDClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('SGDClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned XGBClassifier For Validation\n",
    "\n",
    "pipeline.set_params(Model = XGBClassifier(**XGBClassifierTuned.best_params,random_state = seed))\n",
    "pipeline.fit(X_train, y_train)\n",
    "classifierListVal.append('XGBClassifier')\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "y_prob = pipeline.predict_proba(X_validation)\n",
    "y_pred\n",
    "print(y_pred)\n",
    "y_predListVal.append(y_pred)\n",
    "y_validationListVal.append(y_validation)\n",
    "y_probListVal.append(y_prob)\n",
    "seedListStatsVal.append(seed)\n",
    "trainFracListStatsVal.append(trainFrac)\n",
    "\n",
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "print(\"MCC\", sklearn.metrics.matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "balancedAccuracyListVal.append(sklearn.metrics.balanced_accuracy_score(y_validation, y_pred))\n",
    "precisionListVal.append(sklearn.metrics.precision_score(y_validation, y_pred))\n",
    "recallListVal.append(sklearn.metrics.recall_score(y_validation, y_pred))\n",
    "ROCAUCScoreListVal.append(sklearn.metrics.roc_auc_score(y_validation, y_pred))\n",
    "F1ScoreListVal.append(sklearn.metrics.f1_score(y_validation, y_pred))\n",
    "MCCListVal.append(matthews_corrcoef(y_validation, y_pred))\n",
    "\n",
    "s = pipeline.score(X_validation, y_validation)\n",
    "print(f\"Validation score {s}\")\n",
    "\n",
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_validation, y_pred, normalize=True,figsize=(4,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make final pipeline for each seed and 'test' it on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;Transforming Distribution&#x27;,\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                (&#x27;Standard Scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;Model&#x27;,\n",
       "                 KNeighborsClassifier(algorithm=&#x27;ball_tree&#x27;, leaf_size=45,\n",
       "                                      metric=&#x27;euclidean&#x27;, n_jobs=6,\n",
       "                                      n_neighbors=2))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;Transforming Distribution&#x27;,\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                (&#x27;Standard Scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;Model&#x27;,\n",
       "                 KNeighborsClassifier(algorithm=&#x27;ball_tree&#x27;, leaf_size=45,\n",
       "                                      metric=&#x27;euclidean&#x27;, n_jobs=6,\n",
       "                                      n_neighbors=2))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler(quantile_range=(25, 75))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(algorithm=&#x27;ball_tree&#x27;, leaf_size=45, metric=&#x27;euclidean&#x27;,\n",
       "                     n_jobs=6, n_neighbors=2)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('Transforming Distribution',\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                ('Standard Scaler', StandardScaler()),\n",
       "                ('Model',\n",
       "                 KNeighborsClassifier(algorithm='ball_tree', leaf_size=45,\n",
       "                                      metric='euclidean', n_jobs=6,\n",
       "                                      n_neighbors=2))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final pipeline\n",
    "# KNeighborsClassifier for dfHarmonized (e.g. pan-cancer dataset)\n",
    "\n",
    "pipeline.set_params(Model = KNeighborsClassifier(**KNeighborsClassifierTuned.best_params, n_jobs=nJobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;Transforming Distribution&#x27;,\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                (&#x27;Standard Scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;Model&#x27;,\n",
       "                 KNeighborsClassifier(algorithm=&#x27;ball_tree&#x27;, leaf_size=45,\n",
       "                                      metric=&#x27;euclidean&#x27;, n_jobs=6,\n",
       "                                      n_neighbors=2))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;Transforming Distribution&#x27;,\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                (&#x27;Standard Scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;Model&#x27;,\n",
       "                 KNeighborsClassifier(algorithm=&#x27;ball_tree&#x27;, leaf_size=45,\n",
       "                                      metric=&#x27;euclidean&#x27;, n_jobs=6,\n",
       "                                      n_neighbors=2))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler(quantile_range=(25, 75))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(algorithm=&#x27;ball_tree&#x27;, leaf_size=45, metric=&#x27;euclidean&#x27;,\n",
       "                     n_jobs=6, n_neighbors=2)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('Transforming Distribution',\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                ('Standard Scaler', StandardScaler()),\n",
       "                ('Model',\n",
       "                 KNeighborsClassifier(algorithm='ball_tree', leaf_size=45,\n",
       "                                      metric='euclidean', n_jobs=6,\n",
       "                                      n_neighbors=2))])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the pipeline\n",
    "\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# load test data from saved\n",
    "dfTestFromDisk = pd.read_excel('_dfTest_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.xlsx')\n",
    "df = dfTestFromDisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features based on dfTrain's RFE\n",
    "\n",
    "fileName = '_selectedFeaturesRFECV_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed)\n",
    "\n",
    "with open(fileName + '.json', 'r') as f:\n",
    "    selected_featuresFromDisk = json.load(f)\n",
    "\n",
    "colsToKeep = []\n",
    "colsToKeep = copy.deepcopy(selected_featuresFromDisk)\n",
    "colsToKeep.append(target)\n",
    "\n",
    "df = df[colsToKeep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X and Y\n",
    "\n",
    "X_test, y_test = X_y_split(df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode y, as above\n",
    "y_test = pd.Series(le.fit_transform(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACTBL2</th>\n",
       "      <th>ACTG1</th>\n",
       "      <th>ACTG2</th>\n",
       "      <th>ACTN1</th>\n",
       "      <th>ACTN4</th>\n",
       "      <th>ADAM10</th>\n",
       "      <th>ADAMTS13</th>\n",
       "      <th>ADIPOQ</th>\n",
       "      <th>AFM</th>\n",
       "      <th>AGT</th>\n",
       "      <th>...</th>\n",
       "      <th>SVEP1</th>\n",
       "      <th>TFRC</th>\n",
       "      <th>THBS1</th>\n",
       "      <th>TLN2</th>\n",
       "      <th>TPM3</th>\n",
       "      <th>TUBA8</th>\n",
       "      <th>UGT8</th>\n",
       "      <th>VCL</th>\n",
       "      <th>VIM</th>\n",
       "      <th>VWF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.238035</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.605869</td>\n",
       "      <td>0.220409</td>\n",
       "      <td>-0.132511</td>\n",
       "      <td>-0.596869</td>\n",
       "      <td>-0.865540</td>\n",
       "      <td>-0.289417</td>\n",
       "      <td>-0.372957</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.948434</td>\n",
       "      <td>1.096785</td>\n",
       "      <td>-0.605986</td>\n",
       "      <td>0.487313</td>\n",
       "      <td>0.093620</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>0.854703</td>\n",
       "      <td>-0.662153</td>\n",
       "      <td>1.552332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.164659</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.146783</td>\n",
       "      <td>-0.171783</td>\n",
       "      <td>-0.298763</td>\n",
       "      <td>-0.241311</td>\n",
       "      <td>-0.445396</td>\n",
       "      <td>0.533932</td>\n",
       "      <td>0.222233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.651067</td>\n",
       "      <td>0.142736</td>\n",
       "      <td>0.337740</td>\n",
       "      <td>0.046313</td>\n",
       "      <td>0.324696</td>\n",
       "      <td>0.066913</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.247514</td>\n",
       "      <td>0.116570</td>\n",
       "      <td>1.714698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.310621</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.622334</td>\n",
       "      <td>0.296659</td>\n",
       "      <td>-0.191976</td>\n",
       "      <td>-1.654262</td>\n",
       "      <td>-0.505006</td>\n",
       "      <td>-0.360024</td>\n",
       "      <td>-0.191644</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.089110</td>\n",
       "      <td>0.121975</td>\n",
       "      <td>0.877530</td>\n",
       "      <td>-0.639579</td>\n",
       "      <td>0.141469</td>\n",
       "      <td>0.150554</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>0.764212</td>\n",
       "      <td>-0.093682</td>\n",
       "      <td>1.187440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.319442</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.337444</td>\n",
       "      <td>0.027481</td>\n",
       "      <td>-0.339895</td>\n",
       "      <td>-0.337704</td>\n",
       "      <td>-0.997932</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>0.389514</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.681654</td>\n",
       "      <td>-0.072275</td>\n",
       "      <td>0.454740</td>\n",
       "      <td>-0.242015</td>\n",
       "      <td>0.063445</td>\n",
       "      <td>-0.009888</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>0.302514</td>\n",
       "      <td>0.394435</td>\n",
       "      <td>1.805422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.105398</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.411348</td>\n",
       "      <td>-0.490759</td>\n",
       "      <td>-0.080501</td>\n",
       "      <td>-0.365737</td>\n",
       "      <td>-0.277148</td>\n",
       "      <td>0.424039</td>\n",
       "      <td>0.145391</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.856880</td>\n",
       "      <td>-1.931650</td>\n",
       "      <td>0.180562</td>\n",
       "      <td>-0.231454</td>\n",
       "      <td>-0.195705</td>\n",
       "      <td>0.588111</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>0.142215</td>\n",
       "      <td>0.146078</td>\n",
       "      <td>0.058512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.542626</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.062442</td>\n",
       "      <td>-0.526324</td>\n",
       "      <td>-0.712063</td>\n",
       "      <td>0.498297</td>\n",
       "      <td>0.504817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.948027</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>-0.487850</td>\n",
       "      <td>0.141260</td>\n",
       "      <td>-0.902553</td>\n",
       "      <td>-0.477969</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.663922</td>\n",
       "      <td>-0.503718</td>\n",
       "      <td>0.410547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.647026</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.309178</td>\n",
       "      <td>-0.054176</td>\n",
       "      <td>-0.470395</td>\n",
       "      <td>-1.672675</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.077890</td>\n",
       "      <td>-0.107259</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.549213</td>\n",
       "      <td>0.105562</td>\n",
       "      <td>0.570888</td>\n",
       "      <td>0.243236</td>\n",
       "      <td>0.612258</td>\n",
       "      <td>0.290803</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.047862</td>\n",
       "      <td>0.533338</td>\n",
       "      <td>1.494334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-0.844119</td>\n",
       "      <td>-0.141399</td>\n",
       "      <td>-0.671580</td>\n",
       "      <td>0.132653</td>\n",
       "      <td>-0.213185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.541840</td>\n",
       "      <td>-0.020314</td>\n",
       "      <td>-0.455560</td>\n",
       "      <td>0.035101</td>\n",
       "      <td>-0.552049</td>\n",
       "      <td>0.029566</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.621279</td>\n",
       "      <td>-0.115550</td>\n",
       "      <td>0.352758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>0.059684</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.302856</td>\n",
       "      <td>0.469412</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-2.212042</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.240386</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.913663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.566865</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-0.727181</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.339604</td>\n",
       "      <td>-0.022949</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-2.212042</td>\n",
       "      <td>-0.266810</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-0.369020</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.352981</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.157736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.111537</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-0.366964</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.262192</td>\n",
       "      <td>-0.573179</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-2.212042</td>\n",
       "      <td>-0.022853</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-0.335951</td>\n",
       "      <td>-0.143303</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.094270</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.015387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.518616</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-0.712679</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-0.943910</td>\n",
       "      <td>-0.265349</td>\n",
       "      <td>-0.443246</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.977919</td>\n",
       "      <td>0.150015</td>\n",
       "      <td>-0.443500</td>\n",
       "      <td>-1.400045</td>\n",
       "      <td>-0.644660</td>\n",
       "      <td>0.119929</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.917223</td>\n",
       "      <td>0.134167</td>\n",
       "      <td>0.714242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.560181</td>\n",
       "      <td>0.544833</td>\n",
       "      <td>0.162462</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.968511</td>\n",
       "      <td>-0.881045</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>1.198314</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.914506</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.685215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.251677</td>\n",
       "      <td>1.358475</td>\n",
       "      <td>1.242558</td>\n",
       "      <td>0.782361</td>\n",
       "      <td>0.635046</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.888320</td>\n",
       "      <td>-0.412423</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.479385</td>\n",
       "      <td>-0.468624</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>0.420096</td>\n",
       "      <td>0.689763</td>\n",
       "      <td>1.409121</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>0.826844</td>\n",
       "      <td>1.035853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.317586</td>\n",
       "      <td>0.691659</td>\n",
       "      <td>0.566534</td>\n",
       "      <td>-0.527146</td>\n",
       "      <td>-0.779873</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.447781</td>\n",
       "      <td>-0.495967</td>\n",
       "      <td>0.214131</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.591210</td>\n",
       "      <td>0.481686</td>\n",
       "      <td>-0.863229</td>\n",
       "      <td>-0.951502</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.386907</td>\n",
       "      <td>-0.766551</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.466945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.666744</td>\n",
       "      <td>-0.426753</td>\n",
       "      <td>0.361841</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-0.825229</td>\n",
       "      <td>0.053316</td>\n",
       "      <td>-0.881045</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.854914</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.657721</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>-0.841744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.354246</td>\n",
       "      <td>-0.529297</td>\n",
       "      <td>0.156518</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-0.078220</td>\n",
       "      <td>-0.273276</td>\n",
       "      <td>0.234181</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.650896</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.524331</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>0.579147</td>\n",
       "      <td>1.320569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.306275</td>\n",
       "      <td>-0.319189</td>\n",
       "      <td>-0.623059</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.200790</td>\n",
       "      <td>-0.270904</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-1.100746</td>\n",
       "      <td>0.153237</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.092549</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.807970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.078103</td>\n",
       "      <td>0.668771</td>\n",
       "      <td>0.733058</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.293392</td>\n",
       "      <td>-0.424678</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.143846</td>\n",
       "      <td>1.170063</td>\n",
       "      <td>-0.778114</td>\n",
       "      <td>-0.129786</td>\n",
       "      <td>-0.800246</td>\n",
       "      <td>1.514158</td>\n",
       "      <td>-0.382649</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.028943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.124282</td>\n",
       "      <td>0.491639</td>\n",
       "      <td>0.087445</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-0.692685</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.400563</td>\n",
       "      <td>0.062053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.861677</td>\n",
       "      <td>-0.265849</td>\n",
       "      <td>-0.772325</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-0.152089</td>\n",
       "      <td>1.442185</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.233082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.120665</td>\n",
       "      <td>-0.159721</td>\n",
       "      <td>-0.007275</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>0.091164</td>\n",
       "      <td>0.207575</td>\n",
       "      <td>-0.512949</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.022261</td>\n",
       "      <td>-0.524455</td>\n",
       "      <td>0.703959</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.763738</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>-0.607622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.945370</td>\n",
       "      <td>-0.264794</td>\n",
       "      <td>-1.137084</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.495928</td>\n",
       "      <td>0.389485</td>\n",
       "      <td>-0.601540</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.258549</td>\n",
       "      <td>-0.241983</td>\n",
       "      <td>-0.232726</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.823087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.907975</td>\n",
       "      <td>-0.120321</td>\n",
       "      <td>-0.192863</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.288493</td>\n",
       "      <td>-0.105683</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.049206</td>\n",
       "      <td>-0.593395</td>\n",
       "      <td>0.338131</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.225254</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.298017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.574986</td>\n",
       "      <td>-0.780223</td>\n",
       "      <td>-0.650690</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.177773</td>\n",
       "      <td>-0.007961</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.051644</td>\n",
       "      <td>0.310237</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.548240</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>-0.219977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>0.400107</td>\n",
       "      <td>0.355674</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.415286</td>\n",
       "      <td>-0.286618</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.206153</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.525455</td>\n",
       "      <td>-1.133267</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.573840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.306907</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.337481</td>\n",
       "      <td>-0.614056</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-1.410997</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>-0.396214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-0.538612</td>\n",
       "      <td>-0.538612</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-0.266315</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.024745</td>\n",
       "      <td>-0.222664</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.204238</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.685608</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.600477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-1.546379</td>\n",
       "      <td>-1.232325</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.136901</td>\n",
       "      <td>0.154829</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.072725</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.737113</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.217506</td>\n",
       "      <td>-0.870685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.038460</td>\n",
       "      <td>-0.644721</td>\n",
       "      <td>-0.009443</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-0.113763</td>\n",
       "      <td>-0.376972</td>\n",
       "      <td>-0.356950</td>\n",
       "      <td>0.372239</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.578188</td>\n",
       "      <td>0.137592</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.693413</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.698812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.472820</td>\n",
       "      <td>0.678585</td>\n",
       "      <td>0.651427</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-0.999879</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.968511</td>\n",
       "      <td>-0.473735</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.024787</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>2.047760</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.479332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.715963</td>\n",
       "      <td>-0.862155</td>\n",
       "      <td>-1.031285</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.326948</td>\n",
       "      <td>0.615792</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.303557</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.474389</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>-0.059296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.211535</td>\n",
       "      <td>1.682050</td>\n",
       "      <td>1.487347</td>\n",
       "      <td>0.214984</td>\n",
       "      <td>0.232982</td>\n",
       "      <td>-0.735157</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.325208</td>\n",
       "      <td>0.297780</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.605795</td>\n",
       "      <td>0.735557</td>\n",
       "      <td>0.279451</td>\n",
       "      <td>0.509170</td>\n",
       "      <td>0.026759</td>\n",
       "      <td>1.451548</td>\n",
       "      <td>0.688723</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.253672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.517525</td>\n",
       "      <td>-0.292850</td>\n",
       "      <td>-0.357244</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.511592</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.538804</td>\n",
       "      <td>-0.108588</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-1.190624</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.034546</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.594409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.225752</td>\n",
       "      <td>1.567625</td>\n",
       "      <td>1.418026</td>\n",
       "      <td>0.634106</td>\n",
       "      <td>0.470620</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.282337</td>\n",
       "      <td>0.950599</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.094850</td>\n",
       "      <td>0.470029</td>\n",
       "      <td>0.746587</td>\n",
       "      <td>0.096326</td>\n",
       "      <td>0.578469</td>\n",
       "      <td>0.373212</td>\n",
       "      <td>1.809622</td>\n",
       "      <td>0.490993</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.001913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.351284</td>\n",
       "      <td>0.810913</td>\n",
       "      <td>0.533514</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-0.629982</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.274823</td>\n",
       "      <td>-0.321923</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.265859</td>\n",
       "      <td>-0.298498</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.318380</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.645993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.118371</td>\n",
       "      <td>1.464801</td>\n",
       "      <td>1.296012</td>\n",
       "      <td>0.355235</td>\n",
       "      <td>0.139351</td>\n",
       "      <td>-0.474816</td>\n",
       "      <td>-1.030897</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>0.235523</td>\n",
       "      <td>0.405769</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.516065</td>\n",
       "      <td>0.819675</td>\n",
       "      <td>-0.054017</td>\n",
       "      <td>0.463809</td>\n",
       "      <td>0.074526</td>\n",
       "      <td>1.570868</td>\n",
       "      <td>0.374176</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.840446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-0.112933</td>\n",
       "      <td>-0.130619</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-0.550134</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.610157</td>\n",
       "      <td>-0.352111</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-2.212042</td>\n",
       "      <td>-0.796169</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.061120</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.542996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.575008</td>\n",
       "      <td>1.773153</td>\n",
       "      <td>1.727010</td>\n",
       "      <td>-0.155473</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>0.405695</td>\n",
       "      <td>-1.165220</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>1.121927</td>\n",
       "      <td>0.153250</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.140190</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>0.685391</td>\n",
       "      <td>0.861907</td>\n",
       "      <td>-0.059660</td>\n",
       "      <td>1.869754</td>\n",
       "      <td>1.035715</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.309079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-1.435105</td>\n",
       "      <td>-0.262634</td>\n",
       "      <td>-0.107894</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>0.583577</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.287568</td>\n",
       "      <td>0.197638</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.491841</td>\n",
       "      <td>-1.259770</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-1.117115</td>\n",
       "      <td>1.623980</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>0.012445</td>\n",
       "      <td>1.914975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.040652</td>\n",
       "      <td>0.484594</td>\n",
       "      <td>0.045431</td>\n",
       "      <td>-0.817767</td>\n",
       "      <td>-1.483101</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.150274</td>\n",
       "      <td>0.151766</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.217914</td>\n",
       "      <td>0.275741</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-0.232793</td>\n",
       "      <td>1.512694</td>\n",
       "      <td>-1.287931</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.768839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.007663</td>\n",
       "      <td>0.612934</td>\n",
       "      <td>0.257528</td>\n",
       "      <td>-0.608294</td>\n",
       "      <td>-0.587654</td>\n",
       "      <td>-0.661967</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.637876</td>\n",
       "      <td>-0.232350</td>\n",
       "      <td>-0.881045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.760438</td>\n",
       "      <td>0.342271</td>\n",
       "      <td>0.987446</td>\n",
       "      <td>-1.624791</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-0.083127</td>\n",
       "      <td>1.404424</td>\n",
       "      <td>-1.204875</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.035254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.830038</td>\n",
       "      <td>0.986249</td>\n",
       "      <td>0.949634</td>\n",
       "      <td>-0.792149</td>\n",
       "      <td>-1.104959</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-1.294871</td>\n",
       "      <td>0.340790</td>\n",
       "      <td>0.722486</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.537973</td>\n",
       "      <td>0.493954</td>\n",
       "      <td>-0.182145</td>\n",
       "      <td>-1.378484</td>\n",
       "      <td>-0.287746</td>\n",
       "      <td>1.619888</td>\n",
       "      <td>-0.297898</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.172519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.121603</td>\n",
       "      <td>0.736612</td>\n",
       "      <td>0.378226</td>\n",
       "      <td>-0.645225</td>\n",
       "      <td>-0.931143</td>\n",
       "      <td>-1.759557</td>\n",
       "      <td>-1.710125</td>\n",
       "      <td>-0.292686</td>\n",
       "      <td>0.434549</td>\n",
       "      <td>0.543902</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>-0.009679</td>\n",
       "      <td>-0.345377</td>\n",
       "      <td>-0.651122</td>\n",
       "      <td>-1.575466</td>\n",
       "      <td>-0.382705</td>\n",
       "      <td>1.019992</td>\n",
       "      <td>-0.402562</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>0.784080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.542420</td>\n",
       "      <td>1.693424</td>\n",
       "      <td>1.588997</td>\n",
       "      <td>0.891286</td>\n",
       "      <td>0.741147</td>\n",
       "      <td>-0.623075</td>\n",
       "      <td>-1.008431</td>\n",
       "      <td>-0.322892</td>\n",
       "      <td>0.052546</td>\n",
       "      <td>0.426901</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.475926</td>\n",
       "      <td>0.291694</td>\n",
       "      <td>0.936839</td>\n",
       "      <td>0.453057</td>\n",
       "      <td>0.577657</td>\n",
       "      <td>0.949400</td>\n",
       "      <td>1.667692</td>\n",
       "      <td>0.712856</td>\n",
       "      <td>-0.807743</td>\n",
       "      <td>1.245262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44 rows × 239 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ACTBL2     ACTG1     ACTG2     ACTN1     ACTN4    ADAM10  ADAMTS13  \\\n",
       "0   0.238035 -1.546379 -1.517205  0.605869  0.220409 -0.132511 -0.596869   \n",
       "1   0.164659 -1.546379 -1.517205  0.146783 -0.171783 -0.298763 -0.241311   \n",
       "2   0.310621 -1.546379 -1.517205  0.622334  0.296659 -0.191976 -1.654262   \n",
       "3   0.319442 -1.546379 -1.517205  0.337444  0.027481 -0.339895 -0.337704   \n",
       "4  -0.105398 -1.546379 -1.517205 -0.411348 -0.490759 -0.080501 -0.365737   \n",
       "5  -1.435105 -1.546379 -1.517205 -0.542626 -1.483101 -1.062442 -0.526324   \n",
       "6  -0.647026 -1.546379 -1.517205  0.309178 -0.054176 -0.470395 -1.672675   \n",
       "7  -1.435105 -1.546379 -1.517205 -0.817767 -1.483101 -0.844119 -0.141399   \n",
       "8  -1.435105 -1.546379 -1.517205 -0.817767  0.059684 -1.759557 -1.710125   \n",
       "9  -1.435105 -1.546379 -1.517205 -0.566865 -1.483101 -1.759557 -0.727181   \n",
       "10 -1.435105 -1.546379 -1.517205 -0.111537 -1.483101 -1.759557 -0.366964   \n",
       "11 -1.435105 -1.546379 -1.517205 -0.518616 -1.483101 -0.712679 -1.710125   \n",
       "12 -0.560181  0.544833  0.162462 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "13  1.251677  1.358475  1.242558  0.782361  0.635046 -1.759557 -1.710125   \n",
       "14  0.317586  0.691659  0.566534 -0.527146 -0.779873 -1.759557 -1.447781   \n",
       "15  0.666744 -0.426753  0.361841 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "16  0.354246 -0.529297  0.156518 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "17 -1.306275 -0.319189 -0.623059 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "18  0.078103  0.668771  0.733058 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "19 -0.124282  0.491639  0.087445 -0.817767 -1.483101 -1.759557 -0.692685   \n",
       "20 -0.120665 -0.159721 -0.007275 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "21 -0.945370 -0.264794 -1.137084 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "22 -0.907975 -0.120321 -0.192863 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "23 -0.574986 -0.780223 -0.650690 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "24 -1.435105  0.400107  0.355674 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "25 -1.306907 -1.546379 -1.517205 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "26 -1.435105 -0.538612 -0.538612 -0.817767 -1.483101 -1.759557 -0.266315   \n",
       "27 -1.435105 -1.546379 -1.232325 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "28  0.038460 -0.644721 -0.009443 -0.817767 -1.483101 -1.759557 -0.113763   \n",
       "29  0.472820  0.678585  0.651427 -0.817767 -1.483101 -1.759557 -0.999879   \n",
       "30 -0.715963 -0.862155 -1.031285 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "31  1.211535  1.682050  1.487347  0.214984  0.232982 -0.735157 -1.710125   \n",
       "32 -0.517525 -0.292850 -0.357244 -0.817767 -1.483101 -1.759557 -1.511592   \n",
       "33  1.225752  1.567625  1.418026  0.634106  0.470620 -1.759557 -1.710125   \n",
       "34  0.351284  0.810913  0.533514 -0.817767 -1.483101 -1.759557 -0.629982   \n",
       "35  1.118371  1.464801  1.296012  0.355235  0.139351 -0.474816 -1.030897   \n",
       "36 -1.435105 -0.112933 -0.130619 -0.817767 -1.483101 -1.759557 -0.550134   \n",
       "37  1.575008  1.773153  1.727010 -0.155473 -1.483101  0.405695 -1.165220   \n",
       "38 -1.435105 -0.262634 -0.107894 -0.817767 -1.483101 -1.759557  0.583577   \n",
       "39  0.040652  0.484594  0.045431 -0.817767 -1.483101 -1.759557 -1.710125   \n",
       "40  0.007663  0.612934  0.257528 -0.608294 -0.587654 -0.661967 -1.710125   \n",
       "41  0.830038  0.986249  0.949634 -0.792149 -1.104959 -1.759557 -1.710125   \n",
       "42  0.121603  0.736612  0.378226 -0.645225 -0.931143 -1.759557 -1.710125   \n",
       "43  1.542420  1.693424  1.588997  0.891286  0.741147 -0.623075 -1.008431   \n",
       "\n",
       "      ADIPOQ       AFM       AGT  ...     SVEP1      TFRC     THBS1      TLN2  \\\n",
       "0  -0.865540 -0.289417 -0.372957  ... -1.475926 -0.948434  1.096785 -0.605986   \n",
       "1  -0.445396  0.533932  0.222233  ... -0.651067  0.142736  0.337740  0.046313   \n",
       "2  -0.505006 -0.360024 -0.191644  ... -1.089110  0.121975  0.877530 -0.639579   \n",
       "3  -0.997932  0.253700  0.389514  ... -0.681654 -0.072275  0.454740 -0.242015   \n",
       "4  -0.277148  0.424039  0.145391  ... -0.856880 -1.931650  0.180562 -0.231454   \n",
       "5  -0.712063  0.498297  0.504817  ... -0.948027 -0.018118 -0.487850  0.141260   \n",
       "6  -1.637876 -0.077890 -0.107259  ... -0.549213  0.105562  0.570888  0.243236   \n",
       "7  -0.671580  0.132653 -0.213185  ... -0.541840 -0.020314 -0.455560  0.035101   \n",
       "8  -1.637876  0.302856  0.469412  ... -1.475926 -2.212042 -1.259770 -1.624791   \n",
       "9  -1.637876 -0.339604 -0.022949  ... -1.475926 -2.212042 -0.266810 -1.624791   \n",
       "10 -1.637876  0.262192 -0.573179  ... -1.475926 -2.212042 -0.022853 -1.624791   \n",
       "11 -0.943910 -0.265349 -0.443246  ... -0.977919  0.150015 -0.443500 -1.400045   \n",
       "12 -1.637876 -0.968511 -0.881045  ... -1.475926  1.198314 -1.259770 -1.624791   \n",
       "13 -1.637876 -0.888320 -0.412423  ... -1.475926 -0.479385 -0.468624 -1.624791   \n",
       "14 -0.495967  0.214131  0.382353  ... -1.475926 -0.591210  0.481686 -0.863229   \n",
       "15 -0.825229  0.053316 -0.881045  ... -1.475926  0.854914 -1.259770 -1.624791   \n",
       "16 -0.078220 -0.273276  0.234181  ... -1.475926 -0.650896 -1.259770 -1.624791   \n",
       "17 -1.637876 -0.200790 -0.270904  ... -1.475926 -1.100746  0.153237 -1.624791   \n",
       "18 -1.637876 -0.293392 -0.424678  ... -1.475926 -0.143846  1.170063 -0.778114   \n",
       "19 -1.637876  0.400563  0.062053  ... -0.861677 -0.265849 -0.772325 -1.624791   \n",
       "20  0.091164  0.207575 -0.512949  ... -1.022261 -0.524455  0.703959 -1.624791   \n",
       "21 -1.495928  0.389485 -0.601540  ... -1.258549 -0.241983 -0.232726 -1.624791   \n",
       "22 -1.637876  0.288493 -0.105683  ... -1.049206 -0.593395  0.338131 -1.624791   \n",
       "23 -1.637876  0.177773 -0.007961  ... -1.475926 -0.051644  0.310237 -1.624791   \n",
       "24 -1.637876  0.415286 -0.286618  ... -1.475926  0.206153 -1.259770 -1.624791   \n",
       "25 -1.637876  0.337481 -0.614056  ... -1.475926 -1.410997 -1.259770 -1.624791   \n",
       "26 -1.637876  0.024745 -0.222664  ... -1.475926 -0.204238 -1.259770 -1.624791   \n",
       "27 -1.637876  0.136901  0.154829  ... -1.475926 -0.072725 -1.259770 -1.624791   \n",
       "28 -0.376972 -0.356950  0.372239  ... -1.475926  0.578188  0.137592 -1.624791   \n",
       "29 -1.637876 -0.968511 -0.473735  ... -1.475926  0.024787 -1.259770 -1.624791   \n",
       "30 -1.637876  0.326948  0.615792  ... -1.475926  0.303557 -1.259770 -1.624791   \n",
       "31 -1.637876 -0.325208  0.297780  ... -1.475926  0.605795  0.735557  0.279451   \n",
       "32 -1.637876 -0.538804 -0.108588  ... -1.475926 -1.190624 -1.259770 -1.624791   \n",
       "33 -1.637876  0.282337  0.950599  ... -1.094850  0.470029  0.746587  0.096326   \n",
       "34 -1.637876  0.274823 -0.321923  ... -1.475926  0.265859 -0.298498 -1.624791   \n",
       "35 -1.637876  0.235523  0.405769  ... -1.475926  0.516065  0.819675 -0.054017   \n",
       "36 -1.637876 -0.610157 -0.352111  ... -1.475926 -2.212042 -0.796169 -1.624791   \n",
       "37 -1.637876  1.121927  0.153250  ... -1.475926  0.140190 -1.259770  0.685391   \n",
       "38 -1.637876 -0.287568  0.197638  ... -1.475926 -0.491841 -1.259770 -1.624791   \n",
       "39 -1.637876 -0.150274  0.151766  ... -1.475926 -0.217914  0.275741 -1.624791   \n",
       "40 -1.637876 -0.232350 -0.881045  ... -0.760438  0.342271  0.987446 -1.624791   \n",
       "41 -1.294871  0.340790  0.722486  ... -1.475926 -0.537973  0.493954 -0.182145   \n",
       "42 -0.292686  0.434549  0.543902  ... -1.475926 -0.009679 -0.345377 -0.651122   \n",
       "43 -0.322892  0.052546  0.426901  ... -1.475926  0.291694  0.936839  0.453057   \n",
       "\n",
       "        TPM3     TUBA8      UGT8       VCL       VIM       VWF  \n",
       "0   0.487313  0.093620  1.019992  0.854703 -0.662153  1.552332  \n",
       "1   0.324696  0.066913  1.019992 -0.247514  0.116570  1.714698  \n",
       "2   0.141469  0.150554  1.019992  0.764212 -0.093682  1.187440  \n",
       "3   0.063445 -0.009888  1.019992  0.302514  0.394435  1.805422  \n",
       "4  -0.195705  0.588111  1.019992  0.142215  0.146078  0.058512  \n",
       "5  -0.902553 -0.477969  1.019992 -0.663922 -0.503718  0.410547  \n",
       "6   0.612258  0.290803  1.019992 -0.047862  0.533338  1.494334  \n",
       "7  -0.552049  0.029566  1.019992 -0.621279 -0.115550  0.352758  \n",
       "8  -1.575466 -1.117115  1.019992 -0.240386 -0.807743  0.913663  \n",
       "9  -0.369020 -1.117115  1.019992 -0.352981 -0.807743  1.157736  \n",
       "10 -0.335951 -0.143303  1.019992 -0.094270 -0.807743  1.015387  \n",
       "11 -0.644660  0.119929  1.019992 -0.917223  0.134167  0.714242  \n",
       "12 -1.575466 -1.117115  1.914506 -1.287931 -0.807743  1.685215  \n",
       "13  0.420096  0.689763  1.409121  0.177200  0.826844  1.035853  \n",
       "14 -0.951502 -1.117115  1.386907 -0.766551 -0.807743  1.466945  \n",
       "15 -1.575466 -1.117115  1.657721 -1.287931 -0.807743 -0.841744  \n",
       "16 -1.575466 -1.117115  1.524331 -1.287931  0.579147  1.320569  \n",
       "17 -1.575466 -1.117115  1.092549 -1.287931 -0.807743  0.807970  \n",
       "18 -0.129786 -0.800246  1.514158 -0.382649 -0.807743  1.028943  \n",
       "19 -1.575466 -0.152089  1.442185 -1.287931 -0.807743  1.233082  \n",
       "20 -1.575466 -1.117115  1.763738 -1.287931 -0.807743 -0.607622  \n",
       "21 -1.575466 -1.117115  1.019992 -1.287931 -0.807743  0.823087  \n",
       "22 -1.575466 -1.117115  1.225254 -1.287931 -0.807743  0.298017  \n",
       "23 -1.575466 -1.117115  1.548240 -1.287931 -0.807743 -0.219977  \n",
       "24 -1.575466 -1.117115  1.525455 -1.133267 -0.807743  0.573840  \n",
       "25 -1.575466 -1.117115  1.019992 -1.287931 -0.807743 -0.396214  \n",
       "26 -1.575466 -1.117115  1.685608 -1.287931 -0.807743  1.600477  \n",
       "27 -1.575466 -1.117115  1.737113 -1.287931 -0.217506 -0.870685  \n",
       "28 -1.575466 -1.117115  1.693413 -1.287931 -0.807743  1.698812  \n",
       "29 -1.575466 -1.117115  2.047760 -1.287931 -0.807743  1.479332  \n",
       "30 -1.575466 -1.117115  1.474389 -1.287931 -0.807743 -0.059296  \n",
       "31  0.509170  0.026759  1.451548  0.688723 -0.807743  1.253672  \n",
       "32 -1.575466 -1.117115  1.034546 -1.287931 -0.807743  1.594409  \n",
       "33  0.578469  0.373212  1.809622  0.490993 -0.807743  1.001913  \n",
       "34 -1.575466 -1.117115  1.318380 -1.287931 -0.807743  1.645993  \n",
       "35  0.463809  0.074526  1.570868  0.374176 -0.807743  0.840446  \n",
       "36 -1.575466 -1.117115  1.061120 -1.287931 -0.807743  1.542996  \n",
       "37  0.861907 -0.059660  1.869754  1.035715 -0.807743  1.309079  \n",
       "38 -1.575466 -1.117115  1.623980 -1.287931  0.012445  1.914975  \n",
       "39 -1.575466 -0.232793  1.512694 -1.287931 -0.807743  0.768839  \n",
       "40 -1.575466 -0.083127  1.404424 -1.204875 -0.807743  0.035254  \n",
       "41 -1.378484 -0.287746  1.619888 -0.297898 -0.807743  1.172519  \n",
       "42 -1.575466 -0.382705  1.019992 -0.402562 -0.807743  0.784080  \n",
       "43  0.577657  0.949400  1.667692  0.712856 -0.807743  1.245262  \n",
       "\n",
       "[44 rows x 239 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# impute 0s, as above\n",
    "\n",
    "imputeWideDFMinOr0(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute any new proteins in X_test with the min value of the dataset, as above\n",
    "genesWithNANs = X_test.columns[X_test.isna().any()].tolist()\n",
    "\n",
    "X_test[genesWithNANs] = min(X_test.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = pipeline.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-format this output\n",
    "y_prob = [i[1] for i in y_prob]\n",
    "\n",
    "# append to lists\n",
    "y_predList.append(y_pred)\n",
    "y_testList.append(y_test)\n",
    "y_probList.append(y_prob)\n",
    "seedListStats.append(seed)\n",
    "trainFracListStats.append(trainFrac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy 0.8928571428571428\n",
      "Precision 1.0\n",
      "Recall 0.7857142857142857\n",
      "ROC AUC score 0.8928571428571428\n",
      "F1 score 0.88\n",
      "Test score 0.9318181818181818\n"
     ]
    }
   ],
   "source": [
    "print(\"Balanced Accuracy\", sklearn.metrics.balanced_accuracy_score(y_test, y_pred))\n",
    "print(\"Precision\", sklearn.metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall\", sklearn.metrics.recall_score(y_test, y_pred))\n",
    "print(\"ROC AUC score\", sklearn.metrics.roc_auc_score(y_test, y_pred))\n",
    "print(\"F1 score\", sklearn.metrics.f1_score(y_test, y_pred))\n",
    "\n",
    "s = pipeline.score(X_test, y_test)\n",
    "print(f\"Test score {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      " [[30  0]\n",
      " [ 3 11]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix\\n\", sklearn.metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot normalized confusion matrix from the current seed\n",
    "skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True,figsize=(2,2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1377fd300>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from collections.abc import Iterable\n",
    "\n",
    "def flatten(lis):\n",
    "     for item in lis:\n",
    "         if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "             for x in flatten(item):\n",
    "                 yield x\n",
    "         else:        \n",
    "             yield item\n",
    "             \n",
    "cm = confusion_matrix(list(flatten(y_testList)), list(flatten(y_predList))) # for many trials\n",
    "\n",
    "# for all models\n",
    "from collections.abc import Iterable\n",
    "\n",
    "cm = confusion_matrix(list(flatten(y_testList)), list(flatten(y_predList))) # for all models\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot normalized confusion matrix for all seeds/models in the current loop\n",
    "skplt.metrics.plot_confusion_matrix(list(flatten(y_testList)), list(flatten(y_predList)), normalize=True,figsize=(2,2))\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test stastistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = cm[0][0]\n",
    "FN = cm[1][0] # type II error\n",
    "TP = cm[1][1]\n",
    "FP = cm[0][1] # Type I error\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "precision = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# balanced accurcy\n",
    "bACC = (TPR + TNR) / 2\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# F1 score\n",
    "F1 = (2 * TP) / (2 * TP + FP + FN)\n",
    "# False ommission rate\n",
    "FOR = 1 - NPV\n",
    "# LR+, positive liklihood ratio\n",
    "LRpos = TPR / FPR\n",
    "# LR-, negative liklihood ratio\n",
    "LRneg = FNR / TNR\n",
    "# diagnostic odds ratio\n",
    "oddsRatio = LRpos/LRneg\n",
    "\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sensitivity/TPR is: 0.7857142857142857\n",
      "the specificity/TNR is: 1.0\n",
      "the accuracy is: 0.9318181818181818\n",
      "the balanced accuracy is: 0.8928571428571428\n",
      "the negative predictive value (NPV) is 0.9090909090909091\n",
      "the diagnostic odds ratio is inf\n",
      "The positive LR is inf\n",
      "The negative LR is 0.21428571428571427\n",
      "The F1 score is 0.88\n"
     ]
    }
   ],
   "source": [
    "print(\"the sensitivity/TPR is: \" + str(TPR))\n",
    "print(\"the specificity/TNR is: \" + str(TNR))\n",
    "print(\"the accuracy is: \" + str(ACC))\n",
    "print(\"the balanced accuracy is: \" + str(bACC))\n",
    "print(\"the negative predictive value (NPV) is \" + str(NPV))\n",
    "print(\"the diagnostic odds ratio is \" + str(oddsRatio))\n",
    "print(\"The positive LR is \" + str(LRpos))\n",
    "print(\"The negative LR is \" + str(LRneg))\n",
    "print(\"The F1 score is \" + str(F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_false_positive(threshold_vector, y_test):\n",
    "    true_positive = np.equal(threshold_vector, 1) & np.equal(y_test, 1)\n",
    "    true_negative = np.equal(threshold_vector, 0) & np.equal(y_test, 0)\n",
    "    false_positive = np.equal(threshold_vector, 1) & np.equal(y_test, 0)\n",
    "    false_negative = np.equal(threshold_vector, 0) & np.equal(y_test, 1)\n",
    "\n",
    "    tpr = true_positive.sum() / (true_positive.sum() + false_negative.sum())\n",
    "    fpr = false_positive.sum() / (false_positive.sum() + true_negative.sum())\n",
    "\n",
    "    return tpr, fpr\n",
    "\n",
    "def roc_from_scratch(probabilities, y_test, partitions=100):\n",
    "    roc = np.array([])\n",
    "    for i in range(partitions + 1):\n",
    "        \n",
    "        threshold_vector = np.greater_equal(probabilities, i / partitions).astype(int)\n",
    "        tpr, fpr = true_false_positive(threshold_vector, y_test)\n",
    "        roc = np.append(roc, [fpr, tpr])\n",
    "        \n",
    "    return roc.reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUROC is: 0.9880952380952381\n"
     ]
    }
   ],
   "source": [
    "# calc the AUROC\n",
    "partitions = 100\n",
    "\n",
    "ROC = roc_from_scratch(y_prob, y_test, partitions=partitions) # y_prob[:,1]\n",
    "fpr, tpr = ROC[:, 0], ROC[:, 1]\n",
    "rectangle_roc = 0\n",
    "for k in range(partitions):\n",
    "        rectangle_roc = rectangle_roc + (fpr[k]- fpr[k + 1]) * tpr[k]\n",
    "print('The AUROC is: ' + str(rectangle_roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC is 0.9880952380952381\n",
      "AUROC is 0.9880952380952381\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# overlaid ROC\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100) # regularly spaced points\n",
    "\n",
    "#set up plotting area\n",
    "plt.figure(figsize=(3, 3))\n",
    "ax = plt.axes() # enables overlay\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'--', color = 'black', lw = 2)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "fprList = []\n",
    "tprList = []\n",
    "\n",
    "for y_prob, y_test in zip(y_probList,y_testList):\n",
    "    \n",
    "    ROC = roc_from_scratch(np.reshape(y_prob,(-1)),y_test,partitions=100) # y_prob[:,1]\n",
    "    plt.plot(ROC[:,0],ROC[:,1],color='black', alpha = 0.025)\n",
    "    plt.title('ROC Curve',fontsize=12)\n",
    "    plt.xlabel('False Positive Rate',fontsize=12)\n",
    "    plt.ylabel('True Positive Rate',fontsize=12)\n",
    "\n",
    "    sort = np.argsort(ROC[:,0])\n",
    "    interp_tpr = np.interp(mean_fpr, ROC[:,0][sort], ROC[:,1][sort]) # interp needs to be sorted\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    rectangle_roc = 0\n",
    "    for k in range(partitions):\n",
    "        fpr, tpr = ROC[:, 0], ROC[:, 1]\n",
    "        rectangle_roc = rectangle_roc + (fpr[k]- fpr[k + 1]) * tpr[k]\n",
    "    print('AUROC is ' + str(rectangle_roc))    \n",
    "    aucs.append(rectangle_roc)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.3,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"ROC curve\")\n",
    "\n",
    "plt.plot()\n",
    "ax.legend(bbox_to_anchor=(3.3, .5), loc='center right', borderaxespad=0)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "y_testListFlat = list(chain.from_iterable(y_testList)) \n",
    "y_predListFlat = list(chain.from_iterable(y_predList))\n",
    "y_probListFlat = list(chain.from_iterable(y_probList)) \n",
    "y_1MinusProbListFlat = [1 - x for x in y_probListFlat]\n",
    "y_predProbAFormattedLikeSKLearn = np.stack((y_1MinusProbListFlat, y_probListFlat), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save final lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save y_predList\n",
    "fileName = '_y_predList_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl'\n",
    "\n",
    "with open(fileName, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(y_predList, fp)\n",
    "\n",
    "# reload y_predList\n",
    "with open(fileName, \"rb\") as fp:   # Unpickling\n",
    "    y_predList = pickle.load(fp)\n",
    "\n",
    "\n",
    "\n",
    "# save y_probList\n",
    "fileName = '_y_probList_target-' + target + '_quantType_' + quantType + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl'\n",
    "\n",
    "with open(fileName, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(y_probList, fp)\n",
    "\n",
    "# reload y_probList\n",
    "with open(fileName, \"rb\") as fp:   # Unpickling\n",
    "    y_probList = pickle.load(fp)\n",
    "\n",
    "\n",
    "\n",
    "# save y_testList\n",
    "fileName = '_y_testList_target-' + target + '_quantType_' + quantType + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl'\n",
    "\n",
    "with open(fileName, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(y_testList, fp)\n",
    "\n",
    "# reload y_probList\n",
    "with open(fileName, \"rb\") as fp:   # Unpickling\n",
    "    y_testList = pickle.load(fp)\n",
    "\n",
    "\n",
    "# save finalImportancesMeanList\n",
    "fileName = '_finalImportancesMeanList_target-' + target + '_quantType_' + quantType + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl'\n",
    "\n",
    "with open(fileName, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(finalImportancesMeanList, fp)\n",
    "\n",
    "# reload finalImportancesMeanList\n",
    "with open(fileName, \"rb\") as fp:   # Unpickling\n",
    "    finalImportancesMeanList = pickle.load(fp)\n",
    "    \n",
    "\n",
    "# save finalFeaturesList\n",
    "fileName = '_finalFeaturesList_target-' + target + '_quantType_' + quantType + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_seed-' + str(seed) + '.pkl'\n",
    "\n",
    "with open(fileName, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(finalFeaturesList, fp)\n",
    "\n",
    "# reload finalFeaturesList\n",
    "with open(fileName, \"rb\") as fp:   # Unpickling\n",
    "    finalFeaturesList = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a KNN classifier on the whole dataset that can be utilized for analysis of other plasma EV proteomics datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# load from saved\n",
    "dfMLFromDisk = pd.read_excel('_dfML-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "\n",
    "X, y = X_y_split(dfMLFromDisk, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A2M</th>\n",
       "      <th>ACTA1</th>\n",
       "      <th>ACTB</th>\n",
       "      <th>ACTBL2</th>\n",
       "      <th>ACTG1</th>\n",
       "      <th>ACTG2</th>\n",
       "      <th>ACTN1</th>\n",
       "      <th>ACTN4</th>\n",
       "      <th>...</th>\n",
       "      <th>VIM</th>\n",
       "      <th>VTN</th>\n",
       "      <th>VWF</th>\n",
       "      <th>WDR1</th>\n",
       "      <th>YWHAB</th>\n",
       "      <th>YWHAE</th>\n",
       "      <th>YWHAG</th>\n",
       "      <th>YWHAH</th>\n",
       "      <th>YWHAQ</th>\n",
       "      <th>YWHAZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.830252</td>\n",
       "      <td>1.617173</td>\n",
       "      <td>1.529702</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.238035</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.605869</td>\n",
       "      <td>0.220409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.662153</td>\n",
       "      <td>1.651220</td>\n",
       "      <td>1.552332</td>\n",
       "      <td>0.394781</td>\n",
       "      <td>0.568536</td>\n",
       "      <td>0.793171</td>\n",
       "      <td>0.088769</td>\n",
       "      <td>0.297304</td>\n",
       "      <td>0.425608</td>\n",
       "      <td>0.877359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.332276</td>\n",
       "      <td>1.617173</td>\n",
       "      <td>1.309366</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.478284</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.456010</td>\n",
       "      <td>0.041927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410026</td>\n",
       "      <td>1.576208</td>\n",
       "      <td>1.427785</td>\n",
       "      <td>-0.010363</td>\n",
       "      <td>-0.163921</td>\n",
       "      <td>0.264368</td>\n",
       "      <td>-0.276322</td>\n",
       "      <td>0.307422</td>\n",
       "      <td>0.205803</td>\n",
       "      <td>0.476802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.357704</td>\n",
       "      <td>1.617173</td>\n",
       "      <td>1.208670</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-0.486699</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.312867</td>\n",
       "      <td>-0.110075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.271596</td>\n",
       "      <td>1.465018</td>\n",
       "      <td>1.357034</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>-0.153920</td>\n",
       "      <td>0.293756</td>\n",
       "      <td>-0.187815</td>\n",
       "      <td>0.363499</td>\n",
       "      <td>0.159687</td>\n",
       "      <td>0.525286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.482507</td>\n",
       "      <td>1.617173</td>\n",
       "      <td>1.400820</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.432288</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.640151</td>\n",
       "      <td>0.196756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263245</td>\n",
       "      <td>1.466153</td>\n",
       "      <td>1.407550</td>\n",
       "      <td>0.464374</td>\n",
       "      <td>0.455905</td>\n",
       "      <td>0.528796</td>\n",
       "      <td>0.104778</td>\n",
       "      <td>0.395489</td>\n",
       "      <td>0.370452</td>\n",
       "      <td>0.871807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.652147</td>\n",
       "      <td>1.617173</td>\n",
       "      <td>1.550001</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.495373</td>\n",
       "      <td>-1.732577</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>0.642554</td>\n",
       "      <td>0.355340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089003</td>\n",
       "      <td>1.599650</td>\n",
       "      <td>1.635177</td>\n",
       "      <td>0.675694</td>\n",
       "      <td>0.686577</td>\n",
       "      <td>0.867544</td>\n",
       "      <td>-0.069715</td>\n",
       "      <td>0.407808</td>\n",
       "      <td>0.310482</td>\n",
       "      <td>0.880123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>172</td>\n",
       "      <td>0.695520</td>\n",
       "      <td>2.162397</td>\n",
       "      <td>0.620386</td>\n",
       "      <td>0.152694</td>\n",
       "      <td>0.029239</td>\n",
       "      <td>0.256713</td>\n",
       "      <td>0.125511</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.286680</td>\n",
       "      <td>1.280491</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-0.504451</td>\n",
       "      <td>-0.612937</td>\n",
       "      <td>-0.544749</td>\n",
       "      <td>-0.490847</td>\n",
       "      <td>-0.575711</td>\n",
       "      <td>-0.186766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>173</td>\n",
       "      <td>0.561369</td>\n",
       "      <td>2.294855</td>\n",
       "      <td>1.989700</td>\n",
       "      <td>1.506687</td>\n",
       "      <td>1.542420</td>\n",
       "      <td>1.693424</td>\n",
       "      <td>1.588997</td>\n",
       "      <td>0.891286</td>\n",
       "      <td>0.741147</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.260654</td>\n",
       "      <td>1.245262</td>\n",
       "      <td>0.099659</td>\n",
       "      <td>0.677961</td>\n",
       "      <td>0.876450</td>\n",
       "      <td>0.702891</td>\n",
       "      <td>0.786348</td>\n",
       "      <td>0.816816</td>\n",
       "      <td>1.141525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>174</td>\n",
       "      <td>0.592910</td>\n",
       "      <td>2.337891</td>\n",
       "      <td>1.489827</td>\n",
       "      <td>1.014633</td>\n",
       "      <td>0.979343</td>\n",
       "      <td>1.144509</td>\n",
       "      <td>1.081129</td>\n",
       "      <td>-0.105925</td>\n",
       "      <td>-0.386698</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.214992</td>\n",
       "      <td>0.924127</td>\n",
       "      <td>-0.631446</td>\n",
       "      <td>0.277585</td>\n",
       "      <td>0.419303</td>\n",
       "      <td>0.280890</td>\n",
       "      <td>0.324307</td>\n",
       "      <td>0.330923</td>\n",
       "      <td>0.792864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>175</td>\n",
       "      <td>0.531825</td>\n",
       "      <td>2.062484</td>\n",
       "      <td>0.391271</td>\n",
       "      <td>-0.456651</td>\n",
       "      <td>-0.667611</td>\n",
       "      <td>-0.169866</td>\n",
       "      <td>-0.128379</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>0.803730</td>\n",
       "      <td>0.231407</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-2.172128</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.723978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>176</td>\n",
       "      <td>0.836052</td>\n",
       "      <td>2.133768</td>\n",
       "      <td>0.569608</td>\n",
       "      <td>-0.203207</td>\n",
       "      <td>-2.183991</td>\n",
       "      <td>0.050999</td>\n",
       "      <td>-0.056756</td>\n",
       "      <td>-1.836067</td>\n",
       "      <td>-2.099970</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.101460</td>\n",
       "      <td>1.086213</td>\n",
       "      <td>0.768596</td>\n",
       "      <td>-2.104248</td>\n",
       "      <td>-2.172128</td>\n",
       "      <td>-1.862757</td>\n",
       "      <td>-2.926084</td>\n",
       "      <td>-2.320411</td>\n",
       "      <td>-2.936509</td>\n",
       "      <td>-1.723978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 374 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0      A1BG       A2M     ACTA1      ACTB    ACTBL2     ACTG1  \\\n",
       "0             0  0.830252  1.617173  1.529702 -1.517205  0.238035 -1.732577   \n",
       "1             1  0.332276  1.617173  1.309366 -1.517205  0.478284 -1.732577   \n",
       "2             2  0.357704  1.617173  1.208670 -1.517205 -0.486699 -1.732577   \n",
       "3             3  0.482507  1.617173  1.400820 -1.517205  0.432288 -1.732577   \n",
       "4             4  0.652147  1.617173  1.550001 -1.517205  0.495373 -1.732577   \n",
       "..          ...       ...       ...       ...       ...       ...       ...   \n",
       "172         172  0.695520  2.162397  0.620386  0.152694  0.029239  0.256713   \n",
       "173         173  0.561369  2.294855  1.989700  1.506687  1.542420  1.693424   \n",
       "174         174  0.592910  2.337891  1.489827  1.014633  0.979343  1.144509   \n",
       "175         175  0.531825  2.062484  0.391271 -0.456651 -0.667611 -0.169866   \n",
       "176         176  0.836052  2.133768  0.569608 -0.203207 -2.183991  0.050999   \n",
       "\n",
       "        ACTG2     ACTN1     ACTN4  ...       VIM       VTN       VWF  \\\n",
       "0   -1.517205  0.605869  0.220409  ... -0.662153  1.651220  1.552332   \n",
       "1   -1.517205  0.456010  0.041927  ...  0.410026  1.576208  1.427785   \n",
       "2   -1.517205  0.312867 -0.110075  ... -0.271596  1.465018  1.357034   \n",
       "3   -1.517205  0.640151  0.196756  ...  0.263245  1.466153  1.407550   \n",
       "4   -1.517205  0.642554  0.355340  ... -0.089003  1.599650  1.635177   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "172  0.125511 -1.836067 -2.099970  ... -1.101460  1.286680  1.280491   \n",
       "173  1.588997  0.891286  0.741147  ... -1.101460  1.260654  1.245262   \n",
       "174  1.081129 -0.105925 -0.386698  ... -1.101460  1.214992  0.924127   \n",
       "175 -0.128379 -1.836067 -2.099970  ... -1.101460  0.803730  0.231407   \n",
       "176 -0.056756 -1.836067 -2.099970  ... -1.101460  1.086213  0.768596   \n",
       "\n",
       "         WDR1     YWHAB     YWHAE     YWHAG     YWHAH     YWHAQ     YWHAZ  \n",
       "0    0.394781  0.568536  0.793171  0.088769  0.297304  0.425608  0.877359  \n",
       "1   -0.010363 -0.163921  0.264368 -0.276322  0.307422  0.205803  0.476802  \n",
       "2    0.002520 -0.153920  0.293756 -0.187815  0.363499  0.159687  0.525286  \n",
       "3    0.464374  0.455905  0.528796  0.104778  0.395489  0.370452  0.871807  \n",
       "4    0.675694  0.686577  0.867544 -0.069715  0.407808  0.310482  0.880123  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "172 -2.104248 -0.504451 -0.612937 -0.544749 -0.490847 -0.575711 -0.186766  \n",
       "173  0.099659  0.677961  0.876450  0.702891  0.786348  0.816816  1.141525  \n",
       "174 -0.631446  0.277585  0.419303  0.280890  0.324307  0.330923  0.792864  \n",
       "175 -2.104248 -2.172128 -1.862757 -2.926084 -2.320411 -2.936509 -1.723978  \n",
       "176 -2.104248 -2.172128 -1.862757 -2.926084 -2.320411 -2.936509 -1.723978  \n",
       "\n",
       "[177 rows x 374 columns]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputeWideDFMinOr0(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and scale the data as in the pipeline below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform, matching the pipeline\n",
    "\n",
    "X_transformed = transformX(X, RobustScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# scale to same variance, this matches the pipeline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_transformed)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_transformed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection with recursive feature extraction (RFE)\n",
    "* tune minFeaturesToSelect and cv to retain ~60-150 proteins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def customRFECV (X_input):\n",
    "\n",
    "    global selected_features\n",
    "    global rfe\n",
    "    global min_features_to_select\n",
    "    \n",
    "    estimator = RandomForestClassifier(random_state=seed)\n",
    "    \n",
    "    minFeaturesToSelect = 100\n",
    "    \n",
    "    rfe = RFECV(\n",
    "        estimator=estimator, \n",
    "        \n",
    "        cv=3, \n",
    "        scoring= make_scorer(quadratic_weighted_kappa),\n",
    "        n_jobs = nJobs,\n",
    "        step = 1,\n",
    "    )\n",
    "    \n",
    "    rfe.fit(X_scaled, y)\n",
    "    \n",
    "    selected_features = []\n",
    "    \n",
    "    for i, feature in enumerate(X_input.columns):\n",
    "        if rfe.support_[i]:\n",
    "            selected_features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customRFECV(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "\n",
    "fileName = '_selectedFeaturesRFECV_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_wholeDataset'\n",
    "\n",
    "with open(fileName + '.json', 'w') as f:\n",
    "    json.dump(selected_features, f, indent=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to open\n",
    "\n",
    "fileName = '_selectedFeaturesRFECV_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_wholeDataset'\n",
    "\n",
    "with open(fileName + '.json', 'r') as f:\n",
    "    selected_features = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;Transforming Distribution&#x27;,\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                (&#x27;Standard Scaler&#x27;, StandardScaler()), (&#x27;Model&#x27;, None)])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-45\" type=\"checkbox\" ><label for=\"sk-estimator-id-45\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;Transforming Distribution&#x27;,\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                (&#x27;Standard Scaler&#x27;, StandardScaler()), (&#x27;Model&#x27;, None)])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-46\" type=\"checkbox\" ><label for=\"sk-estimator-id-46\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler(quantile_range=(25, 75))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-47\" type=\"checkbox\" ><label for=\"sk-estimator-id-47\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-48\" type=\"checkbox\" ><label for=\"sk-estimator-id-48\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">None</label><div class=\"sk-toggleable__content\"><pre>None</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('Transforming Distribution',\n",
       "                 RobustScaler(quantile_range=(25, 75))),\n",
       "                ('Standard Scaler', StandardScaler()), ('Model', None)])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make an empty sklearn pipeline without a classifier\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Transforming Distribution',  RobustScaler(with_centering = True, with_scaling = True, quantile_range=(25, 75), unit_variance = False)),\n",
    "    ('Standard Scaler', StandardScaler()),\n",
    "    ('Model', None),\n",
    "])\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = dfMLFromDisk.copy(deep = True)\n",
    "\n",
    "categorical_features,continuous_features, binary_features = columns_catNumOrBin(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "\n",
    "df = dfMLFromDisk.copy(deep = True)\n",
    "\n",
    "# filtered proteins based on RFE above\n",
    "fileName = '_selectedFeaturesRFECV_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_wholeDataset'\n",
    "\n",
    "with open(fileName + '.json', 'r') as f:\n",
    "    selected_featuresFromDisk = json.load(f)\n",
    "\n",
    "colsToFilter = []\n",
    "colsToFilter = copy.deepcopy(selected_featuresFromDisk)\n",
    "colsToFilter.append(target)\n",
    "df = df[colsToFilter] # selected features from RFE\n",
    "\n",
    "X, y = X_y_split(df, target)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = pd.Series(le.fit_transform(y))\n",
    "\n",
    "categorical_features,continuous_features, binary_features = columns_catNumOrBin(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>C1QB</th>\n",
       "      <th>C1R</th>\n",
       "      <th>CD5L</th>\n",
       "      <th>FGA</th>\n",
       "      <th>FGG</th>\n",
       "      <th>P4HB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.498124</td>\n",
       "      <td>-1.252469</td>\n",
       "      <td>1.417838</td>\n",
       "      <td>0.230725</td>\n",
       "      <td>2.364291</td>\n",
       "      <td>-0.362685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.498124</td>\n",
       "      <td>-1.252469</td>\n",
       "      <td>1.077541</td>\n",
       "      <td>0.230725</td>\n",
       "      <td>2.424627</td>\n",
       "      <td>-0.143682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.498124</td>\n",
       "      <td>-1.252469</td>\n",
       "      <td>1.135589</td>\n",
       "      <td>0.230725</td>\n",
       "      <td>2.304597</td>\n",
       "      <td>-0.321209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.498124</td>\n",
       "      <td>-1.252469</td>\n",
       "      <td>1.406347</td>\n",
       "      <td>0.230725</td>\n",
       "      <td>2.353432</td>\n",
       "      <td>-0.145631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.498124</td>\n",
       "      <td>-1.252469</td>\n",
       "      <td>1.093112</td>\n",
       "      <td>0.230725</td>\n",
       "      <td>2.559772</td>\n",
       "      <td>-0.190198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>172</td>\n",
       "      <td>1.076128</td>\n",
       "      <td>0.524960</td>\n",
       "      <td>0.795438</td>\n",
       "      <td>1.829274</td>\n",
       "      <td>1.978604</td>\n",
       "      <td>-2.386640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>173</td>\n",
       "      <td>1.398414</td>\n",
       "      <td>0.760424</td>\n",
       "      <td>1.486885</td>\n",
       "      <td>2.220795</td>\n",
       "      <td>2.376403</td>\n",
       "      <td>-2.151825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>174</td>\n",
       "      <td>1.278032</td>\n",
       "      <td>0.827503</td>\n",
       "      <td>1.638920</td>\n",
       "      <td>1.723370</td>\n",
       "      <td>1.921522</td>\n",
       "      <td>-2.386640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>175</td>\n",
       "      <td>1.202623</td>\n",
       "      <td>0.741566</td>\n",
       "      <td>1.476451</td>\n",
       "      <td>1.676672</td>\n",
       "      <td>1.738884</td>\n",
       "      <td>-2.386640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>176</td>\n",
       "      <td>1.201461</td>\n",
       "      <td>0.752376</td>\n",
       "      <td>1.513334</td>\n",
       "      <td>1.510180</td>\n",
       "      <td>1.541008</td>\n",
       "      <td>-2.386640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0      C1QB       C1R      CD5L       FGA       FGG      P4HB\n",
       "0             0 -0.498124 -1.252469  1.417838  0.230725  2.364291 -0.362685\n",
       "1             1 -0.498124 -1.252469  1.077541  0.230725  2.424627 -0.143682\n",
       "2             2 -0.498124 -1.252469  1.135589  0.230725  2.304597 -0.321209\n",
       "3             3 -0.498124 -1.252469  1.406347  0.230725  2.353432 -0.145631\n",
       "4             4 -0.498124 -1.252469  1.093112  0.230725  2.559772 -0.190198\n",
       "..          ...       ...       ...       ...       ...       ...       ...\n",
       "172         172  1.076128  0.524960  0.795438  1.829274  1.978604 -2.386640\n",
       "173         173  1.398414  0.760424  1.486885  2.220795  2.376403 -2.151825\n",
       "174         174  1.278032  0.827503  1.638920  1.723370  1.921522 -2.386640\n",
       "175         175  1.202623  0.741566  1.476451  1.676672  1.738884 -2.386640\n",
       "176         176  1.201461  0.752376  1.513334  1.510180  1.541008 -2.386640\n",
       "\n",
       "[177 rows x 7 columns]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# impute 0s, as above\n",
    "\n",
    "imputeWideDFMinOr0(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of classification algorithms\n",
    "\n",
    "classifiers = [\n",
    "    ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=3)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation:\n",
      "\n",
      "\n",
      "\n",
      "KNeighborsClassifier Classifier:\n",
      "\n",
      "\n",
      "  Mean Quadratic Weighted Kappa: 0.8755 ± 0.1508\n",
      "\n",
      "  Matthews correlation coefficient: 0.8890 ± 0.1309\n",
      "\n",
      "  F1 weighted score: 0.9434 ± 0.0675\n",
      "\n",
      "  Accuracy (normalized): 0.9504 ± 0.0614\n",
      "\n",
      "  CrossValScore: 0.7089 ± 0.1891\n"
     ]
    }
   ],
   "source": [
    "# perform cross validation on every model in the list. Use of a network computer cluster may be required\n",
    "\n",
    "print('\\nCross-Validation:')\n",
    "\n",
    "for j, (name, clf) in enumerate(classifiers):\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresCrossValScore = []\n",
    "    r2_scores = []\n",
    "    pipeline.set_params(Model = clf)\n",
    "    \n",
    "    print('\\n')\n",
    "    print(f'\\n{name} Classifier:\\n')\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        \n",
    "        #print('===================================================')\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        fold_std = np.std(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "        \n",
    "        print(f'\\n  Mean Quadratic Weighted Kappa: {mean_score:.4f} \\u00B1 {fold_std:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use Optuna to individually tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optuna logger\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# KNeighborsClassifier\n",
    "\n",
    "def objectiveKNeighborsClassifier(trial):\n",
    "    params = {\n",
    "        'n_neighbors': trial.suggest_int('n_neighbors', 1,30),\n",
    "        'weights': trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        'metric': trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        'algorithm': trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),\n",
    "        'leaf_size': trial.suggest_int('leaf_size', 2,200),\n",
    "        'p': trial.suggest_float('p', 0.1,10),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(Model = KNeighborsClassifier(**params, n_jobs = nJobs))\n",
    "    scores = []\n",
    "    scoresMCC = []\n",
    "    scoresF1Weighted = []\n",
    "    scoresAccuracy = []\n",
    "    scoresROC = []\n",
    "    scoresCrossValScore = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(cv_splits):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y_val, y_pred, weights = 'quadratic')\n",
    "        mcc = matthews_corrcoef(y_val, y_pred)\n",
    "        f1Weighted = f1_score(y_val, y_pred, average = 'weighted')\n",
    "        predictions = pipeline.predict_proba(X_val)[:,1] # new!\n",
    "        auc = roc_auc_score(y_val, predictions)\n",
    "        accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "        crossValScore = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "        \n",
    "        print(f'Fold {i + 1}:\\n')\n",
    "        \n",
    "        scores.append(kappa)\n",
    "        scoresMCC.append(mcc)\n",
    "        scoresF1Weighted.append(f1Weighted)\n",
    "        scoresAccuracy.append(accuracy)\n",
    "        scoresROC.append(auc)\n",
    "        scoresCrossValScore.append(crossValScore)\n",
    "        fold_std = np.std(scores)\n",
    "        fold_stdMCC = np.std(scoresMCC)\n",
    "        fold_stdF1Weighted = np.std(scoresF1Weighted)\n",
    "        fold_stdAccuracy = np.std(scoresAccuracy)\n",
    "        fold_scoreROC = np.std(scoresROC)\n",
    "        fold_scoreCrossValScore = np.std(scoresCrossValScore)\n",
    "    \n",
    "    if i == len(cv_splits) - 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_scoreMCC = np.mean(scoresMCC)\n",
    "        mean_scoreF1Weighted = np.mean(scoresF1Weighted)\n",
    "        mean_scoreAccuracy = np.mean(scoresAccuracy)\n",
    "        mean_scoreROC = np.mean(scoresROC)\n",
    "        mean_scoreCrossValScore = np.mean(scoresCrossValScore)\n",
    "           \n",
    "        print('* * * * * * * * * * * * * * * * * * * * * * * * * * * *\\n')\n",
    "        print(f'  Mean Quadratic Weighted Kappa: {mean_score:.4f}')\n",
    "        print(f'\\n  Matthews correlation coefficient: {mean_scoreMCC:.4f} \\u00B1 {fold_stdMCC:.4f}')\n",
    "        print(f'\\n  F1 weighted score: {mean_scoreF1Weighted:.4f} \\u00B1 {fold_stdF1Weighted:.4f}')\n",
    "        print(f'\\n  Accuracy (normalized): {mean_scoreAccuracy:.4f} \\u00B1 {fold_stdAccuracy:.4f}')\n",
    "        print(f'\\n  ROC: {mean_scoreROC:.4f}')\n",
    "        print(f'\\n  CrossValScore: {mean_scoreCrossValScore:.4f} \\u00B1 {fold_scoreCrossValScore:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return mean_scoreAccuracy\n",
    "\n",
    "studyName = 'studyKNeighborsClassifier_' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_wholeDataset'\n",
    "storagestudyKNeighborsClassifier = 'sqlite:///' + studyName +'.db'\n",
    "studyKNeighborsClassifier = optuna.create_study(study_name = studyName, direction='maximize', storage = storagestudyKNeighborsClassifier, load_if_exists=True)\n",
    "studyKNeighborsClassifier.optimize(objectiveKNeighborsClassifier, n_trials = 500, show_progress_bar = True, catch=(FloatingPointError, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNeighborsClassifier:\n",
      "\n",
      "Number of finished trials: 551\n",
      "\n",
      " Best RMSE score = 0.9718333333333333 \n",
      "\n",
      "\n",
      " Best Params = {'n_neighbors': 1, 'weights': 'distance', 'metric': 'manhattan', 'algorithm': 'auto', 'leaf_size': 169, 'p': 2.976730890736924} \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['KNeighborsClassifierTuned_target-dfHarmonizedCancer_nSampleFilter60_trainFrac-0.75_wholeDataset.pkl']"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_paramsKNeighborsClassifier = studyKNeighborsClassifier.best_params\n",
    "best_rmse_scoreKNeighborsClassifier = studyKNeighborsClassifier.best_value\n",
    "\n",
    "print(f'\\nKNeighborsClassifier:\\n')\n",
    "print('Number of finished trials:', len(studyKNeighborsClassifier.trials))\n",
    "print(f'\\n Best RMSE score = {best_rmse_scoreKNeighborsClassifier} \\n')\n",
    "print(f'\\n Best Params = {best_paramsKNeighborsClassifier} \\n')\n",
    "\n",
    "# save\n",
    "joblib.dump(studyKNeighborsClassifier, 'KNeighborsClassifierTuned_target-' + target + '_nSampleFilter' + str(nSamplesVal) + '_trainFrac-' + str(trainFrac) + '_wholeDataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plotly.com"
       },
       "data": [
        {
         "mode": "markers",
         "name": "Objective Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550
         ],
         "y": [
          0.9359007936507936,
          0.8224007936507935,
          0.7980515873015874,
          0.938718253968254,
          0.8996626984126983,
          0.9148928571428573,
          0.8291190476190476,
          0.9109246031746032,
          0.912079365079365,
          0.8696230158730159,
          0.9138650793650794,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9127539682539683,
          0.9718333333333333,
          0.8537222222222224,
          0.9171984126984128,
          0.9290238095238095,
          0.8922777777777778,
          0.9664047619047621,
          0.9718333333333333,
          0.9141269841269842,
          0.9469484126984128,
          0.9718333333333333,
          0.9121269841269841,
          0.9647380952380954,
          0.9267460317460318,
          0.9032103174603173,
          0.9469484126984128,
          0.8376746031746031,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.938718253968254,
          0.9503531746031747,
          0.9250793650793651,
          0.906424603174603,
          0.9651309523809525,
          0.924138888888889,
          0.9482698412698415,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9267460317460318,
          0.8824761904761905,
          0.9651309523809525,
          0.9520198412698414,
          0.938718253968254,
          0.805515873015873,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.9093492063492065,
          0.9056746031746034,
          0.938718253968254,
          0.9469484126984128,
          0.8320079365079365,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9664047619047621,
          0.9320674603174604,
          0.9250793650793651,
          0.9718333333333333,
          0.938718253968254,
          0.8922777777777778,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9520198412698414,
          0.9718333333333333,
          0.9320674603174604,
          0.9469484126984128,
          0.9095079365079366,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.879797619047619,
          0.938718253968254,
          0.9718333333333333,
          0.9503531746031747,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9350079365079367,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9171984126984127,
          0.9664047619047621,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9346507936507936,
          0.9718333333333333,
          0.9718333333333333,
          0.9357579365079366,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.9469484126984128,
          0.8318968253968254,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9647380952380954,
          0.9718333333333333,
          0.9469484126984128,
          0.9053888888888888,
          0.8855714285714287,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.8605793650793652,
          0.9718333333333333,
          0.9718333333333333,
          0.9357579365079366,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9458373015873016,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.9538690476190478,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9320674603174604,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9458373015873016,
          0.8951349206349206,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9049126984126984,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9357579365079366,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.9538690476190478,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9141269841269842,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9320674603174604,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9651309523809525,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9538690476190478,
          0.8605793650793652,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9513373015873018,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.8318968253968254,
          0.9718333333333333,
          0.9109246031746032,
          0.9127539682539683,
          0.9718333333333333,
          0.8897380952380953,
          0.9718333333333333,
          0.9718333333333333,
          0.9399246031746032,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9538690476190478,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9320674603174604,
          0.9718333333333333,
          0.8470158730158731,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9170436507936509,
          0.9121269841269841,
          0.9458373015873016,
          0.9718333333333333,
          0.9664047619047621,
          0.9538690476190478,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.8526150793650793,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.8951349206349206,
          0.9718333333333333,
          0.9718333333333333,
          0.9503531746031747,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9320674603174604,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9651309523809525,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9538690476190478,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.8213611111111112,
          0.9171984126984127,
          0.9357579365079366,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9651309523809525,
          0.9267460317460318,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.8824761904761905,
          0.9538690476190478,
          0.9171984126984128,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9320674603174604,
          0.9138650793650794,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9538690476190478,
          0.938718253968254,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9320674603174604,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9141269841269842,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.8989603174603176,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9357579365079366,
          0.8605793650793652,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.849793650793651,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9538690476190478,
          0.8897380952380953,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.8060873015873016,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9538690476190478,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9320674603174604,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9127539682539683,
          0.9458373015873016,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.8656825396825398,
          0.9718333333333333,
          0.9718333333333333,
          0.9095079365079366,
          0.9469484126984128,
          0.9718333333333333,
          0.9320674603174604,
          0.938718253968254,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9651309523809525,
          0.9718333333333333,
          0.9718333333333333,
          0.9664047619047621,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.8922777777777778,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9538690476190478,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9320674603174604,
          0.9718333333333333,
          0.9458373015873016,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9121269841269841,
          0.9469484126984128,
          0.9538690476190478,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9651309523809525,
          0.9320674603174604,
          0.9664047619047621,
          0.9718333333333333,
          0.9469484126984128,
          0.9718333333333333,
          0.9718333333333333,
          0.9171984126984127,
          0.9718333333333333
         ]
        },
        {
         "mode": "lines",
         "name": "Best Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550
         ],
         "y": [
          0.9359007936507936,
          0.9359007936507936,
          0.9359007936507936,
          0.938718253968254,
          0.938718253968254,
          0.938718253968254,
          0.938718253968254,
          0.938718253968254,
          0.938718253968254,
          0.938718253968254,
          0.938718253968254,
          0.938718253968254,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333,
          0.9718333333333333
         ]
        },
        {
         "marker": {
          "color": "#cccccc"
         },
         "mode": "markers",
         "name": "Infeasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [],
         "y": []
        }
       ],
       "layout": {
        "autosize": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Optimization History Plot"
        },
        "xaxis": {
         "autorange": true,
         "range": [
          -40.51499348109518,
          590.5149934810952
         ],
         "title": {
          "text": "Trial"
         },
         "type": "linear"
        },
        "yaxis": {
         "autorange": true,
         "range": [
          0.783710375250424,
          0.9861745453844968
         ],
         "title": {
          "text": "Objective Value"
         },
         "type": "linear"
        }
       }
      },
      "image/png": "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",
      "text/html": [
       "<div>                            <div id=\"ec2b51b5-2196-466e-aca0-27a4a92e41a9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ec2b51b5-2196-466e-aca0-27a4a92e41a9\")) {                    Plotly.newPlot(                        \"ec2b51b5-2196-466e-aca0-27a4a92e41a9\",                        [{\"mode\":\"markers\",\"name\":\"Objective Value\",\"x\":[0,1,2,3,4,5,6,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550],\"y\":[0.9359007936507936,0.8224007936507935,0.7980515873015874,0.938718253968254,0.8996626984126983,0.9148928571428573,0.8291190476190476,0.9109246031746032,0.912079365079365,0.8696230158730159,0.9138650793650794,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9127539682539683,0.9718333333333333,0.8537222222222224,0.9171984126984128,0.9290238095238095,0.8922777777777778,0.9664047619047621,0.9718333333333333,0.9141269841269842,0.9469484126984128,0.9718333333333333,0.9121269841269841,0.9647380952380954,0.9267460317460318,0.9032103174603173,0.9469484126984128,0.8376746031746031,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.938718253968254,0.9503531746031747,0.9250793650793651,0.906424603174603,0.9651309523809525,0.924138888888889,0.9482698412698415,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9267460317460318,0.8824761904761905,0.9651309523809525,0.9520198412698414,0.938718253968254,0.805515873015873,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.9093492063492065,0.9056746031746034,0.938718253968254,0.9469484126984128,0.8320079365079365,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9664047619047621,0.9320674603174604,0.9250793650793651,0.9718333333333333,0.938718253968254,0.8922777777777778,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9520198412698414,0.9718333333333333,0.9320674603174604,0.9469484126984128,0.9095079365079366,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.879797619047619,0.938718253968254,0.9718333333333333,0.9503531746031747,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9350079365079367,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9171984126984127,0.9664047619047621,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9346507936507936,0.9718333333333333,0.9718333333333333,0.9357579365079366,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.9469484126984128,0.8318968253968254,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9647380952380954,0.9718333333333333,0.9469484126984128,0.9053888888888888,0.8855714285714287,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.8605793650793652,0.9718333333333333,0.9718333333333333,0.9357579365079366,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9458373015873016,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.9538690476190478,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9320674603174604,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9458373015873016,0.8951349206349206,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9049126984126984,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9357579365079366,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.9538690476190478,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9141269841269842,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9320674603174604,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9651309523809525,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9538690476190478,0.8605793650793652,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9513373015873018,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.8318968253968254,0.9718333333333333,0.9109246031746032,0.9127539682539683,0.9718333333333333,0.8897380952380953,0.9718333333333333,0.9718333333333333,0.9399246031746032,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9538690476190478,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9320674603174604,0.9718333333333333,0.8470158730158731,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9170436507936509,0.9121269841269841,0.9458373015873016,0.9718333333333333,0.9664047619047621,0.9538690476190478,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.8526150793650793,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.8951349206349206,0.9718333333333333,0.9718333333333333,0.9503531746031747,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9320674603174604,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9651309523809525,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9538690476190478,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.8213611111111112,0.9171984126984127,0.9357579365079366,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9651309523809525,0.9267460317460318,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.8824761904761905,0.9538690476190478,0.9171984126984128,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9320674603174604,0.9138650793650794,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9538690476190478,0.938718253968254,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9320674603174604,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9141269841269842,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.8989603174603176,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9357579365079366,0.8605793650793652,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.849793650793651,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9538690476190478,0.8897380952380953,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.8060873015873016,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9538690476190478,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9320674603174604,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9127539682539683,0.9458373015873016,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.8656825396825398,0.9718333333333333,0.9718333333333333,0.9095079365079366,0.9469484126984128,0.9718333333333333,0.9320674603174604,0.938718253968254,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9651309523809525,0.9718333333333333,0.9718333333333333,0.9664047619047621,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.8922777777777778,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9538690476190478,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9320674603174604,0.9718333333333333,0.9458373015873016,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9121269841269841,0.9469484126984128,0.9538690476190478,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9651309523809525,0.9320674603174604,0.9664047619047621,0.9718333333333333,0.9469484126984128,0.9718333333333333,0.9718333333333333,0.9171984126984127,0.9718333333333333],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Best Value\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550],\"y\":[0.9359007936507936,0.9359007936507936,0.9359007936507936,0.938718253968254,0.938718253968254,0.938718253968254,0.938718253968254,0.938718253968254,0.938718253968254,0.938718253968254,0.938718253968254,0.938718253968254,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333,0.9718333333333333],\"type\":\"scatter\"},{\"marker\":{\"color\":\"#cccccc\"},\"mode\":\"markers\",\"name\":\"Infeasible Trial\",\"showlegend\":false,\"x\":[],\"y\":[],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Optimization History Plot\"},\"xaxis\":{\"title\":{\"text\":\"Trial\"}},\"yaxis\":{\"title\":{\"text\":\"Objective Value\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ec2b51b5-2196-466e-aca0-27a4a92e41a9');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optuna.visualization.plot_optimization_history(studyKNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
